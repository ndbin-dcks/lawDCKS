import streamlit as st
from openai import OpenAI
import requests
import json
from datetime import datetime
import re
from urllib.parse import quote, urljoin
import time
import os
import tiktoken
from bs4 import BeautifulSoup

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="üèîÔ∏è Tr·ª£ l√Ω Ph√°p ch·∫ø Kho√°ng s·∫£n Vi·ªát Nam",
    page_icon="‚öñÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Pricing cho c√°c models OpenAI (USD per 1K tokens)
MODEL_PRICING = {
    "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002},
    "gpt-4": {"input": 0.03, "output": 0.06},
    "gpt-4-turbo": {"input": 0.01, "output": 0.03},
    "gpt-4-turbo-preview": {"input": 0.01, "output": 0.03}
}

class TokenCounter:
    """L·ªõp ƒë·∫øm token v√† t√≠nh chi ph√≠"""
    
    def __init__(self):
        self.reset_stats()
    
    def reset_stats(self):
        """Reset th·ªëng k√™ v·ªÅ 0"""
        if "token_stats" not in st.session_state:
            st.session_state.token_stats = {
                "total_input_tokens": 0,
                "total_output_tokens": 0,
                "total_cost": 0.0,
                "session_start": datetime.now(),
                "request_count": 0
            }
    
    def count_tokens(self, text, model="gpt-3.5-turbo"):
        """ƒê·∫øm s·ªë token trong text"""
        try:
            encoding = tiktoken.encoding_for_model(model)
            return len(encoding.encode(str(text)))
        except:
            # Fallback estimation: ~4 characters per token
            return len(str(text)) // 4
    
    def calculate_cost(self, input_tokens, output_tokens, model):
        """T√≠nh chi ph√≠ d·ª±a tr√™n s·ªë token"""
        if model not in MODEL_PRICING:
            model = "gpt-3.5-turbo"  # Default fallback
        
        pricing = MODEL_PRICING[model]
        input_cost = (input_tokens / 1000) * pricing["input"]
        output_cost = (output_tokens / 1000) * pricing["output"]
        return input_cost + output_cost
    
    def update_stats(self, input_tokens, output_tokens, model):
        """C·∫≠p nh·∫≠t th·ªëng k√™"""
        cost = self.calculate_cost(input_tokens, output_tokens, model)
        
        st.session_state.token_stats["total_input_tokens"] += input_tokens
        st.session_state.token_stats["total_output_tokens"] += output_tokens
        st.session_state.token_stats["total_cost"] += cost
        st.session_state.token_stats["request_count"] += 1
    
    def get_stats_display(self):
        """L·∫•y th·ªëng k√™ ƒë·ªÉ hi·ªÉn th·ªã"""
        stats = st.session_state.token_stats
        total_tokens = stats["total_input_tokens"] + stats["total_output_tokens"]
        
        return {
            "total_tokens": total_tokens,
            "input_tokens": stats["total_input_tokens"],
            "output_tokens": stats["total_output_tokens"],
            "total_cost_usd": stats["total_cost"],
            "total_cost_vnd": stats["total_cost"] * 24000,  # Estimate VND rate
            "requests": stats["request_count"],
            "session_duration": datetime.now() - stats["session_start"]
        }

# Kh·ªüi t·∫°o token counter
@st.cache_resource
def get_token_counter():
    return TokenCounter()

token_counter = get_token_counter()

# H√†m ƒë·ªçc n·ªôi dung t·ª´ file vƒÉn b·∫£n v·ªõi fallback cho chuy√™n ng√†nh kho√°ng s·∫£n
def rfile(name_file):
    try:
        with open(name_file, "r", encoding="utf-8") as file:
            return file.read()
    except FileNotFoundError:
        # Fallback content chuy√™n bi·ªát cho kho√°ng s·∫£n
        fallback_content = {
            "00.xinchao.txt": "‚öñÔ∏è Tr·ª£ l√Ω Ph√°p ch·∫ø Kho√°ng s·∫£n Vi·ªát Nam",
            "01.system_trainning.txt": """B·∫°n l√† chuy√™n gia ph√°p ch·∫ø v·ªÅ qu·∫£n l√Ω nh√† n∆∞·ªõc trong lƒ©nh v·ª±c kho√°ng s·∫£n t·∫°i Vi·ªát Nam. B·∫°n c√≥ ki·∫øn th·ª©c s√¢u r·ªông v·ªÅ:

üèîÔ∏è Lƒ®NH V·ª∞C CHUY√äN M√îN:
- Lu·∫≠t Kho√°ng s·∫£n 2017 v√† c√°c vƒÉn b·∫£n h∆∞·ªõng d·∫´n thi h√†nh
- Ngh·ªã ƒë·ªãnh v·ªÅ qu·∫£n l√Ω ho·∫°t ƒë·ªông kho√°ng s·∫£n
- Th√¥ng t∆∞ c·ªßa B·ªô T√†i nguy√™n v√† M√¥i tr∆∞·ªùng
- Quy ƒë·ªãnh v·ªÅ thƒÉm d√≤, khai th√°c kho√°ng s·∫£n
- C·∫•p ph√©p ho·∫°t ƒë·ªông kho√°ng s·∫£n
- Thu·∫ø, ph√≠ li√™n quan ƒë·∫øn kho√°ng s·∫£n
- B·∫£o v·ªá m√¥i tr∆∞·ªùng trong ho·∫°t ƒë·ªông kho√°ng s·∫£n
- X·ª≠ ph·∫°t vi ph·∫°m h√†nh ch√≠nh trong lƒ©nh v·ª±c kho√°ng s·∫£n

‚öñÔ∏è NGUY√äN T·∫ÆC L√ÄM VI·ªÜC:
1. Ch·ªâ t·∫≠p trung v√†o c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn kho√°ng s·∫£n ·ªü Vi·ªát Nam
2. ƒê∆∞a ra th√¥ng tin ch√≠nh x√°c, d·∫´n chi·∫øu c·ª• th·ªÉ ƒëi·ªÅu kho·∫£n ph√°p lu·∫≠t
3. Gi·∫£i th√≠ch r√µ r√†ng, d·ªÖ hi·ªÉu cho c·∫£ chuy√™n gia v√† ng∆∞·ªùi d√¢n
4. Khi t√¨m ki·∫øm web, ∆∞u ti√™n ngu·ªìn ch√≠nh th·ªëng: portal.gov.vn, monre.gov.vn, thuvienphapluat.vn
5. T·ª´ ch·ªëi tr·∫£ l·ªùi c√°c c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn kho√°ng s·∫£n

üéØ C√ÅCH TR√çCH D·∫™N:
- Lu√¥n ghi r√µ t√™n vƒÉn b·∫£n ph√°p lu·∫≠t, ƒëi·ªÅu, kho·∫£n c·ª• th·ªÉ
- V√≠ d·ª•: "Theo ƒêi·ªÅu 15 Lu·∫≠t Kho√°ng s·∫£n 2017..."
- Khi c√≥ th√¥ng tin web: "D·ª±a theo th√¥ng tin t·ª´ [ngu·ªìn ch√≠nh th·ªëng]..."

QUAN TR·ªåNG: Ch·ªâ tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ kho√°ng s·∫£n. N·∫øu c√¢u h·ªèi kh√¥ng li√™n quan, h√£y l·ªãch s·ª± chuy·ªÉn h∆∞·ªõng v·ªÅ lƒ©nh v·ª±c chuy√™n m√¥n.""",
            
            "02.assistant.txt": """Xin ch√†o! ‚öñÔ∏è 

T√¥i l√† Tr·ª£ l√Ω Ph√°p ch·∫ø chuy√™n v·ªÅ **Qu·∫£n l√Ω Nh√† n∆∞·ªõc trong lƒ©nh v·ª±c Kho√°ng s·∫£n t·∫°i Vi·ªát Nam**.

üèîÔ∏è **T√¥i c√≥ th·ªÉ h·ªó tr·ª£ b·∫°n v·ªÅ:**

‚úÖ **Ph√°p lu·∫≠t Kho√°ng s·∫£n:**
   ‚Ä¢ Lu·∫≠t Kho√°ng s·∫£n 2017 v√† vƒÉn b·∫£n h∆∞·ªõng d·∫´n
   ‚Ä¢ Ngh·ªã ƒë·ªãnh, Th√¥ng t∆∞ c·ªßa B·ªô TN&MT
   ‚Ä¢ Quy ƒë·ªãnh v·ªÅ thƒÉm d√≤, khai th√°c

‚úÖ **Th·ªß t·ª•c h√†nh ch√≠nh:**
   ‚Ä¢ C·∫•p gi·∫•y ph√©p thƒÉm d√≤, khai th√°c
   ‚Ä¢ H·ªì s∆°, quy tr√¨nh xin ph√©p
   ‚Ä¢ Th·ªùi h·∫°n, ƒëi·ªÅu ki·ªán c·∫•p ph√©p

‚úÖ **Thu·∫ø, ph√≠ Kho√°ng s·∫£n:**
   ‚Ä¢ Thu·∫ø t√†i nguy√™n
   ‚Ä¢ Ph√≠ thƒÉm d√≤, khai th√°c
   ‚Ä¢ Ti·ªÅn c·∫•p quy·ªÅn khai th√°c

‚úÖ **X·ª≠ ph·∫°t vi ph·∫°m:**
   ‚Ä¢ Ngh·ªã ƒë·ªãnh x·ª≠ ph·∫°t vi ph·∫°m h√†nh ch√≠nh
   ‚Ä¢ M·ª©c ph·∫°t, bi·ªán ph√°p kh·∫Øc ph·ª•c

‚úÖ **B·∫£o v·ªá m√¥i tr∆∞·ªùng:**
   ‚Ä¢ ƒê√°nh gi√° t√°c ƒë·ªông m√¥i tr∆∞·ªùng
   ‚Ä¢ Ph·ª•c h·ªìi m√¥i tr∆∞·ªùng sau khai th√°c

**üéØ L∆∞u √Ω:** T√¥i ch·ªâ t∆∞ v·∫•n v·ªÅ lƒ©nh v·ª±c Kho√°ng s·∫£n. ƒê·ªëi v·ªõi c√°c v·∫•n ƒë·ªÅ kh√°c, b·∫°n vui l√≤ng tham kh·∫£o chuy√™n gia ph√π h·ª£p.

**B·∫°n c√≥ th·∫Øc m·∫Øc g√¨ v·ªÅ ph√°p lu·∫≠t Kho√°ng s·∫£n kh√¥ng?** ü§î""",
            
            "module_chatgpt.txt": "gpt-3.5-turbo"
        }
        return fallback_content.get(name_file, f"N·ªôi dung m·∫∑c ƒë·ªãnh cho {name_file}")

# L·ªõp t√¨m ki·∫øm ph√°p lu·∫≠t n√¢ng cao
class AdvancedLegalSearcher:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # Patterns ƒë·ªÉ detect c·∫•u tr√∫c ph√°p lu·∫≠t
        self.legal_patterns = {
            'dieu': r'ƒêi·ªÅu\s+(\d+)',
            'khoan': r'kho·∫£n\s+(\d+)',
            'diem': r'ƒëi·ªÉm\s+([a-z])',
            'luat': r'Lu·∫≠t\s+([^0-9]*?)\s*(?:s·ªë\s*)?(\d+/\d+/[A-Z]+\d*)?',
            'nghi_dinh': r'Ngh·ªã ƒë·ªãnh\s*(?:s·ªë\s*)?(\d+/\d+/[A-Z\-]+)',
            'thong_tu': r'Th√¥ng t∆∞\s*(?:s·ªë\s*)?(\d+/\d+/[A-Z\-]+)'
        }
    
    def search(self, query, max_results=3):
        """T√¨m ki·∫øm ph√°p lu·∫≠t v·ªõi ƒë·ªô ch√≠nh x√°c cao"""
        try:
            # Ph√¢n t√≠ch query ƒë·ªÉ x√°c ƒë·ªãnh lo·∫°i t√¨m ki·∫øm
            search_type = self._analyze_query(query)
            
            results = []
            
            if search_type == 'specific_article':
                # T√¨m ki·∫øm ƒëi·ªÅu kho·∫£n c·ª• th·ªÉ
                results = self._search_specific_article(query, max_results)
            elif search_type == 'law_content':
                # T√¨m ki·∫øm n·ªôi dung ph√°p lu·∫≠t t·ªïng qu√°t
                results = self._search_law_content(query, max_results)
            else:
                # T√¨m ki·∫øm chung
                results = self._search_general_legal(query, max_results)
            
            # N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ fallback search
            if not results:
                results = self._fallback_search(query, max_results)
            
            return results[:max_results]
            
        except Exception as e:
            st.error(f"L·ªói t√¨m ki·∫øm ph√°p lu·∫≠t: {str(e)}")
            return self._get_error_result(query)
    
    def _analyze_query(self, query):
        """Ph√¢n t√≠ch query ƒë·ªÉ x√°c ƒë·ªãnh lo·∫°i t√¨m ki·∫øm"""
        query_lower = query.lower()
        
        # Ki·ªÉm tra t√¨m ki·∫øm ƒëi·ªÅu kho·∫£n c·ª• th·ªÉ
        if re.search(r'ƒëi·ªÅu\s+\d+', query_lower):
            return 'specific_article'
        
        # Ki·ªÉm tra t√¨m ki·∫øm n·ªôi dung lu·∫≠t
        if any(keyword in query_lower for keyword in ['lu·∫≠t', 'ngh·ªã ƒë·ªãnh', 'th√¥ng t∆∞']):
            return 'law_content'
        
        return 'general'
    
    def _search_specific_article(self, query, max_results):
        """T√¨m ki·∫øm ƒëi·ªÅu kho·∫£n c·ª• th·ªÉ trong ph√°p lu·∫≠t"""
        results = []
        
        try:
            # Tr√≠ch xu·∫•t th√¥ng tin t·ª´ query
            article_match = re.search(r'ƒëi·ªÅu\s+(\d+)', query.lower())
            law_match = re.search(r'lu·∫≠t\s+([^0-9]*?)(?:\s*nƒÉm\s*(\d{4})|\s*s·ªë\s*([0-9/A-Z]+))?', query.lower())
            
            if article_match:
                article_num = article_match.group(1)
                
                if law_match:
                    law_name = law_match.group(1).strip()
                    year = law_match.group(2) if law_match.group(2) else None
                    law_number = law_match.group(3) if law_match.group(3) else None
                    
                    # T√¨m ki·∫øm tr√™n thuvienphapluat.vn
                    results.extend(self._search_thuvienphapluat(
                        f"ƒëi·ªÅu {article_num} {law_name}", max_results
                    ))
                    
                    # T√¨m ki·∫øm tr√™n ngu·ªìn kh√°c
                    results.extend(self._search_legal_portals(
                        f"ƒëi·ªÅu {article_num} lu·∫≠t {law_name}", max_results - len(results)
                    ))
        
        except Exception as e:
            pass
        
        return results
    
    def _search_thuvienphapluat(self, query, max_results):
        """T√¨m ki·∫øm tr√™n thuvienphapluat.vn v·ªõi ƒë·ªô ch√≠nh x√°c cao"""
        results = []
        
        try:
            # T·ªëi ∆∞u query cho thuvienphapluat.vn
            optimized_query = f"site:thuvienphapluat.vn {query}"
            
            # S·ª≠ d·ª•ng DuckDuckGo ƒë·ªÉ search
            params = {
                'q': optimized_query,
                'format': 'json',
                'no_html': '1',
                'skip_disambig': '1'
            }
            
            response = self.session.get("https://api.duckduckgo.com/", params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                # Abstract answer
                if data.get('Abstract') and len(data['Abstract']) > 100:
                    confidence = self._calculate_confidence(query, data.get('AbstractText', ''), data.get('Abstract'))
                    results.append({
                        'title': data.get('AbstractText', 'Th√¥ng tin ph√°p lu·∫≠t')[:100],
                        'content': data.get('Abstract'),
                        'url': data.get('AbstractURL', ''),
                        'source': 'Th∆∞ vi·ªán Ph√°p lu·∫≠t',
                        'priority': True,
                        'confidence': confidence
                    })
                
                # Related topics
                for topic in data.get('RelatedTopics', [])[:max_results-len(results)]:
                    if isinstance(topic, dict) and topic.get('Text'):
                        confidence = self._calculate_confidence(query, topic.get('Text', ''), topic.get('Text', ''))
                        results.append({
                            'title': topic.get('Text', '')[:80] + '...',
                            'content': topic.get('Text', ''),
                            'url': topic.get('FirstURL', ''),
                            'source': 'Th∆∞ vi·ªán Ph√°p lu·∫≠t',
                            'priority': True,
                            'confidence': confidence
                        })
        
        except Exception as e:
            pass
        
        return results
    
    def _search_legal_portals(self, query, max_results):
        """T√¨m ki·∫øm tr√™n c√°c portal ph√°p lu·∫≠t kh√°c"""
        results = []
        
        portals = [
            ('portal.gov.vn', 'C·ªïng th√¥ng tin ƒëi·ªán t·ª≠ Ch√≠nh ph·ªß'),
            ('monre.gov.vn', 'B·ªô T√†i nguy√™n v√† M√¥i tr∆∞·ªùng'),
            ('moj.gov.vn', 'B·ªô T∆∞ ph√°p')
        ]
        
        for domain, source_name in portals:
            try:
                portal_results = self._search_domain(query, domain, source_name, max_results // len(portals))
                results.extend(portal_results)
                
                if len(results) >= max_results:
                    break
                    
            except:
                continue
        
        return results
    
    def _search_domain(self, query, domain, source_name, max_results):
        """T√¨m ki·∫øm tr√™n domain c·ª• th·ªÉ"""
        results = []
        
        try:
            # DuckDuckGo search v·ªõi site filter
            params = {
                'q': f"site:{domain} {query}",
                'format': 'json',
                'no_html': '1'
            }
            
            response = self.session.get("https://api.duckduckgo.com/", params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                # Abstract answer
                if data.get('Abstract'):
                    confidence = self._calculate_confidence(query, data.get('AbstractText', ''), data.get('Abstract'))
                    results.append({
                        'title': data.get('AbstractText', 'Th√¥ng tin ph√°p lu·∫≠t')[:100],
                        'content': data.get('Abstract'),
                        'url': data.get('AbstractURL', ''),
                        'source': source_name,
                        'priority': True,
                        'confidence': confidence
                    })
                
                # Related topics
                for topic in data.get('RelatedTopics', [])[:max_results-len(results)]:
                    if isinstance(topic, dict) and topic.get('Text'):
                        confidence = self._calculate_confidence(query, topic.get('Text', ''), topic.get('Text', ''))
                        results.append({
                            'title': topic.get('Text', '')[:80] + '...',
                            'content': topic.get('Text', ''),
                            'url': topic.get('FirstURL', ''),
                            'source': source_name,
                            'priority': True,
                            'confidence': confidence
                        })
        
        except Exception as e:
            pass
        
        return results
    
    def _search_law_content(self, query, max_results):
        """T√¨m ki·∫øm n·ªôi dung ph√°p lu·∫≠t t·ªïng qu√°t"""
        results = []
        
        # T√¨m ki·∫øm v·ªõi keywords ƒë∆∞·ª£c t·ªëi ∆∞u
        optimized_query = self._optimize_legal_query(query)
        
        # T√¨m tr√™n thuvienphapluat.vn
        results.extend(self._search_thuvienphapluat(optimized_query, max_results))
        
        # T√¨m tr√™n c√°c portal kh√°c
        if len(results) < max_results:
            results.extend(self._search_legal_portals(optimized_query, max_results - len(results)))
        
        return results
    
    def _search_general_legal(self, query, max_results):
        """T√¨m ki·∫øm ph√°p lu·∫≠t chung"""
        enhanced_query = f"{query} kho√°ng s·∫£n Vi·ªát Nam ph√°p lu·∫≠t"
        
        results = []
        
        # DuckDuckGo search c∆° b·∫£n
        try:
            params = {
                'q': enhanced_query,
                'format': 'json',
                'no_html': '1'
            }
            
            response = self.session.get("https://api.duckduckgo.com/", params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                if data.get('Abstract'):
                    confidence = self._calculate_confidence(query, data.get('AbstractText', ''), data.get('Abstract'))
                    results.append({
                        'title': data.get('AbstractText', 'Th√¥ng tin ph√°p lu·∫≠t'),
                        'content': data.get('Abstract'),
                        'url': data.get('AbstractURL', ''),
                        'source': 'DuckDuckGo',
                        'priority': False,
                        'confidence': confidence
                    })
                
                for topic in data.get('RelatedTopics', [])[:max_results-len(results)]:
                    if isinstance(topic, dict) and topic.get('Text'):
                        confidence = self._calculate_confidence(query, topic.get('Text', ''), topic.get('Text', ''))
                        results.append({
                            'title': topic.get('Text', '')[:80] + '...',
                            'content': topic.get('Text', ''),
                            'url': topic.get('FirstURL', ''),
                            'source': 'DuckDuckGo',
                            'priority': False,
                            'confidence': confidence
                        })
        
        except Exception as e:
            pass
        
        return results
    
    def _fallback_search(self, query, max_results):
        """T√¨m ki·∫øm d·ª± ph√≤ng"""
        return [{
            'title': 'Kh√¥ng t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ',
            'content': f'Kh√¥ng th·ªÉ t√¨m th·∫•y th√¥ng tin ch√≠nh x√°c cho "{query}". T√¥i s·∫Ω tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c ph√°p lu·∫≠t c√≥ s·∫µn v√† khuy·∫øn ngh·ªã b·∫°n tham kh·∫£o tr·ª±c ti·∫øp t·∫°i thuvienphapluat.vn ho·∫∑c li√™n h·ªá c∆° quan c√≥ th·∫©m quy·ªÅn.',
            'url': 'https://thuvienphapluat.vn',
            'source': 'H·ªá th·ªëng',
            'priority': False,
            'confidence': 0.1
        }]
    
    def _get_error_result(self, query):
        """K·∫øt qu·∫£ khi c√≥ l·ªói"""
        return [{
            'title': 'L·ªói t√¨m ki·∫øm ph√°p lu·∫≠t',
            'content': f'ƒê√£ x·∫£y ra l·ªói khi t√¨m ki·∫øm th√¥ng tin cho "{query}". Vui l√≤ng th·ª≠ l·∫°i sau ho·∫∑c tham kh·∫£o tr·ª±c ti·∫øp t·∫°i c√°c ngu·ªìn ch√≠nh th·ªëng.',
            'url': '',
            'source': 'H·ªá th·ªëng',
            'priority': False,
            'confidence': 0.0
        }]
    
    def _is_legal_content(self, title, content):
        """Ki·ªÉm tra xem c√≥ ph·∫£i n·ªôi dung ph√°p lu·∫≠t kh√¥ng"""
        legal_indicators = [
            'lu·∫≠t', 'ngh·ªã ƒë·ªãnh', 'th√¥ng t∆∞', 'quy·∫øt ƒë·ªãnh', 'ƒëi·ªÅu',
            'kho·∫£n', 'ƒëi·ªÉm', 'ch∆∞∆°ng', 'm·ª•c', 'b·ªô lu·∫≠t'
        ]
        
        text = (title + ' ' + content).lower()
        return any(indicator in text for indicator in legal_indicators)
    
    def _calculate_confidence(self, query, title, content):
        """T√≠nh ƒë·ªô tin c·∫≠y c·ªßa k·∫øt qu·∫£"""
        confidence = 0.3  # Base confidence
        
        query_words = set(query.lower().split())
        content_words = set((title + ' ' + content).lower().split())
        
        # Intersection of words
        common_words = query_words.intersection(content_words)
        if len(query_words) > 0:
            word_match_ratio = len(common_words) / len(query_words)
            confidence += word_match_ratio * 0.4
        
        # Legal structure indicators
        if re.search(r'ƒëi·ªÅu\s+\d+', content.lower()):
            confidence += 0.2
        
        if re.search(r'kho·∫£n\s+\d+', content.lower()):
            confidence += 0.1
        
        if any(domain in content.lower() for domain in ['thuvienphapluat', 'gov.vn']):
            confidence += 0.2
        
        # Specific legal document indicators
        if re.search(r'lu·∫≠t\s+kho√°ng s·∫£n', content.lower()):
            confidence += 0.3
        
        return min(confidence, 1.0)
    
    def _optimize_legal_query(self, query):
        """T·ªëi ∆∞u query cho t√¨m ki·∫øm ph√°p lu·∫≠t"""
        # Th√™m c√°c t·ª´ kh√≥a ph√°p lu·∫≠t li√™n quan
        legal_terms = ['vƒÉn b·∫£n', 'quy ƒë·ªãnh', 'h∆∞·ªõng d·∫´n', 'ph√°p lu·∫≠t']
        
        # N·∫øu query ch∆∞a c√≥ t·ª´ kh√≥a ph√°p lu·∫≠t, th√™m v√†o
        query_lower = query.lower()
        if not any(term in query_lower for term in legal_terms):
            query += ' quy ƒë·ªãnh ph√°p lu·∫≠t'
        
        return query

# Kh·ªüi t·∫°o advanced legal searcher
@st.cache_resource
def get_advanced_legal_searcher():
    return AdvancedLegalSearcher()

advanced_legal_searcher = get_advanced_legal_searcher()

def is_mineral_related(message):
    """Ki·ªÉm tra c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn kho√°ng s·∫£n kh√¥ng"""
    mineral_keywords = [
        # T·ª´ kh√≥a ch√≠nh
        'kho√°ng s·∫£n', 'khai th√°c', 'thƒÉm d√≤', 'ƒë√°', 'c√°t', 's·ªèi',
        'than', 'qu·∫∑ng', 'kim lo·∫°i', 'phi kim lo·∫°i', 'kho√°ng',
        
        # Ph√°p lu·∫≠t
        'lu·∫≠t kho√°ng s·∫£n', 'gi·∫•y ph√©p', 'c·∫•p ph√©p', 'thu·∫ø t√†i nguy√™n',
        'ph√≠ thƒÉm d√≤', 'ti·ªÅn c·∫•p quy·ªÅn', 'vi ph·∫°m h√†nh ch√≠nh',
        
        # C∆° quan
        'b·ªô t√†i nguy√™n', 's·ªü t√†i nguy√™n', 'monre', 'tn&mt',
        
        # Ho·∫°t ƒë·ªông
        'm·ªè', 'm·ªè ƒë√°', 'm·ªè c√°t', 'm·ªè than', 'quarry', 'mining'
    ]
    
    message_lower = message.lower()
    return any(keyword in message_lower for keyword in mineral_keywords)

def should_search_legal_web(message):
    """Ki·ªÉm tra c√≥ c·∫ßn t√¨m ki·∫øm ph√°p lu·∫≠t tr√™n web"""
    search_indicators = [
        'm·ªõi nh·∫•t', 'c·∫≠p nh·∫≠t', 'hi·ªán h√†nh', 'ban h√†nh', 's·ª≠a ƒë·ªïi',
        'b·ªï sung', 'thay th·∫ø', 'c√≥ hi·ªáu l·ª±c', 'quy ƒë·ªãnh m·ªõi',
        'ngh·ªã ƒë·ªãnh', 'th√¥ng t∆∞', 'lu·∫≠t', 'ph√°p lu·∫≠t'
    ]
    
    message_lower = message.lower()
    return (is_mineral_related(message) and 
            any(indicator in message_lower for indicator in search_indicators))

def create_enhanced_legal_prompt(user_message, search_results):
    """T·∫°o prompt n√¢ng cao v·ªõi k·∫øt qu·∫£ t√¨m ki·∫øm ƒë∆∞·ª£c s·∫Øp x·∫øp theo ƒë·ªô tin c·∫≠y"""
    if not search_results or not any(r.get('content') for r in search_results):
        return f"""
{user_message}

QUAN TR·ªåNG: Kh√¥ng t√¨m th·∫•y th√¥ng tin ch√≠nh x√°c t·ª´ c√°c ngu·ªìn ph√°p lu·∫≠t ch√≠nh th·ªëng. 
H√£y tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c c√≥ s·∫µn v√† L∆ØU √ù:
1. Ghi r√µ ƒë√¢y l√† th√¥ng tin tham kh·∫£o, ch∆∞a ƒë∆∞·ª£c x√°c minh t·ª´ ngu·ªìn ch√≠nh th·ªëng
2. Khuy·∫øn ngh·ªã ng∆∞·ªùi h·ªèi tham kh·∫£o tr·ª±c ti·∫øp t·∫°i thuvienphapluat.vn
3. N·∫øu l√† ƒëi·ªÅu kho·∫£n c·ª• th·ªÉ, ƒë·ªÅ xu·∫•t t√¨m ki·∫øm tr·ª±c ti·∫øp tr√™n website ch√≠nh th·ªëng
4. ƒê∆∞a ra link tr·ª±c ti·∫øp: https://thuvienphapluat.vn
"""
    
    # S·∫Øp x·∫øp k·∫øt qu·∫£ theo ƒë·ªô tin c·∫≠y v√† priority
    sorted_results = sorted(search_results, 
                          key=lambda x: (x.get('priority', False), x.get('confidence', 0)), 
                          reverse=True)
    
    legal_info = "\n\n=== TH√îNG TIN PH√ÅP LU·∫¨T T√åM KI·∫æM ===\n"
    
    high_confidence_found = False
    
    for i, result in enumerate(sorted_results, 1):
        if result.get('content'):
            priority_mark = "‚≠ê " if result.get('priority') else ""
            confidence = result.get('confidence', 0)
            confidence_mark = f"[Tin c·∫≠y: {confidence:.1f}]" if confidence > 0 else ""
            
            if confidence > 0.7:
                high_confidence_found = True
            
            legal_info += f"\n{priority_mark}Ngu·ªìn {i} ({result['source']}) {confidence_mark}:\n"
            legal_info += f"Ti√™u ƒë·ªÅ: {result['title']}\n"
            legal_info += f"N·ªôi dung: {result['content'][:600]}...\n"
            
            if result.get('url'):
                legal_info += f"URL: {result['url']}\n"
            legal_info += "---\n"
    
    confidence_instruction = ""
    if high_confidence_found:
        confidence_instruction = "C√ì NGU·ªíN TIN C·∫¨Y CAO - H√£y ∆∞u ti√™n c√°c ngu·ªìn c√≥ ƒë·ªô tin c·∫≠y > 0.7"
    else:
        confidence_instruction = "KH√îNG C√ì NGU·ªíN TIN C·∫¨Y CAO - H√£y th·∫≠n tr·ªçng khi tr√≠ch d·∫´n v√† ghi r√µ c·∫ßn x√°c minh"
    
    legal_info += f"""
{confidence_instruction}

H∆Ø·ªöNG D·∫™N TR√çCH D·∫™N CH√çNH X√ÅC:
1. ∆Øu ti√™n ngu·ªìn c√≥ ‚≠ê (ngu·ªìn ch√≠nh th·ªëng)
2. ∆Øu ti√™n k·∫øt qu·∫£ c√≥ ƒë·ªô tin c·∫≠y cao (> 0.7)
3. PH·∫¢I tr√≠ch d·∫´n c·ª• th·ªÉ: "Theo ƒêi·ªÅu X kho·∫£n Y Lu·∫≠t/Ngh·ªã ƒë·ªãnh s·ªë Z..."
4. N·∫øu ƒë·ªô tin c·∫≠y th·∫•p: "Th√¥ng tin tham kh·∫£o t·ª´ [ngu·ªìn], c·∫ßn x√°c minh th√™m"
5. Lu√¥n khuy·∫øn ngh·ªã: "ƒê·ªÉ c√≥ th√¥ng tin ch√≠nh x√°c nh·∫•t, vui l√≤ng tham kh·∫£o t·∫°i thuvienphapluat.vn"
6. N·∫øu l√† ƒëi·ªÅu kho·∫£n c·ª• th·ªÉ nh∆∞ng kh√¥ng t√¨m th·∫•y: "Kh√¥ng t√¨m th·∫•y n·ªôi dung ch√≠nh x√°c c·ªßa ƒëi·ªÅu n√†y. Khuy·∫øn ngh·ªã t√¨m ki·∫øm tr·ª±c ti·∫øp tr√™n thuvienphapluat.vn"

=== K·∫æT TH√öC TH√îNG TIN PH√ÅP LU·∫¨T ===

"""
    
    return legal_info + f"C√¢u h·ªèi: {user_message}"

# Main UI
def main():
    # Custom CSS cho giao di·ªán bot tr√°i, user ph·∫£i
    st.markdown(
        """
        <style>
        .assistant-message {
            background: #f0f8ff;
            padding: 15px;
            border-radius: 15px;
            margin: 10px 0;
            max-width: 80%;
            border-left: 4px solid #4CAF50;
        }
        .assistant-message::before { 
            content: "‚öñÔ∏è Tr·ª£ l√Ω Ph√°p ch·∫ø: "; 
            font-weight: bold; 
            color: #2E7D32;
        }
        
        .user-message {
            background: #e3f2fd;
            padding: 15px;
            border-radius: 15px;
            margin: 10px 0 10px auto;
            max-width: 80%;
            text-align: right;
            border-right: 4px solid #2196F3;
        }
        .user-message::before { 
            content: "üë§ B·∫°n: "; 
            font-weight: bold; 
            color: #1976D2;
        }
        
        .stats-box {
            background: #f5f5f5;
            padding: 10px;
            border-radius: 8px;
            border: 1px solid #ddd;
            margin: 5px 0;
        }
        
        .cost-highlight {
            color: #d32f2f;
            font-weight: bold;
        }
        
        .token-highlight {
            color: #1976d2;
            font-weight: bold;
        }
        </style>
        """,
        unsafe_allow_html=True
    )
    
    # Header
    st.markdown(
        """
        <div style="text-align: center; padding: 20px; background: linear-gradient(90deg, #2E7D32, #4CAF50); border-radius: 10px; margin-bottom: 20px;">
            <h1 style="color: white; margin: 0;">‚öñÔ∏è Tr·ª£ l√Ω Ph√°p ch·∫ø Kho√°ng s·∫£n</h1>
            <p style="color: #E8F5E8; margin: 5px 0 0 0;">Chuy√™n gia t∆∞ v·∫•n Qu·∫£n l√Ω Nh√† n∆∞·ªõc v·ªÅ Kho√°ng s·∫£n t·∫°i Vi·ªát Nam</p>
        </div>
        """,
        unsafe_allow_html=True
    )
    
    # Sidebar v·ªõi th·ªëng k√™ chi ti·∫øt
    with st.sidebar:
        st.markdown("### ‚öôÔ∏è C√†i ƒë·∫∑t h·ªá th·ªëng")
        
        # Toggle web search cho ph√°p lu·∫≠t
        legal_search_enabled = st.toggle("üîç T√¨m ki·∫øm ph√°p lu·∫≠t online", value=True)
        
        # Model selection
        model_options = ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo-preview"]
        selected_model = st.selectbox("ü§ñ Ch·ªçn model AI:", model_options, index=0)
        
        # Temperature setting
        temperature = st.slider("üå°Ô∏è ƒê·ªô s√°ng t·∫°o (Temperature):", 0.0, 1.0, 0.3, 0.1,
                               help="0.0 = Ch√≠nh x√°c, 1.0 = S√°ng t·∫°o")
        
        st.markdown("---")
        
        # Th·ªëng k√™ token v√† chi ph√≠
        st.markdown("### üìä Th·ªëng k√™ s·ª≠ d·ª•ng")
        
        stats = token_counter.get_stats_display()
        
        # Token stats
        st.markdown('<div class="stats-box">', unsafe_allow_html=True)
        st.metric("üéØ T·ªïng Token", f"{stats['total_tokens']:,}")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("üì• Input", f"{stats['input_tokens']:,}")
        with col2:
            st.metric("üì§ Output", f"{stats['output_tokens']:,}")
        st.markdown('</div>', unsafe_allow_html=True)
        
        # Cost stats
        st.markdown('<div class="stats-box">', unsafe_allow_html=True)
        st.metric("üí∞ Chi ph√≠ (USD)", f"${stats['total_cost_usd']:.4f}")
        st.metric("üí∏ Chi ph√≠ (VND)", f"{stats['total_cost_vnd']:,.0f}ƒë")
        st.markdown('</div>', unsafe_allow_html=True)
        
        # Session stats
        st.markdown('<div class="stats-box">', unsafe_allow_html=True)
        st.metric("üìû S·ªë l∆∞·ª£t h·ªèi", stats['requests'])
        duration = str(stats['session_duration']).split('.')[0]
        st.metric("‚è±Ô∏è Th·ªùi gian", duration)
        st.markdown('</div>', unsafe_allow_html=True)
        
        # Reset button
        if st.button("üîÑ Reset th·ªëng k√™", use_container_width=True):
            token_counter.reset_stats()
            st.rerun()
        
        st.markdown("---")
        
        # Clear chat button
        if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
            st.session_state.messages = [
                {"role": "system", "content": rfile("01.system_trainning.txt")},
                {"role": "assistant", "content": rfile("02.assistant.txt")}
            ]
            st.rerun()
        
        st.markdown("---")
        st.markdown("### üìö Lƒ©nh v·ª±c chuy√™n m√¥n")
        st.markdown("‚Ä¢ Lu·∫≠t Kho√°ng s·∫£n 2017")
        st.markdown("‚Ä¢ Ngh·ªã ƒë·ªãnh h∆∞·ªõng d·∫´n")
        st.markdown("‚Ä¢ Th√¥ng t∆∞ B·ªô TN&MT")
        st.markdown("‚Ä¢ Th·ªß t·ª•c c·∫•p ph√©p")
        st.markdown("‚Ä¢ Thu·∫ø, ph√≠ kho√°ng s·∫£n")
        st.markdown("‚Ä¢ X·ª≠ ph·∫°t vi ph·∫°m")
        st.markdown("‚Ä¢ B·∫£o v·ªá m√¥i tr∆∞·ªùng")
    
    # Check API key
    if not st.secrets.get("OPENAI_API_KEY"):
        st.error("‚ùå Ch∆∞a c·∫•u h√¨nh OPENAI_API_KEY trong secrets!")
        st.stop()
    
    # Initialize OpenAI client
    try:
        client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
    except Exception as e:
        st.error(f"‚ùå L·ªói kh·ªüi t·∫°o OpenAI client: {str(e)}")
        st.stop()
    
    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "system", "content": rfile("01.system_trainning.txt")},
            {"role": "assistant", "content": rfile("02.assistant.txt")}
        ]
    
    # Display chat history v·ªõi style t√πy ch·ªânh
    for message in st.session_state.messages:
        if message["role"] == "assistant":
            st.markdown(f'<div class="assistant-message">{message["content"]}</div>', 
                       unsafe_allow_html=True)
        elif message["role"] == "user":
            st.markdown(f'<div class="user-message">{message["content"]}</div>', 
                       unsafe_allow_html=True)
    
    # Chat input
    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi v·ªÅ ph√°p lu·∫≠t kho√°ng s·∫£n..."):
        # Ki·ªÉm tra c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn kho√°ng s·∫£n kh√¥ng
        if not is_mineral_related(prompt):
            st.session_state.messages.append({"role": "user", "content": prompt})
            st.markdown(f'<div class="user-message">{prompt}</div>', unsafe_allow_html=True)
            
            # Ph·∫£n h·ªìi t·ª´ ch·ªëi l·ªãch s·ª±
            polite_refusal = """Xin l·ªói, t√¥i l√† tr·ª£ l√Ω chuy√™n v·ªÅ **ph√°p lu·∫≠t kho√°ng s·∫£n** t·∫°i Vi·ªát Nam. 

T√¥i ch·ªâ c√≥ th·ªÉ t∆∞ v·∫•n v·ªÅ c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn:
- üèîÔ∏è Lu·∫≠t Kho√°ng s·∫£n v√† vƒÉn b·∫£n h∆∞·ªõng d·∫´n
- ‚öñÔ∏è Th·ªß t·ª•c c·∫•p ph√©p thƒÉm d√≤, khai th√°c
- üí∞ Thu·∫ø, ph√≠ li√™n quan ƒë·∫øn kho√°ng s·∫£n  
- üå± B·∫£o v·ªá m√¥i tr∆∞·ªùng trong ho·∫°t ƒë·ªông kho√°ng s·∫£n
- ‚ö†Ô∏è X·ª≠ ph·∫°t vi ph·∫°m h√†nh ch√≠nh

B·∫°n c√≥ th·ªÉ h·ªèi t√¥i v·ªÅ nh·ªØng v·∫•n ƒë·ªÅ n√†y kh√¥ng? V√≠ d·ª•:
- "Th·ªß t·ª•c xin ph√©p khai th√°c ƒë√° nh∆∞ th·∫ø n√†o?"
- "M·ª©c thu·∫ø t√†i nguy√™n hi·ªán t·∫°i ra sao?"
- "Vi ph·∫°m trong khai th√°c kho√°ng s·∫£n b·ªã ph·∫°t nh∆∞ th·∫ø n√†o?"

T√¥i s·∫µn s√†ng h·ªó tr·ª£ b·∫°n! üòä"""
            
            st.session_state.messages.append({"role": "assistant", "content": polite_refusal})
            st.markdown(f'<div class="assistant-message">{polite_refusal}</div>', 
                       unsafe_allow_html=True)
            return
        
        # Add user message
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.markdown(f'<div class="user-message">{prompt}</div>', unsafe_allow_html=True)
        
        # Process response
        with st.spinner("ü§î ƒêang ph√¢n t√≠ch c√¢u h·ªèi ph√°p lu·∫≠t..."):
            # Check if legal web search is needed
            search_results = []
            final_prompt = prompt
            
            if legal_search_enabled and should_search_legal_web(prompt):
                with st.status("üîç ƒêang t√¨m ki·∫øm vƒÉn b·∫£n ph√°p lu·∫≠t ch√≠nh x√°c...", expanded=False) as status:
                    search_results = advanced_legal_searcher.search(prompt, max_results=3)
                    
                    if search_results and any(r.get('content') for r in search_results):
                        # ƒê·∫øm ngu·ªìn ∆∞u ti√™n v√† ƒë·ªô tin c·∫≠y cao
                        priority_count = sum(1 for r in search_results if r.get('priority'))
                        high_confidence_count = sum(1 for r in search_results if r.get('confidence', 0) > 0.7)
                        
                        if high_confidence_count > 0:
                            st.success(f"‚úÖ T√¨m th·∫•y {len(search_results)} k·∫øt qu·∫£ ({priority_count} ngu·ªìn ∆∞u ti√™n, {high_confidence_count} tin c·∫≠y cao)")
                        else:
                            st.warning(f"‚ö†Ô∏è T√¨m th·∫•y {len(search_results)} k·∫øt qu·∫£ ({priority_count} ngu·ªìn ∆∞u ti√™n) - ƒê·ªô tin c·∫≠y ch∆∞a cao")
                        
                        # Hi·ªÉn th·ªã k·∫øt qu·∫£ v·ªõi confidence scores
                        for i, result in enumerate(search_results, 1):
                            if result.get('content'):
                                priority_mark = "‚≠ê " if result.get('priority') else ""
                                confidence = result.get('confidence', 0)
                                confidence_color = "üü¢" if confidence > 0.7 else "üü°" if confidence > 0.4 else "üî¥"
                                
                                st.write(f"**{priority_mark}{i}. {result['source']}** {confidence_color} [{confidence:.2f}]: {result['title'][:50]}...")
                        
                        final_prompt = create_enhanced_legal_prompt(prompt, search_results)
                        status.update(label="‚úÖ Ho√†n t·∫•t t√¨m ki·∫øm ph√°p lu·∫≠t ch√≠nh x√°c", state="complete", expanded=False)
                    else:
                        st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y vƒÉn b·∫£n ph√°p lu·∫≠t li√™n quan - S·∫Ω tr·∫£ l·ªùi t·ª´ ki·∫øn th·ª©c c√≥ s·∫µn")
                        status.update(label="‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y vƒÉn b·∫£n li√™n quan", state="complete", expanded=False)
            
            # ƒê·∫øm input tokens
            messages_for_api = [
                msg for msg in st.session_state.messages[:-1] 
                if msg["role"] != "system" or msg == st.session_state.messages[0]
            ]
            messages_for_api.append({"role": "user", "content": final_prompt})
            
            input_text = "\n".join([msg["content"] for msg in messages_for_api])
            input_tokens = token_counter.count_tokens(input_text, selected_model)
            
            # Generate AI response
            try:
                response = ""
                
                stream = client.chat.completions.create(
                    model=selected_model,
                    messages=messages_for_api,
                    stream=True,
                    temperature=temperature,
                    max_tokens=2000
                )
                
                # Container cho response
                response_container = st.empty()
                
                for chunk in stream:
                    if chunk.choices and chunk.choices[0].delta.content:
                        response += chunk.choices[0].delta.content
                        # Update response hi·ªÉn th·ªã v·ªõi style
                        response_container.markdown(
                            f'<div class="assistant-message">{response}‚ñå</div>', 
                            unsafe_allow_html=True
                        )
                
                # Final response
                response_container.markdown(
                    f'<div class="assistant-message">{response}</div>', 
                    unsafe_allow_html=True
                )
                
                # ƒê·∫øm output tokens v√† c·∫≠p nh·∫≠t stats
                output_tokens = token_counter.count_tokens(response, selected_model)
                token_counter.update_stats(input_tokens, output_tokens, selected_model)
                
                # Hi·ªÉn th·ªã stats c·ªßa request n√†y
                with st.expander("üìä Th·ªëng k√™ request n√†y"):
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Input tokens", f"{input_tokens:,}")
                    with col2:
                        st.metric("Output tokens", f"{output_tokens:,}")
                    with col3:
                        cost = token_counter.calculate_cost(input_tokens, output_tokens, selected_model)
                        st.metric("Chi ph√≠", f"${cost:.4f}")
                
            except Exception as e:
                error_msg = f"‚ùå L·ªói h·ªá th·ªëng: {str(e)}"
                st.markdown(f'<div class="assistant-message">{error_msg}</div>', 
                           unsafe_allow_html=True)
                response = error_msg
        
        # Add assistant response to history
        st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main()