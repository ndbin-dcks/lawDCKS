import streamlit as st
from openai import OpenAI
import requests
import json
from datetime import datetime
import re
from urllib.parse import quote
import time
import os
import traceback

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="‚öñÔ∏è Tr·ª£ l√Ω Ph√°p ch·∫ø Kho√°ng s·∫£n Vi·ªát Nam",
    page_icon="‚öñÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Model pricing (USD per 1K tokens)
MODEL_PRICING = {
    "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
    "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002},
    "gpt-4": {"input": 0.03, "output": 0.06},
    "gpt-4-turbo-preview": {"input": 0.01, "output": 0.03}
}

def init_session_state():
    """Kh·ªüi t·∫°o session state an to√†n"""
    if "token_stats" not in st.session_state:
        st.session_state.token_stats = {
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "total_cost": 0.0,
            "session_start": datetime.now(),
            "request_count": 0
        }
    
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "system", "content": get_strict_system_prompt()},
            {"role": "assistant", "content": get_welcome_message()}
        ]

def safe_get_stats():
    """L·∫•y stats m·ªôt c√°ch an to√†n"""
    try:
        init_session_state()
        stats = st.session_state.token_stats
        total_tokens = stats["total_input_tokens"] + stats["total_output_tokens"]
        
        return {
            "total_tokens": total_tokens,
            "input_tokens": stats["total_input_tokens"],
            "output_tokens": stats["total_output_tokens"],
            "total_cost_usd": stats["total_cost"],
            "total_cost_vnd": stats["total_cost"] * 24000,
            "requests": stats["request_count"],
            "session_duration": datetime.now() - stats["session_start"]
        }
    except Exception as e:
        return {
            "total_tokens": 0,
            "input_tokens": 0,
            "output_tokens": 0,
            "total_cost_usd": 0.0,
            "total_cost_vnd": 0.0,
            "requests": 0,
            "session_duration": datetime.now() - datetime.now()
        }

def update_stats(input_tokens, output_tokens, model):
    """C·∫≠p nh·∫≠t stats an to√†n"""
    try:
        init_session_state()
        
        if model not in MODEL_PRICING:
            model = "gpt-4o-mini"
        
        pricing = MODEL_PRICING[model]
        cost = (input_tokens / 1000) * pricing["input"] + (output_tokens / 1000) * pricing["output"]
        
        st.session_state.token_stats["total_input_tokens"] += input_tokens
        st.session_state.token_stats["total_output_tokens"] += output_tokens
        st.session_state.token_stats["total_cost"] += cost
        st.session_state.token_stats["request_count"] += 1
        
    except Exception as e:
        st.error(f"L·ªói c·∫≠p nh·∫≠t stats: {e}")

def count_tokens(text):
    """∆Ø·ªõc t√≠nh s·ªë token ƒë∆°n gi·∫£n"""
    return len(str(text)) // 4

def get_strict_system_prompt():
    """System prompt nghi√™m ng·∫∑t ngƒÉn hallucination"""
    try:
        with open("01.system_trainning.txt", "r", encoding="utf-8") as file:
            base_prompt = file.read()
            # Th√™m strict instructions v√†o cu·ªëi
            return base_prompt + """

üö´ H∆Ø·ªöNG D·∫™N NGHI√äM NG·∫∂T B·ªî SUNG:

TUY·ªÜT ƒê·ªêI KH√îNG ƒê∆Ø·ª¢C:
1. B·ªãa ƒë·∫∑t s·ªë lu·∫≠t, s·ªë ƒëi·ªÅu, s·ªë kho·∫£n n·∫øu kh√¥ng c√≥ trong th√¥ng tin t√¨m ki·∫øm
2. Tr√≠ch d·∫´n c·ª• th·ªÉ c√°c ƒëi·ªÅu kho·∫£n ph√°p lu·∫≠t m√† kh√¥ng c√≥ ngu·ªìn x√°c th·ª±c
3. ƒê∆∞a ra th√¥ng tin chi ti·∫øt v·ªÅ n·ªôi dung lu·∫≠t n·∫øu kh√¥ng ch·∫Øc ch·∫Øn 100%
4. S·ª≠ d·ª•ng ki·∫øn th·ª©c c≈© v·ªÅ ph√°p lu·∫≠t m√† kh√¥ng c√≥ x√°c nh·∫≠n t·ª´ ngu·ªìn hi·ªán t·∫°i

CH·ªà ƒê∆Ø·ª¢C:
1. Tr√≠ch d·∫´n th√¥ng tin C√ì TRONG k·∫øt qu·∫£ t√¨m ki·∫øm ƒë∆∞·ª£c cung c·∫•p
2. ƒê∆∞a ra c√°c nguy√™n t·∫Øc chung v·ªÅ ph√°p lu·∫≠t kho√°ng s·∫£n
3. H∆∞·ªõng d·∫´n ng∆∞·ªùi h·ªèi tham kh·∫£o ngu·ªìn ch√≠nh th·ªëng
4. N√≥i r√µ khi th√¥ng tin kh√¥ng ƒë·∫ßy ƒë·ªß ho·∫∑c c·∫ßn ki·ªÉm tra th√™m

LU√îN ∆∞u ti√™n an to√†n th√¥ng tin h∆°n vi·ªác ƒë∆∞a ra c√¢u tr·∫£ l·ªùi chi ti·∫øt."""
            
    except FileNotFoundError:
        return """B·∫°n l√† chuy√™n gia ph√°p ch·∫ø v·ªÅ qu·∫£n l√Ω nh√† n∆∞·ªõc trong lƒ©nh v·ª±c kho√°ng s·∫£n t·∫°i Vi·ªát Nam.

‚öñÔ∏è NGUY√äN T·∫ÆC L√ÄM VI·ªÜC NGHI√äM NG·∫∂T:

üö´ TUY·ªÜT ƒê·ªêI KH√îNG ƒê∆Ø·ª¢C:
1. B·ªãa ƒë·∫∑t s·ªë lu·∫≠t, s·ªë ƒëi·ªÅu, s·ªë kho·∫£n n·∫øu kh√¥ng c√≥ trong th√¥ng tin t√¨m ki·∫øm
2. Tr√≠ch d·∫´n c·ª• th·ªÉ c√°c ƒëi·ªÅu kho·∫£n ph√°p lu·∫≠t m√† kh√¥ng c√≥ ngu·ªìn x√°c th·ª±c
3. ƒê∆∞a ra th√¥ng tin chi ti·∫øt v·ªÅ n·ªôi dung lu·∫≠t n·∫øu kh√¥ng ch·∫Øc ch·∫Øn 100%
4. S·ª≠ d·ª•ng ki·∫øn th·ª©c c≈© v·ªÅ ph√°p lu·∫≠t m√† kh√¥ng c√≥ x√°c nh·∫≠n t·ª´ ngu·ªìn hi·ªán t·∫°i

‚úÖ CH·ªà ƒê∆Ø·ª¢C:
1. Tr√≠ch d·∫´n th√¥ng tin C√ì TRONG k·∫øt qu·∫£ t√¨m ki·∫øm ƒë∆∞·ª£c cung c·∫•p
2. ƒê∆∞a ra c√°c nguy√™n t·∫Øc chung v·ªÅ ph√°p lu·∫≠t kho√°ng s·∫£n
3. H∆∞·ªõng d·∫´n ng∆∞·ªùi h·ªèi tham kh·∫£o ngu·ªìn ch√≠nh th·ªëng
4. N√≥i r√µ khi th√¥ng tin kh√¥ng ƒë·∫ßy ƒë·ªß ho·∫∑c c·∫ßn ki·ªÉm tra th√™m

üéØ C√ÅCH TR·∫¢ L·ªúI AN TO√ÄN:
- Khi c√≥ th√¥ng tin t·ª´ search: "D·ª±a theo th√¥ng tin t√¨m ki·∫øm t·ª´ [ngu·ªìn]..."
- Khi kh√¥ng ch·∫Øc ch·∫Øn: "Th√¥ng tin n√†y c·∫ßn ƒë∆∞·ª£c ki·ªÉm tra t·∫°i thuvienphapluat.vn"
- Khi th√¥ng tin kh√¥ng ƒë·ªß: "T√¥i kh√¥ng c√≥ ƒë·ªß th√¥ng tin ch√≠nh x√°c ƒë·ªÉ tr·∫£ l·ªùi chi ti·∫øt"

QUAN TR·ªåNG: An to√†n th√¥ng tin ph√°p lu·∫≠t quan tr·ªçng h∆°n vi·ªác ƒë∆∞a ra c√¢u tr·∫£ l·ªùi chi ti·∫øt."""

def get_welcome_message():
    """L·∫•y tin nh·∫Øn ch√†o m·ª´ng"""
    try:
        with open("02.assistant.txt", "r", encoding="utf-8") as file:
            return file.read()
    except FileNotFoundError:
        return """Xin ch√†o! ‚öñÔ∏è 

T√¥i l√† **Tr·ª£ l√Ω Ph√°p ch·∫ø chuy√™n v·ªÅ Qu·∫£n l√Ω Nh√† n∆∞·ªõc trong lƒ©nh v·ª±c Kho√°ng s·∫£n t·∫°i Vi·ªát Nam**.

üèîÔ∏è **T√¥i c√≥ th·ªÉ h·ªó tr·ª£ b·∫°n v·ªÅ:**

‚úÖ **Ph√°p lu·∫≠t Kho√°ng s·∫£n:**
   ‚Ä¢ Lu·∫≠t Kho√°ng s·∫£n v√† c√°c vƒÉn b·∫£n h∆∞·ªõng d·∫´n
   ‚Ä¢ Ngh·ªã ƒë·ªãnh, Th√¥ng t∆∞ c·ªßa B·ªô T√†i nguy√™n v√† M√¥i tr∆∞·ªùng

‚úÖ **Th·ªß t·ª•c h√†nh ch√≠nh:**
   ‚Ä¢ C·∫•p Gi·∫•y ph√©p thƒÉm d√≤, khai th√°c kho√°ng s·∫£n
   ‚Ä¢ Gia h·∫°n, s·ª≠a ƒë·ªïi, b·ªï sung gi·∫•y ph√©p
   ‚Ä¢ Th·ªß t·ª•c ƒë√≥ng c·ª≠a m·ªè

üéØ **L∆∞u √Ω quan tr·ªçng:** 
T√¥i ch·ªâ t∆∞ v·∫•n v·ªÅ lƒ©nh v·ª±c **Kho√°ng s·∫£n**. ƒê·ªÉ c√≥ th√¥ng tin ch√≠nh x√°c nh·∫•t, b·∫°n n√™n tham kh·∫£o tr·ª±c ti·∫øp t·∫°i **thuvienphapluat.vn**.

**B·∫°n c√≥ th·∫Øc m·∫Øc g√¨ v·ªÅ ph√°p lu·∫≠t Kho√°ng s·∫£n kh√¥ng?** ü§î"""

def get_default_model():
    """L·∫•y model m·∫∑c ƒë·ªãnh"""
    try:
        with open("module_chatgpt.txt", "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "gpt-4o-mini"

def is_mineral_related(message):
    """Ki·ªÉm tra c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn kho√°ng s·∫£n kh√¥ng"""
    mineral_keywords = [
        'kho√°ng s·∫£n', 'khai th√°c', 'thƒÉm d√≤', 'ƒë√°', 'c√°t', 's·ªèi',
        'than', 'qu·∫∑ng', 'kim lo·∫°i', 'phi kim lo·∫°i', 'kho√°ng',
        'lu·∫≠t kho√°ng s·∫£n', 'gi·∫•y ph√©p', 'c·∫•p ph√©p', 'thu·∫ø t√†i nguy√™n',
        'ph√≠ thƒÉm d√≤', 'ti·ªÅn c·∫•p quy·ªÅn', 'vi ph·∫°m h√†nh ch√≠nh',
        'b·ªô t√†i nguy√™n', 's·ªü t√†i nguy√™n', 'monre', 'tn&mt',
        'm·ªè', 'm·ªè ƒë√°', 'm·ªè c√°t', 'm·ªè than', 'quarry', 'mining',
        'thu h·ªìi gi·∫•y ph√©p'
    ]
    
    message_lower = message.lower()
    return any(keyword in message_lower for keyword in mineral_keywords)

def should_search_web(message):
    """Ki·ªÉm tra c√≥ c·∫ßn t√¨m ki·∫øm web kh√¥ng"""
    search_indicators = [
        'm·ªõi nh·∫•t', 'c·∫≠p nh·∫≠t', 'hi·ªán h√†nh', 'ban h√†nh', 's·ª≠a ƒë·ªïi',
        'b·ªï sung', 'thay th·∫ø', 'c√≥ hi·ªáu l·ª±c', 'quy ƒë·ªãnh m·ªõi',
        'ngh·ªã ƒë·ªãnh', 'th√¥ng t∆∞', 'lu·∫≠t', 'ph√°p lu·∫≠t', 'ƒëi·ªÅu',
        'khi n√†o', 'tr∆∞·ªùng h·ª£p n√†o', 'ƒëi·ªÅu ki·ªán', 'thu h·ªìi'
    ]
    
    message_lower = message.lower()
    return (is_mineral_related(message) and 
            any(indicator in message_lower for indicator in search_indicators))

# =================== SEARCH FUNCTIONS ===================

def is_high_quality_legal_content(title, content, url=""):
    """Ki·ªÉm tra n·ªôi dung c√≥ ph·∫£i vƒÉn b·∫£n ph√°p lu·∫≠t ch·∫•t l∆∞·ª£ng cao kh√¥ng"""
    
    # 1. Ki·ªÉm tra ngu·ªìn uy t√≠n
    trusted_domains = ['thuvienphapluat.vn', 'monre.gov.vn', 'moj.gov.vn', 'chinhphu.vn']
    is_trusted_source = any(domain in url.lower() for domain in trusted_domains)
    
    # 2. Ki·ªÉm tra c·∫•u tr√∫c vƒÉn b·∫£n ph√°p lu·∫≠t
    legal_structure_patterns = [
        r'(?:lu·∫≠t|ngh·ªã ƒë·ªãnh|th√¥ng t∆∞|quy·∫øt ƒë·ªãnh)\s+(?:s·ªë\s*)?\d+',
        r'ƒëi·ªÅu\s+\d+',
        r'kho·∫£n\s+\d+',
        r'ch∆∞∆°ng\s+[ivx\d]+',
        r'm·ª•c\s+\d+'
    ]
    
    text = (title + ' ' + content).lower()
    has_legal_structure = sum(1 for pattern in legal_structure_patterns 
                            if re.search(pattern, text)) >= 2
    
    # 3. Ki·ªÉm tra t·ª´ kh√≥a kho√°ng s·∫£n c·ª• th·ªÉ
    mineral_legal_terms = [
        'lu·∫≠t kho√°ng s·∫£n', 'khai th√°c kho√°ng s·∫£n', 'thƒÉm d√≤ kho√°ng s·∫£n',
        'gi·∫•y ph√©p khai th√°c', 'gi·∫•y ph√©p thƒÉm d√≤', 'thu·∫ø t√†i nguy√™n',
        'b·ªô t√†i nguy√™n', 's·ªü t√†i nguy√™n', 'thu h·ªìi gi·∫•y ph√©p'
    ]
    
    has_mineral_terms = any(term in text for term in mineral_legal_terms)
    
    # 4. Lo·∫°i b·ªè n·ªôi dung spam/kh√¥ng ph√π h·ª£p
    spam_indicators = ['qu·∫£ng c√°o', 'b√°n h√†ng', 'tuy·ªÉn d·ª•ng', '404', 'error']
    has_spam = any(spam in text for spam in spam_indicators)
    
    # 5. Ki·ªÉm tra ƒë·ªô d√†i n·ªôi dung
    has_sufficient_content = len(content.strip()) > 100
    
    # T√≠nh ƒëi·ªÉm t·ªïng
    score = 0
    if is_trusted_source: score += 3
    if has_legal_structure: score += 2  
    if has_mineral_terms: score += 2
    if has_sufficient_content: score += 1
    if has_spam: score -= 3
    
    return score >= 4

def calculate_improved_confidence(query, title, content, url=""):
    """T√≠nh confidence score c·∫£i ti·∫øn v·ªõi nhi·ªÅu y·∫øu t·ªë"""
    
    confidence = 0.0
    query_lower = query.lower()
    text_lower = (title + ' ' + content).lower()
    
    # 1. Exact phrase matching (25%)
    if query_lower in text_lower:
        confidence += 0.25
    
    # 2. Word overlap (20%)
    query_words = set(query_lower.split())
    text_words = set(text_lower.split())
    if len(query_words) > 0:
        overlap_ratio = len(query_words.intersection(text_words)) / len(query_words)
        confidence += overlap_ratio * 0.20
    
    # 3. Legal document indicators (20%)
    legal_patterns = [
        (r'lu·∫≠t\s+kho√°ng s·∫£n', 0.15),
        (r'lu·∫≠t\s+(?:s·ªë\s*)?\d+.*kho√°ng s·∫£n', 0.12),
        (r'ngh·ªã ƒë·ªãnh\s+(?:s·ªë\s*)?\d+.*kho√°ng s·∫£n', 0.10),
        (r'th√¥ng t∆∞\s+(?:s·ªë\s*)?\d+.*kho√°ng s·∫£n', 0.08),
        (r'ƒëi·ªÅu\s+\d+', 0.05),
        (r'kho·∫£n\s+\d+', 0.03)
    ]
    
    for pattern, weight in legal_patterns:
        if re.search(pattern, text_lower):
            confidence += weight
            break  # Ch·ªâ t√≠nh pattern c√≥ tr·ªçng s·ªë cao nh·∫•t
    
    # 4. Source reliability (20%)
    source_scores = {
        'thuvienphapluat.vn': 0.20,
        'monre.gov.vn': 0.18,
        'chinhphu.vn': 0.15,
        'moj.gov.vn': 0.12,
        'gov.vn': 0.08
    }
    
    for domain, score in source_scores.items():
        if domain in url.lower():
            confidence += score
            break
    
    # 5. Title relevance bonus (10%)
    title_words = set(title.lower().split())
    title_overlap = len(query_words.intersection(title_words)) / len(query_words) if query_words else 0
    confidence += title_overlap * 0.10
    
    # 6. Content quality (5%)
    if len(content) > 200:
        confidence += 0.05
    elif len(content) > 100:
        confidence += 0.025
    
    # Penalties
    if any(spam in text_lower for spam in ['404', 'error', 'kh√¥ng t√¨m th·∫•y']):
        confidence *= 0.3
    
    return min(confidence, 1.0)

def extract_document_type(title):
    """Tr√≠ch xu·∫•t lo·∫°i vƒÉn b·∫£n t·ª´ ti√™u ƒë·ªÅ"""
    title_lower = title.lower()
    
    if re.search(r'lu·∫≠t\s+(?:s·ªë\s*)?\d+', title_lower):
        return 'Lu·∫≠t'
    elif re.search(r'ngh·ªã ƒë·ªãnh\s+(?:s·ªë\s*)?\d+', title_lower):
        return 'Ngh·ªã ƒë·ªãnh'
    elif re.search(r'th√¥ng t∆∞\s+(?:s·ªë\s*)?\d+', title_lower):
        return 'Th√¥ng t∆∞'
    elif re.search(r'quy·∫øt ƒë·ªãnh\s+(?:s·ªë\s*)?\d+', title_lower):
        return 'Quy·∫øt ƒë·ªãnh'
    else:
        return 'VƒÉn b·∫£n'

def remove_duplicate_results(results):
    """Lo·∫°i b·ªè k·∫øt qu·∫£ tr√πng l·∫∑p"""
    unique_results = []
    seen_urls = set()
    seen_titles = set()
    
    for result in results:
        url = result.get('url', '')
        title = result.get('title', '').lower().strip()
        
        # Skip if URL or title already seen
        if url in seen_urls or title in seen_titles:
            continue
            
        # Skip if title too similar to existing ones
        is_duplicate = False
        for seen_title in seen_titles:
            if calculate_string_similarity(title, seen_title) > 0.8:
                is_duplicate = True
                break
        
        if not is_duplicate:
            seen_urls.add(url)
            seen_titles.add(title)
            unique_results.append(result)
    
    return unique_results

def calculate_string_similarity(str1, str2):
    """T√≠nh similarity gi·ªØa 2 string"""
    if not str1 or not str2:
        return 0.0
    
    words1 = set(str1.split())
    words2 = set(str2.split())
    
    if not words1 or not words2:
        return 0.0
    
    intersection = len(words1.intersection(words2))
    union = len(words1.union(words2))
    
    return intersection / union if union > 0 else 0.0

def advanced_web_search_improved(query, max_results=5):
    """Improved web search v·ªõi better accuracy"""
    results = []
    
    try:
        # Search strategy 1: Direct thuvienphapluat.vn focus
        search_queries = [
            f'site:thuvienphapluat.vn "{query}" lu·∫≠t kho√°ng s·∫£n',
            f'site:thuvienphapluat.vn kho√°ng s·∫£n "{query}"',
            f'site:monre.gov.vn "{query}" kho√°ng s·∫£n',
            f'"lu·∫≠t kho√°ng s·∫£n" "{query}" site:thuvienphapluat.vn'
        ]
        
        for search_query in search_queries:
            if len(results) >= max_results:
                break
                
            try:
                params = {
                    'q': search_query,
                    'format': 'json',
                    'no_html': '1',
                    'skip_disambig': '1'
                }
                
                response = requests.get("https://api.duckduckgo.com/", 
                                      params=params, timeout=8)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # Process Abstract v·ªõi validation
                    if data.get('Abstract') and len(data['Abstract']) > 50:
                        title = data.get('AbstractText', 'Th√¥ng tin ph√°p lu·∫≠t')
                        content = data.get('Abstract')
                        url = data.get('AbstractURL', '')
                        
                        if is_high_quality_legal_content(title, content, url):
                            confidence = calculate_improved_confidence(query, title, content, url)
                            
                            results.append({
                                'title': title,
                                'content': content,
                                'url': url,
                                'source': 'Th∆∞ vi·ªán Ph√°p lu·∫≠t' if 'thuvienphapluat' in url else 'C∆° quan nh√† n∆∞·ªõc',
                                'priority': True,
                                'confidence': confidence,
                                'document_type': extract_document_type(title)
                            })
                    
                    # Process RelatedTopics v·ªõi strict filtering
                    for topic in data.get('RelatedTopics', []):
                        if len(results) >= max_results:
                            break
                            
                        if isinstance(topic, dict) and topic.get('Text'):
                            title = topic.get('Text', '')[:100]
                            content = topic.get('Text', '')
                            url = topic.get('FirstURL', '')
                            
                            if (is_high_quality_legal_content(title, content, url) and
                                len(content) > 100):
                                
                                confidence = calculate_improved_confidence(query, title, content, url)
                                
                                results.append({
                                    'title': title + '...',
                                    'content': content,
                                    'url': url,
                                    'source': 'Th∆∞ vi·ªán Ph√°p lu·∫≠t' if 'thuvienphapluat' in url else 'T√¨m ki·∫øm web',
                                    'priority': 'thuvienphapluat' in url,
                                    'confidence': confidence,
                                    'document_type': extract_document_type(title)
                                })
                
                time.sleep(0.5)  # Rate limiting
                
            except Exception as e:
                continue
    
    except Exception as e:
        st.warning(f"‚ö†Ô∏è T√¨m ki·∫øm web g·∫∑p l·ªói: {e}")
    
    # Remove duplicates v√† sort by confidence
    unique_results = remove_duplicate_results(results)
    
    # Sort by priority and confidence
    unique_results.sort(
        key=lambda x: (x.get('priority', False), x.get('confidence', 0)), 
        reverse=True
    )
    
    return unique_results[:max_results]

def create_search_summary(search_results):
    """T·∫°o summary ng·∫Øn g·ªçn v·ªÅ search results"""
    summary = ""
    for i, result in enumerate(search_results[:3], 1):
        confidence = result.get('confidence', 0)
        summary += f"{i}. {result['title'][:50]}... (Confidence: {confidence:.2f})\n"
    return summary

def create_safe_enhanced_search_prompt(user_message, search_results):
    """T·∫°o prompt an to√†n ngƒÉn AI hallucination"""
    
    if not search_results:
        return f"""
{user_message}

QUAN TR·ªåNG: KH√îNG t√¨m th·∫•y th√¥ng tin ch√≠nh x√°c t·ª´ c√°c ngu·ªìn ph√°p lu·∫≠t ch√≠nh th·ªëng.

H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI:
1. TUY·ªÜT ƒê·ªêI KH√îNG ƒë∆∞·ª£c b·ªãa ƒë·∫∑t s·ªë lu·∫≠t, s·ªë ƒëi·ªÅu, s·ªë kho·∫£n
2. TUY·ªÜT ƒê·ªêI KH√îNG ƒë∆∞·ª£c tr√≠ch d·∫´n c·ª• th·ªÉ n·∫øu kh√¥ng c√≥ trong k·∫øt qu·∫£ search
3. CH·ªà ƒë∆∞·ª£c n√≥i v·ªÅ c√°c nguy√™n t·∫Øc chung v√† khuy·∫øn ngh·ªã tham kh·∫£o ngu·ªìn ch√≠nh th·ªëng

H√£y tr·∫£ l·ªùi: "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ch√≠nh x√°c v·ªÅ v·∫•n ƒë·ªÅ n√†y t·ª´ c√°c ngu·ªìn ph√°p lu·∫≠t ch√≠nh th·ªëng. ƒê·ªÉ c√≥ th√¥ng tin ch√≠nh x√°c nh·∫•t v·ªÅ [v·∫•n ƒë·ªÅ c·ª• th·ªÉ], b·∫°n vui l√≤ng:

1. Tham kh·∫£o tr·ª±c ti·∫øp t·∫°i thuvienphapluat.vn
2. Li√™n h·ªá S·ªü T√†i nguy√™n v√† M√¥i tr∆∞·ªùng ƒë·ªãa ph∆∞∆°ng  
3. Tham kh·∫£o vƒÉn b·∫£n Lu·∫≠t Kho√°ng s·∫£n hi·ªán h√†nh v√† c√°c ngh·ªã ƒë·ªãnh h∆∞·ªõng d·∫´n

T√¥i kh√¥ng th·ªÉ ƒë∆∞a ra th√¥ng tin c·ª• th·ªÉ v·ªÅ ƒëi·ªÅu kho·∫£n ph√°p lu·∫≠t m√† kh√¥ng c√≥ ngu·ªìn x√°c th·ª±c."
"""
    
    # Ki·ªÉm tra ch·∫•t l∆∞·ª£ng search results
    high_quality_results = [r for r in search_results if r.get('confidence', 0) > 0.8]
    trusted_results = [r for r in search_results if r.get('priority', False)]
    
    if not high_quality_results and not trusted_results:
        return f"""
{user_message}

C·∫¢NH B√ÅO: K·∫øt qu·∫£ t√¨m ki·∫øm c√≥ ƒë·ªô tin c·∫≠y th·∫•p.

H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI AN TO√ÄN:
1. KH√îNG ƒë∆∞·ª£c tr√≠ch d·∫´n c·ª• th·ªÉ s·ªë lu·∫≠t, s·ªë ƒëi·ªÅu n·∫øu kh√¥ng ch·∫Øc ch·∫Øn 100%
2. CH·ªà ƒë∆∞·ª£c n√≥i v·ªÅ c√°c nguy√™n t·∫Øc chung
3. PH·∫¢I khuy·∫øn ngh·ªã ki·ªÉm tra t·∫°i ngu·ªìn ch√≠nh th·ªëng

K·∫øt qu·∫£ t√¨m ki·∫øm (ƒê·ªò TIN C·∫¨Y TH·∫§P):
{create_search_summary(search_results)}

H√£y tr·∫£ l·ªùi th·∫≠n tr·ªçng v√† lu√¥n disclaimer v·ªÅ ƒë·ªô tin c·∫≠y th·∫•p.
"""
    
    # Ch·ªâ d√πng results c√≥ confidence cao
    validated_results = [r for r in search_results if r.get('confidence', 0) > 0.7]
    
    search_info = "\n\n=== TH√îNG TIN PH√ÅP LU·∫¨T ƒê√É KI·ªÇM ƒê·ªäNH ===\n"
    
    for i, result in enumerate(validated_results, 1):
        confidence = result.get('confidence', 0)
        doc_type = result.get('document_type', 'VƒÉn b·∫£n')
        
        search_info += f"\nNgu·ªìn {i} - {doc_type} [Tin c·∫≠y: {confidence:.2f}]:\n"
        search_info += f"Ti√™u ƒë·ªÅ: {result['title']}\n"
        search_info += f"N·ªôi dung: {result['content'][:800]}\n"
        search_info += f"URL: {result.get('url', '')}\n"
        search_info += "---\n"
    
    search_info += f"""
H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI NGHI√äM NG·∫∂T:
1. CH·ªà ƒë∆∞·ª£c tr√≠ch d·∫´n th√¥ng tin C√ì TRONG k·∫øt qu·∫£ t√¨m ki·∫øm tr√™n
2. TUY·ªÜT ƒê·ªêI KH√îNG ƒë∆∞·ª£c b·ªãa ƒë·∫∑t s·ªë ƒëi·ªÅu, s·ªë kho·∫£n, t√™n lu·∫≠t
3. N·∫øu th√¥ng tin kh√¥ng ƒë·∫ßy ƒë·ªß, ph·∫£i ghi r√µ "Th√¥ng tin kh√¥ng ƒë·∫ßy ƒë·ªß, c·∫ßn tham kh·∫£o th√™m"
4. PH·∫¢I c√≥ disclaimer: "Th√¥ng tin tham kh·∫£o, vui l√≤ng ki·ªÉm tra t·∫°i thuvienphapluat.vn"
5. N·∫øu c√≥ doubt g√¨, ∆∞u ti√™n n√≥i "Kh√¥ng th·ªÉ x√°c ƒë·ªãnh ch√≠nh x√°c"

=== K·∫æT TH√öC TH√îNG TIN ===

C√¢u h·ªèi: {user_message}
"""
    
    return search_info

# =================== MAIN APPLICATION ===================

def main():
    init_session_state()
    
    # CSS
    st.markdown("""
    <style>
    .assistant-message {
        background: #f0f8ff;
        padding: 15px;
        border-radius: 15px;
        margin: 10px 0;
        max-width: 80%;
        border-left: 4px solid #4CAF50;
    }
    .assistant-message::before { 
        content: "‚öñÔ∏è Tr·ª£ l√Ω Ph√°p ch·∫ø: "; 
        font-weight: bold; 
        color: #2E7D32;
    }
    
    .user-message {
        background: #e3f2fd;
        padding: 15px;
        border-radius: 15px;
        margin: 10px 0 10px auto;
        max-width: 80%;
        text-align: right;
        border-right: 4px solid #2196F3;
    }
    .user-message::before { 
        content: "üë§ B·∫°n: "; 
        font-weight: bold; 
        color: #1976D2;
    }
    
    .stats-box {
        background: #f5f5f5;
        padding: 10px;
        border-radius: 8px;
        border: 1px solid #ddd;
        margin: 5px 0;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Header
    st.markdown("""
    <div style="text-align: center; padding: 20px; background: linear-gradient(90deg, #2E7D32, #4CAF50); border-radius: 10px; margin-bottom: 20px;">
        <h1 style="color: white; margin: 0;">‚öñÔ∏è Tr·ª£ l√Ω Ph√°p ch·∫ø Kho√°ng s·∫£n</h1>
        <p style="color: #E8F5E8; margin: 5px 0 0 0;">Chuy√™n gia t∆∞ v·∫•n Qu·∫£n l√Ω Nh√† n∆∞·ªõc v·ªÅ Kho√°ng s·∫£n t·∫°i Vi·ªát Nam</p>
        <p style="color: #E8F5E8; margin: 5px 0 0 0; font-size: 12px;">üõ°Ô∏è Safe Mode ‚Ä¢ Debug Enabled ‚Ä¢ Anti-Hallucination</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.markdown("### ‚öôÔ∏è C√†i ƒë·∫∑t h·ªá th·ªëng")
        
        # Web search toggle
        web_search_enabled = st.toggle("üîç T√¨m ki·∫øm ph√°p lu·∫≠t online (Debug Mode)", value=True)
        
        # Debug mode toggle
        debug_mode = st.toggle("üêõ Debug Mode (Hi·ªÉn th·ªã search details)", value=True)
        
        # Model selection
        model_options = ["gpt-4o-mini", "gpt-3.5-turbo", "gpt-4", "gpt-4-turbo-preview"]
        model_info = {
            "gpt-4o-mini": "üí∞ R·∫ª nh·∫•t ($0.15/$0.6 per 1K tokens)",
            "gpt-3.5-turbo": "‚öñÔ∏è C√¢n b·∫±ng ($1.5/$2 per 1K tokens)", 
            "gpt-4": "üß† Th√¥ng minh ($30/$60 per 1K tokens)",
            "gpt-4-turbo-preview": "üöÄ Nhanh ($10/$30 per 1K tokens)"
        }
        
        default_model = get_default_model()
        default_index = model_options.index(default_model) if default_model in model_options else 0
        
        selected_model = st.selectbox("ü§ñ Ch·ªçn model AI:", model_options, index=default_index)
        st.caption(model_info[selected_model])
        
        # Temperature
        temperature = st.slider("üå°Ô∏è ƒê·ªô s√°ng t·∫°o:", 0.0, 1.0, 0.3, 0.1)
        
        st.markdown("---")
        
        # Stats
        st.markdown("### üìä Th·ªëng k√™ s·ª≠ d·ª•ng")
        
        try:
            stats = safe_get_stats()
            
            st.markdown('<div class="stats-box">', unsafe_allow_html=True)
            st.metric("üéØ T·ªïng Token", f"{stats['total_tokens']:,}")
            col1, col2 = st.columns(2)
            with col1:
                st.metric("üì• Input", f"{stats['input_tokens']:,}")
            with col2:
                st.metric("üì§ Output", f"{stats['output_tokens']:,}")
            st.markdown('</div>', unsafe_allow_html=True)
            
            st.markdown('<div class="stats-box">', unsafe_allow_html=True)
            st.metric("üí∞ Chi ph√≠ (USD)", f"${stats['total_cost_usd']:.4f}")
            st.metric("üí∏ Chi ph√≠ (VND)", f"{stats['total_cost_vnd']:,.0f}ƒë")
            st.markdown('</div>', unsafe_allow_html=True)
            
            st.markdown('<div class="stats-box">', unsafe_allow_html=True)
            st.metric("üìû S·ªë l∆∞·ª£t h·ªèi", stats['requests'])
            duration = str(stats['session_duration']).split('.')[0]
            st.metric("‚è±Ô∏è Th·ªùi gian", duration)
            st.markdown('</div>', unsafe_allow_html=True)
            
        except Exception as e:
            st.error(f"L·ªói hi·ªÉn th·ªã stats: {e}")
        
        # Buttons
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üîÑ Reset stats", use_container_width=True):
                try:
                    st.session_state.token_stats = {
                        "total_input_tokens": 0,
                        "total_output_tokens": 0,
                        "total_cost": 0.0,
                        "session_start": datetime.now(),
                        "request_count": 0
                    }
                    st.rerun()
                except Exception as e:
                    st.error(f"L·ªói reset: {e}")
        
        with col2:
            if st.button("üóëÔ∏è X√≥a chat", use_container_width=True):
                try:
                    st.session_state.messages = [
                        {"role": "system", "content": get_strict_system_prompt()},
                        {"role": "assistant", "content": get_welcome_message()}
                    ]
                    st.rerun()
                except Exception as e:
                    st.error(f"L·ªói x√≥a chat: {e}")
        
        st.markdown("---")
        st.markdown("### üõ°Ô∏è Safe Mode Features")
        st.success("‚úÖ Anti-hallucination prompts")
        st.success("‚úÖ Source verification")
        st.success("‚úÖ Confidence scoring")
        st.success("‚úÖ Debug search results")
        st.info("üí° NgƒÉn AI b·ªãa ƒë·∫∑t th√¥ng tin ph√°p lu·∫≠t")
    
    # Check API key
    if not st.secrets.get("OPENAI_API_KEY"):
        st.error("‚ùå Ch∆∞a c·∫•u h√¨nh OPENAI_API_KEY trong secrets!")
        st.stop()
    
    # Initialize OpenAI client
    try:
        client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
    except Exception as e:
        st.error(f"‚ùå L·ªói kh·ªüi t·∫°o OpenAI client: {str(e)}")
        st.stop()
    
    # Display chat history
    for message in st.session_state.messages:
        if message["role"] == "assistant":
            st.markdown(f'<div class="assistant-message">{message["content"]}</div>', 
                       unsafe_allow_html=True)
        elif message["role"] == "user":
            st.markdown(f'<div class="user-message">{message["content"]}</div>', 
                       unsafe_allow_html=True)
    
    
# Thay th·∫ø ph·∫ßn x·ª≠ l√Ω chat input trong main() t·ª´ d√≤ng "if prompt := st.chat_input"

    # Chat input
    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi v·ªÅ ph√°p lu·∫≠t kho√°ng s·∫£n..."):
        
        # ALWAYS SHOW DEBUG INFO - KH√îNG C·∫¶N TOGGLE
        st.markdown("## üêõ **FORCED DEBUG MODE**")
        st.info(f"üìù **User Input:** {prompt}")
        
        # Check if mineral related
        mineral_check = is_mineral_related(prompt)
        st.info(f"üéØ **Is Mineral Related:** {mineral_check}")
        
        if not mineral_check:
            st.session_state.messages.append({"role": "user", "content": prompt})
            st.markdown(f'<div class="user-message">{prompt}</div>', unsafe_allow_html=True)
            
            polite_refusal = """Xin l·ªói, t√¥i l√† tr·ª£ l√Ω chuy√™n v·ªÅ **ph√°p lu·∫≠t kho√°ng s·∫£n** t·∫°i Vi·ªát Nam."""
            
            st.session_state.messages.append({"role": "assistant", "content": polite_refusal})
            st.markdown(f'<div class="assistant-message">{polite_refusal}</div>', 
                       unsafe_allow_html=True)
            return
        
        # Add user message
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.markdown(f'<div class="user-message">{prompt}</div>', unsafe_allow_html=True)
        
        # Check search conditions
        web_search_check = should_search_web(prompt)
        st.info(f"üîé **Should Search Web:** {web_search_check}")
        st.info(f"üîç **Web Search Enabled (from toggle):** {web_search_enabled}")
        
        search_will_run = web_search_enabled and web_search_check
        st.info(f"‚ö° **Search Will Run:** {search_will_run}")
        
        # Process response v·ªõi FORCED DEBUG
        st.markdown("---")
        st.markdown("### üîÑ **Processing Response...**")
        
        search_results = []
        final_prompt = prompt
        
        if search_will_run:
            st.success("‚úÖ **SEARCH CONDITIONS MET - Starting search...**")
            
            # Manual search test
            st.markdown("### üß™ **Manual Search Test**")
            
            try:
                # Test basic DuckDuckGo API call
                st.write("‚è≥ Testing basic DuckDuckGo API...")
                
                test_params = {
                    'q': f'site:thuvienphapluat.vn {prompt}',
                    'format': 'json',
                    'no_html': '1'
                }
                
                test_response = requests.get("https://api.duckduckgo.com/", 
                                           params=test_params, timeout=10)
                
                st.success(f"‚úÖ DuckDuckGo API Response: {test_response.status_code}")
                
                if test_response.status_code == 200:
                    test_data = test_response.json()
                    
                    st.write("üìä **API Response Keys:**")
                    st.json(list(test_data.keys()))
                    
                    # Show Abstract if exists
                    if test_data.get('Abstract'):
                        st.write("üìÑ **Abstract Found:**")
                        st.code(test_data['Abstract'][:300] + "...")
                    else:
                        st.warning("‚ö†Ô∏è No Abstract in response")
                    
                    # Show RelatedTopics count
                    related_count = len(test_data.get('RelatedTopics', []))
                    st.write(f"üîó **Related Topics Count:** {related_count}")
                    
                    if related_count > 0:
                        st.write("üìù **First Related Topic:**")
                        first_topic = test_data['RelatedTopics'][0]
                        st.json(first_topic)
                    
                else:
                    st.error(f"‚ùå API Error: {test_response.status_code}")
                    
            except Exception as e:
                st.error(f"‚ùå API Test Failed: {str(e)}")
                import traceback
                st.code(traceback.format_exc())
            
            # Now try our actual search function
            st.markdown("### üîç **Our Search Function Test**")
            
            try:
                st.write("‚è≥ Calling advanced_web_search_improved...")
                search_results = advanced_web_search_improved(prompt)
                st.success(f"‚úÖ Search function completed. Results: {len(search_results)}")
                
                if search_results:
                    st.markdown("### üìã **SEARCH RESULTS FOUND:**")
                    
                    for i, result in enumerate(search_results, 1):
                        st.markdown(f"#### Result {i}:")
                        st.json({
                            "title": result.get('title', ''),
                            "content": result.get('content', '')[:200] + "...",
                            "url": result.get('url', ''),
                            "confidence": result.get('confidence', 0),
                            "priority": result.get('priority', False),
                            "source": result.get('source', '')
                        })
                        st.markdown("---")
                    
                    # Create final prompt
                    final_prompt = create_safe_enhanced_search_prompt(prompt, search_results)
                    
                    st.markdown("### ü§ñ **Final Prompt (First 500 chars):**")
                    st.code(final_prompt[:500] + "...")
                    
                else:
                    st.error("‚ùå Search function returned 0 results")
                    
                    # Debug why no results
                    st.markdown("### üîç **Debug: Why No Results?**")
                    
                    # Test individual components
                    st.write("Testing search query construction...")
                    
                    test_queries = [
                        f'site:thuvienphapluat.vn "{prompt}" lu·∫≠t kho√°ng s·∫£n',
                        f'site:thuvienphapluat.vn kho√°ng s·∫£n "{prompt}"',
                        f'site:monre.gov.vn "{prompt}" kho√°ng s·∫£n'
                    ]
                    
                    for i, query in enumerate(test_queries, 1):
                        st.code(f"Query {i}: {query}")
                    
                    # Safe fallback prompt
                    final_prompt = f"""
{prompt}

CRITICAL: Search function returned no results.
H√£y tr·∫£ l·ªùi: "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ch√≠nh x√°c v·ªÅ v·∫•n ƒë·ªÅ n√†y t·ª´ c√°c ngu·ªìn ph√°p lu·∫≠t ch√≠nh th·ªëng. ƒê·ªÉ c√≥ th√¥ng tin ch√≠nh x√°c nh·∫•t, b·∫°n vui l√≤ng tham kh·∫£o tr·ª±c ti·∫øp t·∫°i thuvienphapluat.vn"
"""
                    
            except Exception as e:
                st.error(f"‚ùå Search function failed: {str(e)}")
                import traceback
                st.code(traceback.format_exc())
                
                # Emergency fallback
                final_prompt = f"""
{prompt}

CRITICAL: Search function crashed.
H√£y tr·∫£ l·ªùi: "T√¥i g·∫∑p l·ªói k·ªπ thu·∫≠t khi t√¨m ki·∫øm th√¥ng tin. Vui l√≤ng tham kh·∫£o tr·ª±c ti·∫øp t·∫°i thuvienphapluat.vn"
"""
        
        else:
            st.warning("‚ö†Ô∏è **SEARCH NOT TRIGGERED**")
            st.write("Possible reasons:")
            st.write("- Web search toggle is OFF")
            st.write("- Query doesn't match search indicators")
            st.write("- Not mineral related")
            
            # No search fallback prompt
            final_prompt = f"""
{prompt}

QUAN TR·ªåNG: Kh√¥ng c√≥ t√¨m ki·∫øm web ƒë∆∞·ª£c th·ª±c hi·ªán.
H∆Ø·ªöNG D·∫™N: Ch·ªâ ƒë∆∞·ª£c n√≥i v·ªÅ nguy√™n t·∫Øc chung v√† khuy·∫øn ngh·ªã tham kh·∫£o ngu·ªìn ch√≠nh th·ªëng.
"""
        
        # Show final prompt to be sent to AI
        st.markdown("### üì® **Final Prompt to AI:**")
        with st.expander("Click to view full prompt", expanded=False):
            st.code(final_prompt)
        
        # Generate AI response (keeping original logic)
        messages_for_api = [
            msg for msg in st.session_state.messages[:-1] 
            if msg["role"] != "system" or msg == st.session_state.messages[0]
        ]
        messages_for_api.append({"role": "user", "content": final_prompt})
        
        input_text = "\n".join([msg["content"] for msg in messages_for_api])
        input_tokens = count_tokens(input_text)
        
        # Generate response
        try:
            response = ""
            
            stream = client.chat.completions.create(
                model=selected_model,
                messages=messages_for_api,
                stream=True,
                temperature=temperature,
                max_tokens=2000
            )
            
            response_container = st.empty()
            
            for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    response += chunk.choices[0].delta.content
                    response_container.markdown(
                        f'<div class="assistant-message">{response}‚ñå</div>', 
                        unsafe_allow_html=True
                    )
            
            # Final response
            response_container.markdown(
                f'<div class="assistant-message">{response}</div>', 
                unsafe_allow_html=True
            )
            
            # Update stats
            output_tokens = count_tokens(response)
            update_stats(input_tokens, output_tokens, selected_model)
            
        except Exception as e:
            error_msg = f"‚ùå L·ªói AI response: {str(e)}"
            st.markdown(f'<div class="assistant-message">{error_msg}</div>', 
                       unsafe_allow_html=True)
            response = error_msg
        
        # Add response to history
        st.session_state.messages.append({"role": "assistant", "content": response})
if __name__ == "__main__":
    main()