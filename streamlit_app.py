import streamlit as st
from openai import OpenAI
import sqlite3
import json
from datetime import datetime
import re
import time
import os
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="‚öñÔ∏è Tr·ª£ l√Ω Ph√°p ch·∫ø Kho√°ng s·∫£n Vi·ªát Nam",
    page_icon="‚öñÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =================== CONFIGURATIONS ===================

MODEL_PRICING = {
    "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
    "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002},
    "gpt-4": {"input": 0.03, "output": 0.06},
    "gpt-4-turbo-preview": {"input": 0.01, "output": 0.03}
}

@dataclass
class VBPLConfig:
    """C·∫•u h√¨nh cho VBPL integration"""
    vbpl_db_path: str = "vbpl.db"
    model: str = "gpt-4o-mini"
    max_tokens: int = 1500
    temperature: float = 0.1
    max_search_results: int = 5
    domain_focus: str = "kho√°ng s·∫£n"
    prioritize_active_docs: bool = True

# =================== VBPL DATABASE INTEGRATION ===================

class VBPLDatabase:
    """Database manager cho VBPL v·ªõi Streamlit integration"""
    
    def __init__(self, config: VBPLConfig):
        self.config = config
        self.conn = None
        self.available = False
        self._init_connection()
    
    def _init_connection(self):
        """Initialize database connection"""
        try:
            if not os.path.exists(self.config.vbpl_db_path):
                st.sidebar.warning(f"‚ö†Ô∏è Database file kh√¥ng t·ªìn t·∫°i: {self.config.vbpl_db_path}")
                return False
            
            self.conn = sqlite3.connect(self.config.vbpl_db_path, check_same_thread=False)
            self.conn.row_factory = sqlite3.Row
            self.available = True
            
            # Analyze database structure
            self._analyze_database()
            return True
            
        except Exception as e:
            st.sidebar.error(f"‚ùå L·ªói k·∫øt n·ªëi database: {e}")
            return False
    
    def _analyze_database(self):
        """Ph√¢n t√≠ch c·∫•u tr√∫c database v√† hi·ªÉn th·ªã stats"""
        try:
            cursor = self.conn.cursor()
            
            # Check tables exist
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = [row[0] for row in cursor.fetchall()]
            
            st.sidebar.success(f"‚úÖ VBPL Database Online")
            st.sidebar.caption(f"üìä {len(tables)} tables detected")
            
            # Check for expected tables
            expected_tables = ['documents', 'vbpl_content']
            missing_tables = [t for t in expected_tables if t not in tables]
            
            if missing_tables:
                st.sidebar.warning(f"‚ö†Ô∏è Missing tables: {missing_tables}")
            
            # Analyze document distribution if documents table exists
            if 'documents' in tables:
                cursor.execute("SELECT state, COUNT(*) FROM documents GROUP BY state ORDER BY COUNT(*) DESC")
                status_dist = cursor.fetchall()
                
                with st.sidebar.expander("üìä Database Statistics", expanded=False):
                    total_docs = sum(count for _, count in status_dist)
                    st.metric("Total Documents", f"{total_docs:,}")
                    
                    for status, count in status_dist[:5]:  # Top 5 statuses
                        percentage = (count / total_docs) * 100
                        st.metric(status, f"{count:,} ({percentage:.1f}%)")
            
            # Check vbpl_content table
            if 'vbpl_content' in tables:
                cursor.execute("SELECT COUNT(*) FROM vbpl_content")
                content_count = cursor.fetchone()[0]
                st.sidebar.metric("Legal Content Items", f"{content_count:,}")
                
                # Check domain coverage
                cursor.execute("""
                    SELECT COUNT(*) FROM vbpl_content 
                    WHERE LOWER(element_content) LIKE '%kho√°ng s·∫£n%' 
                    OR LOWER(document_name) LIKE '%kho√°ng s·∫£n%'
                """)
                mineral_count = cursor.fetchone()[0]
                
                if content_count > 0:
                    mineral_percentage = (mineral_count / content_count) * 100
                    st.sidebar.metric("Kho√°ng s·∫£n Content", f"{mineral_count:,} ({mineral_percentage:.1f}%)")
            
        except Exception as e:
            st.sidebar.error(f"‚ùå Database analysis failed: {e}")
    
    def is_available(self) -> bool:
        """Check if database is available and connected"""
        return self.available and self.conn is not None
    
    def search_domain_specific(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """Search database v·ªõi domain-specific filtering cho kho√°ng s·∫£n"""
        if not self.is_available():
            return []
        
        try:
            cursor = self.conn.cursor()
            keywords = self._extract_smart_keywords(query)
            
            if not keywords:
                st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ tr√≠ch xu·∫•t t·ª´ kh√≥a t·ª´ c√¢u h·ªèi")
                return []
            
            # Display search info
            st.write(f"üîç **Searching for**: {', '.join(keywords[:3])}")
            
            # Build search query v·ªõi domain filtering
            search_conditions = []
            params = []
            
            # 1. Domain filtering - KHO√ÅNG S·∫¢N
            domain_keywords = ['kho√°ng s·∫£n', 't√†i nguy√™n', 'thƒÉm d√≤', 'khai th√°c', 'm·ªè', 'ƒë·ªãa ch·∫•t']
            domain_condition = " OR ".join([f"LOWER(element_content) LIKE ?" for _ in domain_keywords])
            domain_condition += " OR " + " OR ".join([f"LOWER(document_name) LIKE ?" for _ in domain_keywords])
            
            search_conditions.append(f"({domain_condition})")
            params.extend([f"%{kw}%" for kw in domain_keywords])
            params.extend([f"%{kw}%" for kw in domain_keywords])
            
            # 2. Query-specific keywords
            query_condition = " OR ".join([f"LOWER(element_content) LIKE ?" for _ in keywords])
            query_condition += " OR " + " OR ".join([f"LOWER(element_name) LIKE ?" for _ in keywords])
            
            search_conditions.append(f"({query_condition})")
            params.extend([f"%{kw}%" for kw in keywords])
            params.extend([f"%{kw}%" for kw in keywords])
            
            # 3. Build final query v·ªõi prioritization
            state_priority = """
                CASE 
                    WHEN LOWER(document_state) LIKE '%c√≤n hi·ªáu l·ª±c%' THEN 3
                    WHEN LOWER(document_state) LIKE '%c√≥ hi·ªáu l·ª±c%' THEN 3  
                    WHEN LOWER(document_state) LIKE '%h·∫øt hi·ªáu l·ª±c%' THEN 1
                    ELSE 2 
                END
            """
            
            # Element type priority
            element_priority = """
                CASE 
                    WHEN element_type = 'vbpl_section' THEN 3
                    WHEN element_type = 'vbpl_clause' THEN 2
                    WHEN element_type = 'vbpl_point' THEN 1
                    ELSE 0
                END
            """
            
            sql = f"""
            SELECT 
                element_id,
                element_type,
                element_number,
                element_name,
                element_content,
                document_number,
                document_name,
                document_state,
                {state_priority} as status_priority,
                {element_priority} as element_priority,
                LENGTH(element_content) as content_length
            FROM vbpl_content 
            WHERE {' AND '.join(search_conditions)}
            AND element_content IS NOT NULL 
            AND LENGTH(TRIM(element_content)) > 50
            ORDER BY 
                status_priority DESC,
                element_priority DESC,
                content_length DESC,
                element_id
            LIMIT ?
            """
            
            params.append(max_results)
            
            cursor.execute(sql, params)
            results = cursor.fetchall()
            
            st.write(f"üìä **Raw database results**: {len(results)}")
            
            # Process results
            processed_results = []
            for row in results:
                relevance_score = self._calculate_relevance(query, dict(row))
                
                processed_results.append({
                    'element_id': row['element_id'],
                    'element_type': row['element_type'],
                    'element_number': row['element_number'] or '',
                    'element_name': row['element_name'] or '',
                    'element_content': row['element_content'] or '',
                    'document_number': row['document_number'],
                    'document_name': row['document_name'],
                    'document_state': row['document_state'],
                    'is_active': self._is_document_active(row['document_state']),
                    'relevance_score': relevance_score,
                    'status_priority': row['status_priority'],
                    'element_priority': row['element_priority']
                })
            
            # Sort by combined score
            processed_results.sort(
                key=lambda x: (x['is_active'], x['relevance_score'], x['element_priority']), 
                reverse=True
            )
            
            return processed_results
            
        except Exception as e:
            st.error(f"‚ùå Database search failed: {e}")
            import traceback
            st.code(traceback.format_exc())
            return []
    
    def _extract_smart_keywords(self, query: str) -> List[str]:
        """Extract smart keywords t·ª´ query v·ªõi Vietnamese support"""
        # Vietnamese stop words
        stop_words = {
            'l√†', 'c·ªßa', 'v√†', 'c√≥', 'ƒë∆∞·ª£c', 'theo', 'trong', 'v·ªÅ', 'khi', 'n√†o', 'g√¨', 'nh∆∞', 'th·∫ø',
            'v·ªõi', 'ƒë·ªÉ', 'cho', 't·ª´', 't·∫°i', 'tr√™n', 'd∆∞·ªõi', 'n√†y', 'ƒë√≥', 'nh·ªØng', 'c√°c', 'm·ªôt',
            'hai', 'ba', 'b·ªën', 'nƒÉm', 's√°u', 'b·∫£y', 't√°m', 'ch√≠n', 'm∆∞·ªùi'
        }
        
        # Normalize v√† tokenize
        query_normalized = re.sub(r'[^\w\s]', ' ', query.lower())
        words = [w.strip() for w in query_normalized.split() if w.strip()]
        
        # Filter keywords
        keywords = [w for w in words if len(w) > 2 and w not in stop_words]
        
        # Add important bigrams for better matching
        bigrams = []
        for i in range(len(words) - 1):
            if (words[i] not in stop_words and words[i+1] not in stop_words 
                and len(words[i]) > 2 and len(words[i+1]) > 2):
                bigrams.append(f"{words[i]} {words[i+1]}")
        
        # Combine v√† prioritize
        all_keywords = keywords[:4] + bigrams[:2]  # Limit ƒë·ªÉ avoid qu√° complex
        
        return all_keywords
    
    def _calculate_relevance(self, query: str, content: Dict) -> float:
        """Calculate relevance score cho search result"""
        score = 0.0
        query_lower = query.lower()
        
        # Text fields v·ªõi weights kh√°c nhau
        text_fields = [
            (content.get('element_content', ''), 0.4),  # Content cao nh·∫•t
            (content.get('element_name', ''), 0.25),    # T√™n element
            (content.get('document_name', ''), 0.15),   # T√™n document  
            (content.get('element_number', ''), 0.1),   # S·ªë ƒëi·ªÅu/kho·∫£n
            (content.get('document_number', ''), 0.1)   # S·ªë vƒÉn b·∫£n
        ]
        
        # Calculate text matching score
        for text, weight in text_fields:
            if text:
                text_lower = text.lower()
                
                # Exact phrase match
                if query_lower in text_lower:
                    score += weight
                
                # Word overlap
                query_words = set(query_lower.split())
                text_words = set(text_lower.split())
                overlap = len(query_words.intersection(text_words))
                if len(query_words) > 0:
                    overlap_ratio = overlap / len(query_words)
                    score += (overlap_ratio * weight * 0.5)
        
        # Bonus for active documents
        if self._is_document_active(content.get('document_state', '')):
            score += 0.15
        
        # Bonus for important element types
        element_type = content.get('element_type', '')
        if element_type == 'vbpl_section':
            score += 0.1
        elif element_type == 'vbpl_clause':
            score += 0.05
        
        # Content length bonus (longer = more detailed)
        content_length = len(content.get('element_content', ''))
        if content_length > 500:
            score += 0.05
        elif content_length > 200:
            score += 0.02
        
        return min(score, 1.0)
    
    def _is_document_active(self, state: str) -> bool:
        """Check if document is currently active"""
        if not state:
            return False
        
        state_lower = state.lower()
        active_indicators = ['c√≤n hi·ªáu l·ª±c', 'c√≥ hi·ªáu l·ª±c', 'hi·ªán h√†nh']
        return any(indicator in state_lower for indicator in active_indicators)
    
    def close(self):
        """Close database connection"""
        if self.conn:
            self.conn.close()
            self.available = False

class VBPLOpenAIProcessor:
    """OpenAI processor cho VBPL content v·ªõi professional prompting"""
    
    def __init__(self, openai_client: OpenAI, config: VBPLConfig, db_manager: VBPLDatabase):
        self.client = openai_client
        self.config = config
        self.db_manager = db_manager
    
    def process_legal_query(self, query: str) -> Dict[str, Any]:
        """Process legal query v·ªõi VBPL database"""
        
        # Step 1: Search database
        with st.status("üîç ƒêang t√¨m ki·∫øm trong c∆° s·ªü d·ªØ li·ªáu ph√°p lu·∫≠t VBPL...", expanded=True) as status:
            
            search_results = self.db_manager.search_domain_specific(query, self.config.max_search_results)
            
            if search_results:
                active_count = sum(1 for r in search_results if r['is_active'])
                inactive_count = len(search_results) - active_count
                
                st.success(f"‚úÖ T√¨m th·∫•y {len(search_results)} k·∫øt qu·∫£ ch√≠nh x√°c")
                st.info(f"üìä {active_count} vƒÉn b·∫£n c√≤n hi·ªáu l·ª±c, {inactive_count} vƒÉn b·∫£n h·∫øt hi·ªáu l·ª±c")
                
                # Display preview of results
                for i, result in enumerate(search_results, 1):
                    status_icon = "‚úÖ" if result['is_active'] else "‚ö†Ô∏è"
                    st.write(f"**{i}. {status_icon} {result['element_number']}**")
                    if result['element_name']:
                        st.write(f"   üìã {result['element_name'][:80]}...")
                    st.write(f"   üìÑ {result['document_number']} ({result['document_state']})")
                    st.write(f"   üéØ Score: {result['relevance_score']:.2f}")
                
                status.update(label="‚úÖ T√¨m ki·∫øm VBPL ho√†n t·∫•t", state="complete")
            else:
                st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p trong database VBPL")
                status.update(label="‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£", state="complete")
        
        # Step 2: Generate response
        if search_results:
            return self._generate_response_with_sources(query, search_results)
        else:
            return self._generate_fallback_response(query)
    
    def _generate_response_with_sources(self, query: str, search_results: List[Dict]) -> Dict[str, Any]:
        """Generate response v·ªõi database sources"""
        
        # Create context t·ª´ search results
        context = self._create_structured_context(search_results)
        
        # Show context preview
        with st.expander("üìÑ Context ƒë∆∞·ª£c g·ª≠i ƒë·∫øn AI (Click ƒë·ªÉ xem)", expanded=False):
            st.code(context[:1500] + "..." if len(context) > 1500 else context)
        
        # Call OpenAI
        try:
            response = self._call_openai_with_vbpl_context(query, context, search_results)
            
            return {
                'response': response,
                'sources': search_results,
                'active_sources': sum(1 for r in search_results if r['is_active']),
                'inactive_sources': sum(1 for r in search_results if not r['is_active']),
                'total_sources': len(search_results),
                'method': 'vbpl_database',
                'success': True
            }
            
        except Exception as e:
            st.error(f"‚ùå L·ªói OpenAI API: {e}")
            return {
                'response': f"T√¥i ƒë√£ t√¨m th·∫•y {len(search_results)} quy ƒë·ªãnh li√™n quan trong c∆° s·ªü d·ªØ li·ªáu, nh∆∞ng g·∫∑p l·ªói khi x·ª≠ l√Ω th√¥ng tin. Vui l√≤ng tham kh·∫£o tr·ª±c ti·∫øp c√°c quy ƒë·ªãnh sau ho·∫∑c li√™n h·ªá c∆° quan c√≥ th·∫©m quy·ªÅn.",
                'sources': search_results,
                'method': 'vbpl_database_error',
                'success': False
            }
    
    def _generate_fallback_response(self, query: str) -> Dict[str, Any]:
        """Generate fallback response khi kh√¥ng c√≥ database results"""
        
        fallback_response = """T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ v·ªÅ v·∫•n ƒë·ªÅ n√†y trong c∆° s·ªü d·ªØ li·ªáu ph√°p lu·∫≠t hi·ªán c√≥. 

üîç **ƒê·ªÉ c√≥ th√¥ng tin ch√≠nh x√°c nh·∫•t, b·∫°n vui l√≤ng:**

1. **Tham kh·∫£o tr·ª±c ti·∫øp** t·∫°i thuvienphapluat.vn
2. **Li√™n h·ªá** S·ªü T√†i nguy√™n v√† M√¥i tr∆∞·ªùng ƒë·ªãa ph∆∞∆°ng  
3. **Tham kh·∫£o** Lu·∫≠t Kho√°ng s·∫£n hi·ªán h√†nh v√† c√°c ngh·ªã ƒë·ªãnh h∆∞·ªõng d·∫´n
4. **Li√™n h·ªá** hotline t∆∞ v·∫•n ph√°p lu·∫≠t c·ªßa B·ªô T∆∞ ph√°p

üí° **G·ª£i √Ω**: Th·ª≠ di·ªÖn ƒë·∫°t c√¢u h·ªèi kh√°c ho·∫∑c s·ª≠ d·ª•ng t·ª´ kh√≥a c·ª• th·ªÉ h∆°n v·ªÅ lƒ©nh v·ª±c kho√°ng s·∫£n."""

        return {
            'response': fallback_response,
            'sources': [],
            'method': 'vbpl_database_empty',
            'success': False
        }
    
    def _create_structured_context(self, results: List[Dict]) -> str:
        """Create structured context t·ª´ VBPL database results"""
        
        context = "=== TH√îNG TIN T·ª™ C∆† S·ªû D·ªÆ LI·ªÜU PH√ÅP LU·∫¨T VBPL ===\n\n"
        
        # Group by document status
        active_results = [r for r in results if r['is_active']]
        inactive_results = [r for r in results if not r['is_active']]
        
        if active_results:
            context += "üìã **QUY ƒê·ªäNH C√íN HI·ªÜU L·ª∞C (∆Øu ti√™n s·ª≠ d·ª•ng):**\n\n"
            for i, result in enumerate(active_results, 1):
                context += self._format_result_for_context(result, i, "‚úÖ")
        
        if inactive_results:
            context += "\nüìã **QUY ƒê·ªäNH H·∫æT HI·ªÜU L·ª∞C (Ch·ªâ tham kh·∫£o):**\n\n"
            for i, result in enumerate(inactive_results, 1):
                context += self._format_result_for_context(result, i, "‚ö†Ô∏è")
        
        return context
    
    def _format_result_for_context(self, result: Dict, index: int, status_icon: str) -> str:
        """Format single result cho context"""
        
        entry = f"Ngu·ªìn {index} {status_icon}:\n"
        entry += f"‚Ä¢ **VƒÉn b·∫£n**: {result['document_number']} - {result['document_name']}\n"
        entry += f"‚Ä¢ **Tr·∫°ng th√°i**: {result['document_state']}\n"
        entry += f"‚Ä¢ **ƒêi·ªÅu kho·∫£n**: {result['element_number']}"
        
        if result['element_name']:
            entry += f" - {result['element_name']}\n"
        else:
            entry += "\n"
        
        # Clean v√† format content
        content = result['element_content'].strip()
        if len(content) > 800:
            content = content[:800] + "..."
        
        entry += f"‚Ä¢ **N·ªôi dung**: {content}\n"
        entry += f"‚Ä¢ **ƒê·ªô li√™n quan**: {result['relevance_score']:.2f}\n"
        entry += "---\n\n"
        
        return entry
    
    def _call_openai_with_vbpl_context(self, query: str, context: str, sources: List[Dict]) -> str:
        """Call OpenAI v·ªõi VBPL context v√† professional prompting"""
        
        # Count sources
        active_sources = sum(1 for s in sources if s['is_active'])
        inactive_sources = len(sources) - active_sources
        
        # Professional system prompt cho kho√°ng s·∫£n
        system_prompt = """B·∫°n l√† chuy√™n gia ph√°p lu·∫≠t KHO√ÅNG S·∫¢N Vi·ªát Nam v·ªõi chuy√™n m√¥n s√¢u v·ªÅ quy ƒë·ªãnh ph√°p lu·∫≠t.

üéØ **NHI·ªÜM V·ª§ CH√çNH**:
- Tr·∫£ l·ªùi D·ª∞A TR√äN quy ƒë·ªãnh ph√°p lu·∫≠t kho√°ng s·∫£n t·ª´ c∆° s·ªü d·ªØ li·ªáu VBPL
- ∆Øu ti√™n tuy·ªát ƒë·ªëi QUY ƒê·ªäNH C√íN HI·ªÜU L·ª∞C (‚úÖ) h∆°n quy ƒë·ªãnh h·∫øt hi·ªáu l·ª±c (‚ö†Ô∏è)
- Tr√≠ch d·∫´n CH√çNH X√ÅC ƒëi·ªÅu, kho·∫£n, ƒëi·ªÉm t·ª´ vƒÉn b·∫£n ph√°p lu·∫≠t
- Gi·∫£i th√≠ch TH·ª∞C TI·ªÑN v√† d·ªÖ hi·ªÉu cho doanh nghi·ªáp/c√° nh√¢n

üö´ **NGHI√äM C·∫§M**:
- Suy lu·∫≠n ho·∫∑c di·ªÖn gi·∫£i khi kh√¥ng c√≥ th√¥ng tin tr·ª±c ti·∫øp
- S·ª≠ d·ª•ng quy ƒë·ªãnh H·∫æT HI·ªÜU L·ª∞C khi ƒë√£ c√≥ quy ƒë·ªãnh C√íN HI·ªÜU L·ª∞C
- Tr√≠ch d·∫´n ƒëi·ªÅu lu·∫≠t kh√¥ng tr·ª±c ti·∫øp li√™n quan ƒë·∫øn c√¢u h·ªèi
- B·ªãa ƒë·∫∑t th√¥ng tin kh√¥ng c√≥ trong c∆° s·ªü d·ªØ li·ªáu

üìã **FORMAT CHU·∫®N**:
**üîç Tr·∫£ l·ªùi**: [C√¢u tr·∫£ l·ªùi tr·ª±c ti·∫øp v√† r√µ r√†ng]

**‚öñÔ∏è CƒÉn c·ª© ph√°p l√Ω**: 
[Tr√≠ch d·∫´n c·ª• th·ªÉ v·ªõi tr·∫°ng th√°i hi·ªáu l·ª±c]

**üí° L∆∞u √Ω th·ª±c ti·ªÖn**: [N·∫øu c·∫ßn thi·∫øt]

**üìå Khuy·∫øn ngh·ªã**: [H∆∞·ªõng d·∫´n th·ª±c hi·ªán ho·∫∑c tham kh·∫£o th√™m]

üéØ **DOMAIN**: CH·ªà v·ªÅ KHO√ÅNG S·∫¢N - kh√¥ng √°p d·ª•ng cho lƒ©nh v·ª±c kh√°c."""

        user_prompt = f"""‚ùì **C√¢u h·ªèi**: {query}

üìä **Ngu·ªìn t·ª´ Database VBPL**: {active_sources} quy ƒë·ªãnh c√≤n hi·ªáu l·ª±c + {inactive_sources} quy ƒë·ªãnh h·∫øt hi·ªáu l·ª±c

{context}

üéØ **Y√™u c·∫ßu**: Tr·∫£ l·ªùi d·ª±a tr√™n c∆° s·ªü d·ªØ li·ªáu VBPL tr√™n, TUY·ªÜT ƒê·ªêI ∆∞u ti√™n quy ƒë·ªãnh C√íN HI·ªÜU L·ª∞C."""

        try:
            response = self.client.chat.completions.create(
                model=self.config.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            raise Exception(f"OpenAI API call failed: {e}")

# =================== STREAMLIT CORE FUNCTIONS ===================

def init_session_state():
    """Kh·ªüi t·∫°o session state"""
    if "token_stats" not in st.session_state:
        st.session_state.token_stats = {
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "total_cost": 0.0,
            "session_start": datetime.now(),
            "request_count": 0
        }
    
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "system", "content": get_system_prompt()},
            {"role": "assistant", "content": get_welcome_message()}
        ]

def get_system_prompt():
    """Get system prompt t·ª´ file ho·∫∑c default"""
    try:
        with open("01.system_trainning.txt", "r", encoding="utf-8") as file:
            return file.read()
    except FileNotFoundError:
        return """B·∫°n l√† chuy√™n gia ph√°p ch·∫ø v·ªÅ qu·∫£n l√Ω nh√† n∆∞·ªõc trong lƒ©nh v·ª±c kho√°ng s·∫£n t·∫°i Vi·ªát Nam, ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi c∆° s·ªü d·ªØ li·ªáu ph√°p lu·∫≠t VBPL chuy√™n nghi·ªáp."""

def get_welcome_message():
    """Get welcome message"""
    try:
        with open("02.assistant.txt", "r", encoding="utf-8") as file:
            return file.read()
    except FileNotFoundError:
        return """Xin ch√†o! ‚öñÔ∏è 

T√¥i l√† **Tr·ª£ l√Ω Ph√°p ch·∫ø chuy√™n v·ªÅ Kho√°ng s·∫£n** ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi **c∆° s·ªü d·ªØ li·ªáu ph√°p lu·∫≠t VBPL**.

üóÑÔ∏è **T√¥i c√≥ th·ªÉ t√¨m ki·∫øm ch√≠nh x√°c trong h·ªá th·ªëng ph√°p lu·∫≠t v·ªÅ:**
- Lu·∫≠t Kho√°ng s·∫£n v√† c√°c vƒÉn b·∫£n h∆∞·ªõng d·∫´n
- Th·ªß t·ª•c c·∫•p ph√©p thƒÉm d√≤, khai th√°c
- Thu·∫ø, ph√≠ li√™n quan ƒë·∫øn kho√°ng s·∫£n  
- X·ª≠ ph·∫°t vi ph·∫°m h√†nh ch√≠nh
- B·∫£o v·ªá m√¥i tr∆∞·ªùng trong ho·∫°t ƒë·ªông kho√°ng s·∫£n

üí° **H√£y ƒë·∫∑t c√¢u h·ªèi c·ª• th·ªÉ ƒë·ªÉ t√¥i t√¨m ki·∫øm ch√≠nh x√°c trong database!**"""

def get_default_model():
    """Get default model"""
    try:
        with open("module_chatgpt.txt", "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "gpt-4o-mini"

def is_mineral_related(message):
    """Check if message is mineral-related"""
    mineral_keywords = [
        'kho√°ng s·∫£n', 'khai th√°c', 'thƒÉm d√≤', 'ƒë√°', 'c√°t', 's·ªèi',
        'than', 'qu·∫∑ng', 'kim lo·∫°i', 'phi kim lo·∫°i', 'kho√°ng',
        'lu·∫≠t kho√°ng s·∫£n', 'gi·∫•y ph√©p', 'c·∫•p ph√©p', 'thu·∫ø t√†i nguy√™n',
        'ph√≠ thƒÉm d√≤', 'ti·ªÅn c·∫•p quy·ªÅn', 'vi ph·∫°m h√†nh ch√≠nh',
        'b·ªô t√†i nguy√™n', 's·ªü t√†i nguy√™n', 'monre', 'tn&mt',
        'm·ªè', 'm·ªè ƒë√°', 'm·ªè c√°t', 'm·ªè than', 'quarry', 'mining',
        'thu h·ªìi gi·∫•y ph√©p', 'gia h·∫°n', 'ƒë√≥ng c·ª≠a m·ªè'
    ]
    
    message_lower = message.lower()
    return any(keyword in message_lower for keyword in mineral_keywords)

def count_tokens(text):
    """Estimate token count"""
    return len(str(text)) // 4

def update_stats(input_tokens, output_tokens, model):
    """Update token statistics"""
    try:
        if model not in MODEL_PRICING:
            model = "gpt-4o-mini"
        
        pricing = MODEL_PRICING[model]
        cost = (input_tokens / 1000) * pricing["input"] + (output_tokens / 1000) * pricing["output"]
        
        st.session_state.token_stats["total_input_tokens"] += input_tokens
        st.session_state.token_stats["total_output_tokens"] += output_tokens
        st.session_state.token_stats["total_cost"] += cost
        st.session_state.token_stats["request_count"] += 1
        
    except Exception as e:
        st.error(f"Error updating stats: {e}")

# =================== VBPL SYSTEM INITIALIZATION ===================

def init_vbpl_system():
    """Initialize VBPL system"""
    if 'vbpl_system' not in st.session_state:
        config = VBPLConfig()
        db_manager = VBPLDatabase(config)
        
        if db_manager.is_available():
            try:
                client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
                openai_processor = VBPLOpenAIProcessor(client, config, db_manager)
                
                st.session_state.vbpl_system = {
                    'config': config,
                    'db_manager': db_manager,
                    'openai_processor': openai_processor,
                    'available': True
                }
                
            except Exception as e:
                st.sidebar.error(f"‚ùå VBPL system initialization failed: {e}")
                st.session_state.vbpl_system = {'available': False}
        else:
            st.session_state.vbpl_system = {'available': False}

def is_vbpl_available() -> bool:
    """Check if VBPL system is available"""
    return st.session_state.get('vbpl_system', {}).get('available', False)

def process_with_vbpl(query: str) -> Dict[str, Any]:
    """Process query v·ªõi VBPL system"""
    if not is_vbpl_available():
        return {'method': 'vbpl_unavailable', 'success': False}
    
    processor = st.session_state.vbpl_system['openai_processor']
    return processor.process_legal_query(query)

def show_vbpl_results_details(result: Dict[str, Any]):
    """Show detailed VBPL results"""
    if not result.get('sources'):
        return
    
    st.markdown("### üìä **Chi ti·∫øt k·∫øt qu·∫£ t·ª´ Database VBPL**")
    
    # Summary metrics
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("T·ªïng k·∫øt qu·∫£", result['total_sources'])
    with col2:
        st.metric("C√≤n hi·ªáu l·ª±c", result['active_sources'])
    with col3:
        st.metric("H·∫øt hi·ªáu l·ª±c", result['inactive_sources'])
    
    # Detailed results
    for i, source in enumerate(result['sources'], 1):
        with st.expander(
            f"{i}. {'‚úÖ' if source['is_active'] else '‚ö†Ô∏è'} {source['element_number']}: {source['element_name'][:60]}...", 
            expanded=False
        ):
            
            col1, col2 = st.columns([3, 1])
            
            with col1:
                st.write(f"**üìÑ VƒÉn b·∫£n**: {source['document_number']}")
                st.write(f"**üìã T√™n vƒÉn b·∫£n**: {source['document_name']}")
                st.write(f"**‚öñÔ∏è Tr·∫°ng th√°i**: {source['document_state']}")
                st.write(f"**üìç ƒêi·ªÅu kho·∫£n**: {source['element_number']}")
                if source['element_name']:
                    st.write(f"**üè∑Ô∏è T√™n ƒëi·ªÅu kho·∫£n**: {source['element_name']}")
            
            with col2:
                st.metric("Relevance Score", f"{source['relevance_score']:.2f}")
                st.metric("Element Type", source['element_type'])
                st.metric("Priority", "High" if source['is_active'] else "Low")
            
            st.markdown("**üìù N·ªôi dung ƒë·∫ßy ƒë·ªß:**")
            st.write(source['element_content'])

# =================== MAIN APPLICATION ===================

def main():
    # Initialize systems
    init_session_state()
    init_vbpl_system()
    
    # CSS
    st.markdown("""
    <style>
    .assistant-message {
        background: #f0f8ff;
        padding: 15px;
        border-radius: 15px;
        margin: 10px 0;
        max-width: 80%;
        border-left: 4px solid #4CAF50;
    }
    .assistant-message::before { 
        content: "‚öñÔ∏è Tr·ª£ l√Ω Ph√°p ch·∫ø: "; 
        font-weight: bold; 
        color: #2E7D32;
    }
    
    .user-message {
        background: #e3f2fd;
        padding: 15px;
        border-radius: 15px;
        margin: 10px 0 10px auto;
        max-width: 80%;
        text-align: right;
        border-right: 4px solid #2196F3;
    }
    .user-message::before { 
        content: "üë§ B·∫°n: "; 
        font-weight: bold; 
        color: #1976D2;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Header
    st.markdown("""
    <div style="text-align: center; padding: 20px; background: linear-gradient(90deg, #2E7D32, #4CAF50); border-radius: 10px; margin-bottom: 20px;">
        <h1 style="color: white; margin: 0;">‚öñÔ∏è Tr·ª£ l√Ω Ph√°p ch·∫ø Kho√°ng s·∫£n</h1>
        <p style="color: #E8F5E8; margin: 5px 0 0 0;">H·ªó tr·ª£ b·ªüi C∆° s·ªü d·ªØ li·ªáu Ph√°p lu·∫≠t VBPL</p>
        <p style="color: #E8F5E8; margin: 5px 0 0 0; font-size: 12px;">üóÑÔ∏è Database-Powered ‚Ä¢ Accurate Legal Content ‚Ä¢ Real-time Search</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.markdown("### ‚öôÔ∏è C√†i ƒë·∫∑t h·ªá th·ªëng")
        
        # VBPL status
        if is_vbpl_available():
            vbpl_enabled = st.toggle("üóÑÔ∏è S·ª≠ d·ª•ng VBPL Database", value=True)
            st.success("‚úÖ VBPL Database Online")
        else:
            vbpl_enabled = False
            st.toggle("üóÑÔ∏è S·ª≠ d·ª•ng VBPL Database", value=False, disabled=True)
            st.error("‚ùå VBPL Database Offline")
            st.info("üí° C·∫ßn file vbpl.db ƒë·ªÉ s·ª≠ d·ª•ng")
        
        # Model selection
        model_options = ["gpt-4o-mini", "gpt-3.5-turbo", "gpt-4", "gpt-4-turbo-preview"]
        model_info = {
            "gpt-4o-mini": "üí∞ R·∫ª nh·∫•t ($0.15/$0.6 per 1K tokens)",
            "gpt-3.5-turbo": "‚öñÔ∏è C√¢n b·∫±ng ($1.5/$2 per 1K tokens)", 
            "gpt-4": "üß† Th√¥ng minh ($30/$60 per 1K tokens)",
            "gpt-4-turbo-preview": "üöÄ Nhanh ($10/$30 per 1K tokens)"
        }
        
        default_model = get_default_model()
        default_index = model_options.index(default_model) if default_model in model_options else 0
        
        selected_model = st.selectbox("ü§ñ Ch·ªçn model AI:", model_options, index=default_index)
        st.caption(model_info[selected_model])
        
        # Temperature
        temperature = st.slider("üå°Ô∏è ƒê·ªô s√°ng t·∫°o:", 0.0, 1.0, 0.1, 0.1)
        
        st.markdown("---")
        
        # Statistics
        st.markdown("### üìä Th·ªëng k√™ s·ª≠ d·ª•ng")
        try:
            stats = st.session_state.token_stats
            total_tokens = stats["total_input_tokens"] + stats["total_output_tokens"]
            
            st.metric("üéØ T·ªïng Token", f"{total_tokens:,}")
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("üì• Input", f"{stats['total_input_tokens']:,}")
            with col2:
                st.metric("üì§ Output", f"{stats['total_output_tokens']:,}")
            
            st.metric("üí∞ Chi ph√≠ (USD)", f"${stats['total_cost']:.4f}")
            st.metric("üí∏ Chi ph√≠ (VND)", f"{stats['total_cost'] * 24000:,.0f}ƒë")
            st.metric("üìû S·ªë l∆∞·ª£t h·ªèi", stats['request_count'])
            
        except Exception as e:
            st.error(f"Error displaying stats: {e}")
        
        # Reset buttons
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üîÑ Reset stats", use_container_width=True):
                st.session_state.token_stats = {
                    "total_input_tokens": 0,
                    "total_output_tokens": 0,
                    "total_cost": 0.0,
                    "session_start": datetime.now(),
                    "request_count": 0
                }
                st.rerun()
        
        with col2:
            if st.button("üóëÔ∏è X√≥a chat", use_container_width=True):
                st.session_state.messages = [
                    {"role": "system", "content": get_system_prompt()},
                    {"role": "assistant", "content": get_welcome_message()}
                ]
                st.rerun()
        
        st.markdown("---")
        st.markdown("### üóÑÔ∏è VBPL Features")
        if is_vbpl_available():
            st.success("‚úÖ Structured legal database")
            st.success("‚úÖ Domain-specific filtering")
            st.success("‚úÖ Document status prioritization")
            st.success("‚úÖ Professional AI prompting")
            st.info("üí° T√¨m ki·∫øm ch√≠nh x√°c trong database ph√°p lu·∫≠t")
        else:
            st.info("üí° VBPL database cung c·∫•p:")
            st.write("‚Ä¢ T√¨m ki·∫øm structured content")
            st.write("‚Ä¢ ∆Øu ti√™n vƒÉn b·∫£n c√≤n hi·ªáu l·ª±c")
            st.write("‚Ä¢ Tr√≠ch d·∫´n ch√≠nh x√°c ƒëi·ªÅu kho·∫£n")
            st.write("‚Ä¢ Filtering domain kho√°ng s·∫£n")
    
    # Check OpenAI API
    if not st.secrets.get("OPENAI_API_KEY"):
        st.error("‚ùå Ch∆∞a c·∫•u h√¨nh OPENAI_API_KEY trong secrets!")
        st.stop()
    
    # Initialize OpenAI client
    try:
        client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
    except Exception as e:
        st.error(f"‚ùå L·ªói kh·ªüi t·∫°o OpenAI client: {str(e)}")
        st.stop()
    
    # Display chat history
    for message in st.session_state.messages:
        if message["role"] == "assistant":
            st.markdown(f'<div class="assistant-message">{message["content"]}</div>', 
                       unsafe_allow_html=True)
        elif message["role"] == "user":
            st.markdown(f'<div class="user-message">{message["content"]}</div>', 
                       unsafe_allow_html=True)
    
    # Chat input
    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi v·ªÅ ph√°p lu·∫≠t kho√°ng s·∫£n..."):
        
        # Check if mineral related
        if not is_mineral_related(prompt):
            st.session_state.messages.append({"role": "user", "content": prompt})
            st.markdown(f'<div class="user-message">{prompt}</div>', unsafe_allow_html=True)
            
            polite_refusal = """Xin l·ªói, t√¥i l√† tr·ª£ l√Ω chuy√™n v·ªÅ **ph√°p lu·∫≠t kho√°ng s·∫£n** t·∫°i Vi·ªát Nam.

T√¥i ch·ªâ c√≥ th·ªÉ t∆∞ v·∫•n v·ªÅ c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn:
- üèîÔ∏è Lu·∫≠t Kho√°ng s·∫£n v√† c√°c vƒÉn b·∫£n h∆∞·ªõng d·∫´n
- ‚öñÔ∏è Th·ªß t·ª•c c·∫•p ph√©p thƒÉm d√≤, khai th√°c kho√°ng s·∫£n
- üí∞ Thu·∫ø, ph√≠ li√™n quan ƒë·∫øn kho√°ng s·∫£n
- üå± B·∫£o v·ªá m√¥i tr∆∞·ªùng trong ho·∫°t ƒë·ªông kho√°ng s·∫£n
- ‚ö†Ô∏è X·ª≠ ph·∫°t vi ph·∫°m h√†nh ch√≠nh

H√£y ƒë·∫∑t c√¢u h·ªèi v·ªÅ lƒ©nh v·ª±c kho√°ng s·∫£n ƒë·ªÉ t√¥i c√≥ th·ªÉ h·ªó tr·ª£ b·∫°n t·ªët nh·∫•t! üòä"""
            
            st.session_state.messages.append({"role": "assistant", "content": polite_refusal})
            st.markdown(f'<div class="assistant-message">{polite_refusal}</div>', 
                       unsafe_allow_html=True)
            return
        
        # Add user message
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.markdown(f'<div class="user-message">{prompt}</div>', unsafe_allow_html=True)
        
        # Process with VBPL if available
        if vbpl_enabled and is_vbpl_available():
            with st.spinner("ü§î ƒêang ph√¢n t√≠ch c√¢u h·ªèi v·ªõi VBPL database..."):
                
                vbpl_result = process_with_vbpl(prompt)
                
                if vbpl_result.get('success'):
                    # Successful VBPL response
                    response = vbpl_result['response']
                    
                    # Display response
                    st.markdown(f'<div class="assistant-message">{response}</div>', 
                               unsafe_allow_html=True)
                    
                    # Show detailed results
                    if vbpl_result.get('sources'):
                        show_vbpl_results_details(vbpl_result)
                    
                    # Update token stats (estimate)
                    input_tokens = count_tokens(prompt)
                    output_tokens = count_tokens(response)
                    update_stats(input_tokens, output_tokens, selected_model)
                    
                else:
                    # VBPL failed, show fallback response
                    response = vbpl_result['response']
                    st.markdown(f'<div class="assistant-message">{response}</div>', 
                               unsafe_allow_html=True)
        
        else:
            # Fallback when VBPL not available
            fallback_response = """üóÑÔ∏è **C∆° s·ªü d·ªØ li·ªáu VBPL kh√¥ng kh·∫£ d·ª•ng**

ƒê·ªÉ c√≥ th√¥ng tin ch√≠nh x√°c nh·∫•t v·ªÅ v·∫•n ƒë·ªÅ n√†y, b·∫°n vui l√≤ng:

1. **Tham kh·∫£o tr·ª±c ti·∫øp** t·∫°i thuvienphapluat.vn
2. **Li√™n h·ªá** S·ªü T√†i nguy√™n v√† M√¥i tr∆∞·ªùng ƒë·ªãa ph∆∞∆°ng
3. **Tham kh·∫£o** Lu·∫≠t Kho√°ng s·∫£n hi·ªán h√†nh v√† c√°c ngh·ªã ƒë·ªãnh h∆∞·ªõng d·∫´n

üí° **G·ª£i √Ω**: ƒê·ªÉ s·ª≠ d·ª•ng t√≠nh nƒÉng t√¨m ki·∫øm database ch√≠nh x√°c, vui l√≤ng cung c·∫•p file `vbpl.db`."""
            
            st.markdown(f'<div class="assistant-message">{fallback_response}</div>', 
                       unsafe_allow_html=True)
        
        # Add response to history
        st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main()