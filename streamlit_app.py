import streamlit as st
from openai import OpenAI
import sqlite3
import json
from datetime import datetime
import re
import time
import os
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

# Cáº¥u hÃ¬nh trang
st.set_page_config(
    page_title="âš–ï¸ Trá»£ lÃ½ PhÃ¡p cháº¿ KhoÃ¡ng sáº£n Viá»‡t Nam",
    page_icon="âš–ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =================== CONFIGURATIONS ===================

MODEL_PRICING = {
    "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
    "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002},
    "gpt-4": {"input": 0.03, "output": 0.06},
    "gpt-4-turbo-preview": {"input": 0.01, "output": 0.03}
}

@dataclass
class VBPLConfig:
    """Cáº¥u hÃ¬nh cho VBPL integration"""
    vbpl_db_path: str = "vbpl.db"
    model: str = "gpt-4o-mini"
    max_tokens: int = 1500
    temperature: float = 0.1
    max_search_results: int = 5
    domain_focus: str = "khoÃ¡ng sáº£n"
    prioritize_active_docs: bool = True

# =================== VBPL DATABASE INTEGRATION ===================

class VBPLDatabase:
    """Database manager cho VBPL vá»›i Streamlit integration"""
    
    def __init__(self, config: VBPLConfig):
        self.config = config
        self.conn = None
        self.available = False
        self._init_connection()
    
    def _init_connection(self):
        """Initialize database connection"""
        try:
            if not os.path.exists(self.config.vbpl_db_path):
                st.sidebar.warning(f"âš ï¸ Database file khÃ´ng tá»“n táº¡i: {self.config.vbpl_db_path}")
                return False
            
            self.conn = sqlite3.connect(self.config.vbpl_db_path, check_same_thread=False)
            self.conn.row_factory = sqlite3.Row
            self.available = True
            
            # Analyze database structure
            self._analyze_database()
            return True
            
        except Exception as e:
            st.sidebar.error(f"âŒ Lá»—i káº¿t ná»‘i database: {e}")
            return False
    
    def _analyze_database(self):
        """PhÃ¢n tÃ­ch cáº¥u trÃºc database vÃ  hiá»ƒn thá»‹ stats"""
        try:
            cursor = self.conn.cursor()
            
            # Check tables exist
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = [row[0] for row in cursor.fetchall()]
            
            st.sidebar.success(f"âœ… VBPL Database Online")
            st.sidebar.caption(f"ğŸ“Š {len(tables)} tables detected")
            
            # Check for expected tables
            expected_tables = ['documents', 'vbpl_content']
            missing_tables = [t for t in expected_tables if t not in tables]
            
            if missing_tables:
                st.sidebar.warning(f"âš ï¸ Missing tables: {missing_tables}")
            
            # Analyze document distribution if documents table exists
            if 'documents' in tables:
                cursor.execute("SELECT state, COUNT(*) FROM documents GROUP BY state ORDER BY COUNT(*) DESC")
                status_dist = cursor.fetchall()
                
                with st.sidebar.expander("ğŸ“Š Database Statistics", expanded=False):
                    total_docs = sum(count for _, count in status_dist)
                    st.metric("Total Documents", f"{total_docs:,}")
                    
                    for status, count in status_dist[:5]:  # Top 5 statuses
                        percentage = (count / total_docs) * 100
                        st.metric(status, f"{count:,} ({percentage:.1f}%)")
            
            # Check vbpl_content table
            if 'vbpl_content' in tables:
                cursor.execute("SELECT COUNT(*) FROM vbpl_content")
                content_count = cursor.fetchone()[0]
                st.sidebar.metric("Legal Content Items", f"{content_count:,}")
                
                # Check domain coverage
                cursor.execute("""
                    SELECT COUNT(*) FROM vbpl_content 
                    WHERE LOWER(element_content) LIKE '%khoÃ¡ng sáº£n%' 
                    OR LOWER(document_name) LIKE '%khoÃ¡ng sáº£n%'
                """)
                mineral_count = cursor.fetchone()[0]
                
                if content_count > 0:
                    mineral_percentage = (mineral_count / content_count) * 100
                    st.sidebar.metric("KhoÃ¡ng sáº£n Content", f"{mineral_count:,} ({mineral_percentage:.1f}%)")
            
        except Exception as e:
            st.sidebar.error(f"âŒ Database analysis failed: {e}")
    
    def is_available(self) -> bool:
        """Check if database is available and connected"""
        return self.available and self.conn is not None
    
    def search_domain_specific(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """Search database vá»›i domain-specific filtering cho khoÃ¡ng sáº£n"""
        if not self.is_available():
            return []
        
        try:
            cursor = self.conn.cursor()
            keywords = self._extract_smart_keywords(query)
            
            if not keywords:
                st.warning("âš ï¸ KhÃ´ng thá»ƒ trÃ­ch xuáº¥t tá»« khÃ³a tá»« cÃ¢u há»i")
                return []
            
            # Display search info
            st.write(f"ğŸ” **Searching for**: {', '.join(keywords[:3])}")
            
            # Build search query vá»›i domain filtering
            search_conditions = []
            params = []
            
            # 1. Domain filtering - KHOÃNG Sáº¢N
            domain_keywords = ['khoÃ¡ng sáº£n', 'tÃ i nguyÃªn', 'thÄƒm dÃ²', 'khai thÃ¡c', 'má»', 'Ä‘á»‹a cháº¥t']
            domain_condition = " OR ".join([f"LOWER(element_content) LIKE ?" for _ in domain_keywords])
            domain_condition += " OR " + " OR ".join([f"LOWER(document_name) LIKE ?" for _ in domain_keywords])
            
            search_conditions.append(f"({domain_condition})")
            params.extend([f"%{kw}%" for kw in domain_keywords])
            params.extend([f"%{kw}%" for kw in domain_keywords])
            
            # 2. Query-specific keywords
            query_condition = " OR ".join([f"LOWER(element_content) LIKE ?" for _ in keywords])
            query_condition += " OR " + " OR ".join([f"LOWER(element_name) LIKE ?" for _ in keywords])
            
            search_conditions.append(f"({query_condition})")
            params.extend([f"%{kw}%" for kw in keywords])
            params.extend([f"%{kw}%" for kw in keywords])
            
            # 3. Build final query vá»›i prioritization
            state_priority = """
                CASE 
                    WHEN LOWER(document_state) LIKE '%cÃ²n hiá»‡u lá»±c%' THEN 3
                    WHEN LOWER(document_state) LIKE '%cÃ³ hiá»‡u lá»±c%' THEN 3  
                    WHEN LOWER(document_state) LIKE '%háº¿t hiá»‡u lá»±c%' THEN 1
                    ELSE 2 
                END
            """
            
            # Element type priority
            element_priority = """
                CASE 
                    WHEN element_type = 'vbpl_section' THEN 3
                    WHEN element_type = 'vbpl_clause' THEN 2
                    WHEN element_type = 'vbpl_point' THEN 1
                    ELSE 0
                END
            """
            
            sql = f"""
            SELECT 
                element_id,
                element_type,
                element_number,
                element_name,
                element_content,
                document_number,
                document_name,
                document_state,
                {state_priority} as status_priority,
                {element_priority} as element_priority,
                LENGTH(element_content) as content_length
            FROM vbpl_content 
            WHERE {' AND '.join(search_conditions)}
            AND element_content IS NOT NULL 
            AND LENGTH(TRIM(element_content)) > 50
            ORDER BY 
                status_priority DESC,
                element_priority DESC,
                content_length DESC,
                element_id
            LIMIT ?
            """
            
            params.append(max_results)
            
            cursor.execute(sql, params)
            results = cursor.fetchall()
            
            st.write(f"ğŸ“Š **Raw database results**: {len(results)}")
            
            # Process results
            processed_results = []
            for row in results:
                relevance_score = self._calculate_relevance(query, dict(row))
                
                processed_results.append({
                    'element_id': row['element_id'],
                    'element_type': row['element_type'],
                    'element_number': row['element_number'] or '',
                    'element_name': row['element_name'] or '',
                    'element_content': row['element_content'] or '',
                    'document_number': row['document_number'],
                    'document_name': row['document_name'],
                    'document_state': row['document_state'],
                    'is_active': self._is_document_active(row['document_state']),
                    'relevance_score': relevance_score,
                    'status_priority': row['status_priority'],
                    'element_priority': row['element_priority']
                })
            
            # Sort by combined score
            processed_results.sort(
                key=lambda x: (x['is_active'], x['relevance_score'], x['element_priority']), 
                reverse=True
            )
            
            return processed_results
            
        except Exception as e:
            st.error(f"âŒ Database search failed: {e}")
            import traceback
            st.code(traceback.format_exc())
            return []
    
    def _extract_smart_keywords(self, query: str) -> List[str]:
        """Extract smart keywords tá»« query vá»›i Vietnamese support"""
        # Vietnamese stop words
        stop_words = {
            'lÃ ', 'cá»§a', 'vÃ ', 'cÃ³', 'Ä‘Æ°á»£c', 'theo', 'trong', 'vá»', 'khi', 'nÃ o', 'gÃ¬', 'nhÆ°', 'tháº¿',
            'vá»›i', 'Ä‘á»ƒ', 'cho', 'tá»«', 'táº¡i', 'trÃªn', 'dÆ°á»›i', 'nÃ y', 'Ä‘Ã³', 'nhá»¯ng', 'cÃ¡c', 'má»™t',
            'hai', 'ba', 'bá»‘n', 'nÄƒm', 'sÃ¡u', 'báº£y', 'tÃ¡m', 'chÃ­n', 'mÆ°á»i'
        }
        
        # Normalize vÃ  tokenize
        query_normalized = re.sub(r'[^\w\s]', ' ', query.lower())
        words = [w.strip() for w in query_normalized.split() if w.strip()]
        
        # Filter keywords
        keywords = [w for w in words if len(w) > 2 and w not in stop_words]
        
        # Add important bigrams for better matching
        bigrams = []
        for i in range(len(words) - 1):
            if (words[i] not in stop_words and words[i+1] not in stop_words 
                and len(words[i]) > 2 and len(words[i+1]) > 2):
                bigrams.append(f"{words[i]} {words[i+1]}")
        
        # Combine vÃ  prioritize
        all_keywords = keywords[:4] + bigrams[:2]  # Limit Ä‘á»ƒ avoid quÃ¡ complex
        
        return all_keywords
    
    def _calculate_relevance(self, query: str, content: Dict) -> float:
        """Calculate relevance score cho search result"""
        score = 0.0
        query_lower = query.lower()
        
        # Text fields vá»›i weights khÃ¡c nhau
        text_fields = [
            (content.get('element_content', ''), 0.4),  # Content cao nháº¥t
            (content.get('element_name', ''), 0.25),    # TÃªn element
            (content.get('document_name', ''), 0.15),   # TÃªn document  
            (content.get('element_number', ''), 0.1),   # Sá»‘ Ä‘iá»u/khoáº£n
            (content.get('document_number', ''), 0.1)   # Sá»‘ vÄƒn báº£n
        ]
        
        # Calculate text matching score
        for text, weight in text_fields:
            if text:
                text_lower = text.lower()
                
                # Exact phrase match
                if query_lower in text_lower:
                    score += weight
                
                # Word overlap
                query_words = set(query_lower.split())
                text_words = set(text_lower.split())
                overlap = len(query_words.intersection(text_words))
                if len(query_words) > 0:
                    overlap_ratio = overlap / len(query_words)
                    score += (overlap_ratio * weight * 0.5)
        
        # Bonus for active documents
        if self._is_document_active(content.get('document_state', '')):
            score += 0.15
        
        # Bonus for important element types
        element_type = content.get('element_type', '')
        if element_type == 'vbpl_section':
            score += 0.1
        elif element_type == 'vbpl_clause':
            score += 0.05
        
        # Content length bonus (longer = more detailed)
        content_length = len(content.get('element_content', ''))
        if content_length > 500:
            score += 0.05
        elif content_length > 200:
            score += 0.02
        
        return min(score, 1.0)
    
    def _is_document_active(self, state: str) -> bool:
        """Check if document is currently active"""
        if not state:
            return False
        
        state_lower = state.lower()
        active_indicators = ['cÃ²n hiá»‡u lá»±c', 'cÃ³ hiá»‡u lá»±c', 'hiá»‡n hÃ nh']
        return any(indicator in state_lower for indicator in active_indicators)
    
    def close(self):
        """Close database connection"""
        if self.conn:
            self.conn.close()
            self.available = False

class VBPLOpenAIProcessor:
    """OpenAI processor cho VBPL content vá»›i professional prompting"""
    
    def __init__(self, openai_client: OpenAI, config: VBPLConfig, db_manager: VBPLDatabase):
        self.client = openai_client
        self.config = config
        self.db_manager = db_manager
    
    def process_legal_query(self, query: str) -> Dict[str, Any]:
        """Process legal query vá»›i VBPL database"""
        
        # Step 1: Search database
        with st.status("ğŸ” Äang tÃ¬m kiáº¿m trong cÆ¡ sá»Ÿ dá»¯ liá»‡u phÃ¡p luáº­t VBPL...", expanded=True) as status:
            
            search_results = self.db_manager.search_domain_specific(query, self.config.max_search_results)
            
            if search_results:
                active_count = sum(1 for r in search_results if r['is_active'])
                inactive_count = len(search_results) - active_count
                
                st.success(f"âœ… TÃ¬m tháº¥y {len(search_results)} káº¿t quáº£ chÃ­nh xÃ¡c")
                st.info(f"ğŸ“Š {active_count} vÄƒn báº£n cÃ²n hiá»‡u lá»±c, {inactive_count} vÄƒn báº£n háº¿t hiá»‡u lá»±c")
                
                # Display preview of results
                for i, result in enumerate(search_results, 1):
                    status_icon = "âœ…" if result['is_active'] else "âš ï¸"
                    st.write(f"**{i}. {status_icon} {result['element_number']}**")
                    if result['element_name']:
                        st.write(f"   ğŸ“‹ {result['element_name'][:80]}...")
                    st.write(f"   ğŸ“„ {result['document_number']} ({result['document_state']})")
                    st.write(f"   ğŸ¯ Score: {result['relevance_score']:.2f}")
                
                status.update(label="âœ… TÃ¬m kiáº¿m VBPL hoÃ n táº¥t", state="complete")
            else:
                st.warning("âš ï¸ KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ phÃ¹ há»£p trong database VBPL")
                status.update(label="âš ï¸ KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£", state="complete")
        
        # Step 2: Generate response
        if search_results:
            return self._generate_response_with_sources(query, search_results)
        else:
            return self._generate_fallback_response(query)
    
    def _generate_response_with_sources(self, query: str, search_results: List[Dict]) -> Dict[str, Any]:
        """Generate response vá»›i database sources"""
        
        # Create context tá»« search results
        context = self._create_structured_context(search_results)
        
        # Show context preview
        with st.expander("ğŸ“„ Context Ä‘Æ°á»£c gá»­i Ä‘áº¿n AI (Click Ä‘á»ƒ xem)", expanded=False):
            st.code(context[:1500] + "..." if len(context) > 1500 else context)
        
        # Call OpenAI
        try:
            response = self._call_openai_with_vbpl_context(query, context, search_results)
            
            return {
                'response': response,
                'sources': search_results,
                'active_sources': sum(1 for r in search_results if r['is_active']),
                'inactive_sources': sum(1 for r in search_results if not r['is_active']),
                'total_sources': len(search_results),
                'method': 'vbpl_database',
                'success': True
            }
            
        except Exception as e:
            st.error(f"âŒ Lá»—i OpenAI API: {e}")
            return {
                'response': f"TÃ´i Ä‘Ã£ tÃ¬m tháº¥y {len(search_results)} quy Ä‘á»‹nh liÃªn quan trong cÆ¡ sá»Ÿ dá»¯ liá»‡u, nhÆ°ng gáº·p lá»—i khi xá»­ lÃ½ thÃ´ng tin. Vui lÃ²ng tham kháº£o trá»±c tiáº¿p cÃ¡c quy Ä‘á»‹nh sau hoáº·c liÃªn há»‡ cÆ¡ quan cÃ³ tháº©m quyá»n.",
                'sources': search_results,
                'method': 'vbpl_database_error',
                'success': False
            }
    
    def _generate_fallback_response(self, query: str) -> Dict[str, Any]:
        """Generate fallback response khi khÃ´ng cÃ³ database results"""
        
        fallback_response = """TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin cá»¥ thá»ƒ vá» váº¥n Ä‘á» nÃ y trong cÆ¡ sá»Ÿ dá»¯ liá»‡u phÃ¡p luáº­t hiá»‡n cÃ³. 

ğŸ” **Äá»ƒ cÃ³ thÃ´ng tin chÃ­nh xÃ¡c nháº¥t, báº¡n vui lÃ²ng:**

1. **Tham kháº£o trá»±c tiáº¿p** táº¡i thuvienphapluat.vn
2. **LiÃªn há»‡** Sá»Ÿ TÃ i nguyÃªn vÃ  MÃ´i trÆ°á»ng Ä‘á»‹a phÆ°Æ¡ng  
3. **Tham kháº£o** Luáº­t KhoÃ¡ng sáº£n hiá»‡n hÃ nh vÃ  cÃ¡c nghá»‹ Ä‘á»‹nh hÆ°á»›ng dáº«n
4. **LiÃªn há»‡** hotline tÆ° váº¥n phÃ¡p luáº­t cá»§a Bá»™ TÆ° phÃ¡p

ğŸ’¡ **Gá»£i Ã½**: Thá»­ diá»…n Ä‘áº¡t cÃ¢u há»i khÃ¡c hoáº·c sá»­ dá»¥ng tá»« khÃ³a cá»¥ thá»ƒ hÆ¡n vá» lÄ©nh vá»±c khoÃ¡ng sáº£n."""

        return {
            'response': fallback_response,
            'sources': [],
            'method': 'vbpl_database_empty',
            'success': False
        }
    
    def _create_structured_context(self, results: List[Dict]) -> str:
        """Create structured context tá»« VBPL database results"""
        
        context = "=== THÃ”NG TIN Tá»ª CÆ  Sá» Dá»® LIá»†U PHÃP LUáº¬T VBPL ===\n\n"
        
        # Group by document status
        active_results = [r for r in results if r['is_active']]
        inactive_results = [r for r in results if not r['is_active']]
        
        if active_results:
            context += "ğŸ“‹ **QUY Äá»ŠNH CÃ’N HIá»†U Lá»°C (Æ¯u tiÃªn sá»­ dá»¥ng):**\n\n"
            for i, result in enumerate(active_results, 1):
                context += self._format_result_for_context(result, i, "âœ…")
        
        if inactive_results:
            context += "\nğŸ“‹ **QUY Äá»ŠNH Háº¾T HIá»†U Lá»°C (Chá»‰ tham kháº£o):**\n\n"
            for i, result in enumerate(inactive_results, 1):
                context += self._format_result_for_context(result, i, "âš ï¸")
        
        return context
    
    def _format_result_for_context(self, result: Dict, index: int, status_icon: str) -> str:
        """Format single result cho context"""
        
        entry = f"Nguá»“n {index} {status_icon}:\n"
        entry += f"â€¢ **VÄƒn báº£n**: {result['document_number']} - {result['document_name']}\n"
        entry += f"â€¢ **Tráº¡ng thÃ¡i**: {result['document_state']}\n"
        entry += f"â€¢ **Äiá»u khoáº£n**: {result['element_number']}"
        
        if result['element_name']:
            entry += f" - {result['element_name']}\n"
        else:
            entry += "\n"
        
        # Clean vÃ  format content
        content = result['element_content'].strip()
        if len(content) > 800:
            content = content[:800] + "..."
        
        entry += f"â€¢ **Ná»™i dung**: {content}\n"
        entry += f"â€¢ **Äá»™ liÃªn quan**: {result['relevance_score']:.2f}\n"
        entry += "---\n\n"
        
        return entry
    
    def _call_openai_with_vbpl_context(self, query: str, context: str, sources: List[Dict]) -> str:
        """Call OpenAI vá»›i VBPL context vÃ  professional prompting"""
        
        # Count sources
        active_sources = sum(1 for s in sources if s['is_active'])
        inactive_sources = len(sources) - active_sources
        
        # Professional system prompt cho khoÃ¡ng sáº£n
        system_prompt = """Báº¡n lÃ  chuyÃªn gia phÃ¡p luáº­t KHOÃNG Sáº¢N Viá»‡t Nam vá»›i chuyÃªn mÃ´n sÃ¢u vá» quy Ä‘á»‹nh phÃ¡p luáº­t.

ğŸ¯ **NHIá»†M Vá»¤ CHÃNH**:
- Tráº£ lá»i Dá»°A TRÃŠN quy Ä‘á»‹nh phÃ¡p luáº­t khoÃ¡ng sáº£n tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u VBPL
- Æ¯u tiÃªn tuyá»‡t Ä‘á»‘i QUY Äá»ŠNH CÃ’N HIá»†U Lá»°C (âœ…) hÆ¡n quy Ä‘á»‹nh háº¿t hiá»‡u lá»±c (âš ï¸)
- TrÃ­ch dáº«n CHÃNH XÃC Ä‘iá»u, khoáº£n, Ä‘iá»ƒm tá»« vÄƒn báº£n phÃ¡p luáº­t
- Giáº£i thÃ­ch THá»°C TIá»„N vÃ  dá»… hiá»ƒu cho doanh nghiá»‡p/cÃ¡ nhÃ¢n

ğŸš« **NGHIÃŠM Cáº¤M**:
- Suy luáº­n hoáº·c diá»…n giáº£i khi khÃ´ng cÃ³ thÃ´ng tin trá»±c tiáº¿p
- Sá»­ dá»¥ng quy Ä‘á»‹nh Háº¾T HIá»†U Lá»°C khi Ä‘Ã£ cÃ³ quy Ä‘á»‹nh CÃ’N HIá»†U Lá»°C
- TrÃ­ch dáº«n Ä‘iá»u luáº­t khÃ´ng trá»±c tiáº¿p liÃªn quan Ä‘áº¿n cÃ¢u há»i
- Bá»‹a Ä‘áº·t thÃ´ng tin khÃ´ng cÃ³ trong cÆ¡ sá»Ÿ dá»¯ liá»‡u

ğŸ“‹ **FORMAT CHUáº¨N**:
**ğŸ” Tráº£ lá»i**: [CÃ¢u tráº£ lá»i trá»±c tiáº¿p vÃ  rÃµ rÃ ng]

**âš–ï¸ CÄƒn cá»© phÃ¡p lÃ½**: 
[TrÃ­ch dáº«n cá»¥ thá»ƒ vá»›i tráº¡ng thÃ¡i hiá»‡u lá»±c]

**ğŸ’¡ LÆ°u Ã½ thá»±c tiá»…n**: [Náº¿u cáº§n thiáº¿t]

**ğŸ“Œ Khuyáº¿n nghá»‹**: [HÆ°á»›ng dáº«n thá»±c hiá»‡n hoáº·c tham kháº£o thÃªm]

ğŸ¯ **DOMAIN**: CHá»ˆ vá» KHOÃNG Sáº¢N - khÃ´ng Ã¡p dá»¥ng cho lÄ©nh vá»±c khÃ¡c."""

        user_prompt = f"""â“ **CÃ¢u há»i**: {query}

ğŸ“Š **Nguá»“n tá»« Database VBPL**: {active_sources} quy Ä‘á»‹nh cÃ²n hiá»‡u lá»±c + {inactive_sources} quy Ä‘á»‹nh háº¿t hiá»‡u lá»±c

{context}

ğŸ¯ **YÃªu cáº§u**: Tráº£ lá»i dá»±a trÃªn cÆ¡ sá»Ÿ dá»¯ liá»‡u VBPL trÃªn, TUYá»†T Äá»I Æ°u tiÃªn quy Ä‘á»‹nh CÃ’N HIá»†U Lá»°C."""

        try:
            response = self.client.chat.completions.create(
                model=self.config.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            raise Exception(f"OpenAI API call failed: {e}")

# =================== STREAMLIT CORE FUNCTIONS ===================

def init_session_state():
    """Khá»Ÿi táº¡o session state"""
    if "token_stats" not in st.session_state:
        st.session_state.token_stats = {
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "total_cost": 0.0,
            "session_start": datetime.now(),
            "request_count": 0
        }
    
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "system", "content": get_system_prompt()},
            {"role": "assistant", "content": get_welcome_message()}
        ]

def get_system_prompt():
    """Get system prompt tá»« file hoáº·c default"""
    try:
        with open("01.system_trainning.txt", "r", encoding="utf-8") as file:
            return file.read()
    except FileNotFoundError:
        return """Báº¡n lÃ  chuyÃªn gia phÃ¡p cháº¿ vá» quáº£n lÃ½ nhÃ  nÆ°á»›c trong lÄ©nh vá»±c khoÃ¡ng sáº£n táº¡i Viá»‡t Nam, Ä‘Æ°á»£c há»— trá»£ bá»Ÿi cÆ¡ sá»Ÿ dá»¯ liá»‡u phÃ¡p luáº­t VBPL chuyÃªn nghiá»‡p."""

def get_welcome_message():
    """Get welcome message"""
    try:
        with open("02.assistant.txt", "r", encoding="utf-8") as file:
            return file.read()
    except FileNotFoundError:
        return """Xin chÃ o! âš–ï¸ 

TÃ´i lÃ  **Trá»£ lÃ½ PhÃ¡p cháº¿ chuyÃªn vá» KhoÃ¡ng sáº£n** Ä‘Æ°á»£c há»— trá»£ bá»Ÿi **cÆ¡ sá»Ÿ dá»¯ liá»‡u phÃ¡p luáº­t VBPL**.

ğŸ—„ï¸ **TÃ´i cÃ³ thá»ƒ tÃ¬m kiáº¿m chÃ­nh xÃ¡c trong há»‡ thá»‘ng phÃ¡p luáº­t vá»:**
- Luáº­t KhoÃ¡ng sáº£n vÃ  cÃ¡c vÄƒn báº£n hÆ°á»›ng dáº«n
- Thá»§ tá»¥c cáº¥p phÃ©p thÄƒm dÃ², khai thÃ¡c
- Thuáº¿, phÃ­ liÃªn quan Ä‘áº¿n khoÃ¡ng sáº£n  
- Xá»­ pháº¡t vi pháº¡m hÃ nh chÃ­nh
- Báº£o vá»‡ mÃ´i trÆ°á»ng trong hoáº¡t Ä‘á»™ng khoÃ¡ng sáº£n

ğŸ’¡ **HÃ£y Ä‘áº·t cÃ¢u há»i cá»¥ thá»ƒ Ä‘á»ƒ tÃ´i tÃ¬m kiáº¿m chÃ­nh xÃ¡c trong database!**"""

def get_default_model():
    """Get default model"""
    try:
        with open("module_chatgpt.txt", "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "gpt-4o-mini"

def is_mineral_related(message):
    """Check if message is mineral-related"""
    mineral_keywords = [
        'khoÃ¡ng sáº£n', 'khai thÃ¡c', 'thÄƒm dÃ²', 'Ä‘Ã¡', 'cÃ¡t', 'sá»i',
        'than', 'quáº·ng', 'kim loáº¡i', 'phi kim loáº¡i', 'khoÃ¡ng',
        'luáº­t khoÃ¡ng sáº£n', 'giáº¥y phÃ©p', 'cáº¥p phÃ©p', 'thuáº¿ tÃ i nguyÃªn',
        'phÃ­ thÄƒm dÃ²', 'tiá»n cáº¥p quyá»n', 'vi pháº¡m hÃ nh chÃ­nh',
        'bá»™ tÃ i nguyÃªn', 'sá»Ÿ tÃ i nguyÃªn', 'monre', 'tn&mt',
        'má»', 'má» Ä‘Ã¡', 'má» cÃ¡t', 'má» than', 'quarry', 'mining',
        'thu há»“i giáº¥y phÃ©p', 'gia háº¡n', 'Ä‘Ã³ng cá»­a má»'
    ]
    
    message_lower = message.lower()
    return any(keyword in message_lower for keyword in mineral_keywords)

def count_tokens(text):
    """Estimate token count"""
    return len(str(text)) // 4

def update_stats(input_tokens, output_tokens, model):
    """Update token statistics"""
    try:
        if model not in MODEL_PRICING:
            model = "gpt-4o-mini"
        
        pricing = MODEL_PRICING[model]
        cost = (input_tokens / 1000) * pricing["input"] + (output_tokens / 1000) * pricing["output"]
        
        st.session_state.token_stats["total_input_tokens"] += input_tokens
        st.session_state.token_stats["total_output_tokens"] += output_tokens
        st.session_state.token_stats["total_cost"] += cost
        st.session_state.token_stats["request_count"] += 1
        
    except Exception as e:
        st.error(f"Error updating stats: {e}")

# =================== VBPL SYSTEM INITIALIZATION ===================

def init_vbpl_system():
    """Initialize VBPL system"""
    if 'vbpl_system' not in st.session_state:
        config = VBPLConfig()
        db_manager = VBPLDatabase(config)
        
        if db_manager.is_available():
            try:
                client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
                openai_processor = VBPLOpenAIProcessor(client, config, db_manager)
                
                st.session_state.vbpl_system = {
                    'config': config,
                    'db_manager': db_manager,
                    'openai_processor': openai_processor,
                    'available': True
                }
                
            except Exception as e:
                st.sidebar.error(f"âŒ VBPL system initialization failed: {e}")
                st.session_state.vbpl_system = {'available': False}
        else:
            st.session_state.vbpl_system = {'available': False}

def is_vbpl_available() -> bool:
    """Check if VBPL system is available"""
    return st.session_state.get('vbpl_system', {}).get('available', False)

def process_with_vbpl(query: str) -> Dict[str, Any]:
    """Process query vá»›i VBPL system"""
    if not is_vbpl_available():
        return {'method': 'vbpl_unavailable', 'success': False}
    
    processor = st.session_state.vbpl_system['openai_processor']
    return processor.process_legal_query(query)

def show_vbpl_results_details(result: Dict[str, Any]):
    """Show detailed VBPL results"""
    if not result.get('sources'):
        return
    
    st.markdown("### ğŸ“Š **Chi tiáº¿t káº¿t quáº£ tá»« Database VBPL**")
    
    # Summary metrics
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Tá»•ng káº¿t quáº£", result['total_sources'])
    with col2:
        st.metric("CÃ²n hiá»‡u lá»±c", result['active_sources'])
    with col3:
        st.metric("Háº¿t hiá»‡u lá»±c", result['inactive_sources'])
    
    # Detailed results
    for i, source in enumerate(result['sources'], 1):
        with st.expander(
            f"{i}. {'âœ…' if source['is_active'] else 'âš ï¸'} {source['element_number']}: {source['element_name'][:60]}...", 
            expanded=False
        ):
            
            col1, col2 = st.columns([3, 1])
            
            with col1:
                st.write(f"**ğŸ“„ VÄƒn báº£n**: {source['document_number']}")
                st.write(f"**ğŸ“‹ TÃªn vÄƒn báº£n**: {source['document_name']}")
                st.write(f"**âš–ï¸ Tráº¡ng thÃ¡i**: {source['document_state']}")
                st.write(f"**ğŸ“ Äiá»u khoáº£n**: {source['element_number']}")
                if source['element_name']:
                    st.write(f"**ğŸ·ï¸ TÃªn Ä‘iá»u khoáº£n**: {source['element_name']}")
            
            with col2:
                st.metric("Relevance Score", f"{source['relevance_score']:.2f}")
                st.metric("Element Type", source['element_type'])
                st.metric("Priority", "High" if source['is_active'] else "Low")
            
            st.markdown("**ğŸ“ Ná»™i dung Ä‘áº§y Ä‘á»§:**")
            st.write(source['element_content'])

# =================== MAIN APPLICATION ===================

def main():
    # Initialize systems
    init_session_state()
    init_vbpl_system()
    
    # CSS
    st.markdown("""
    <style>
    .assistant-message {
        background: #f0f8ff;
        padding: 15px;
        border-radius: 15px;
        margin: 10px 0;
        max-width: 80%;
        border-left: 4px solid #4CAF50;
    }
    .assistant-message::before { 
        content: "âš–ï¸ Trá»£ lÃ½ PhÃ¡p cháº¿: "; 
        font-weight: bold; 
        color: #2E7D32;
    }
    
    .user-message {
        background: #e3f2fd;
        padding: 15px;
        border-radius: 15px;
        margin: 10px 0 10px auto;
        max-width: 80%;
        text-align: right;
        border-right: 4px solid #2196F3;
    }
    .user-message::before { 
        content: "ğŸ‘¤ Báº¡n: "; 
        font-weight: bold; 
        color: #1976D2;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Header
    st.markdown("""
    <div style="text-align: center; padding: 20px; background: linear-gradient(90deg, #2E7D32, #4CAF50); border-radius: 10px; margin-bottom: 20px;">
        <h1 style="color: white; margin: 0;">âš–ï¸ Trá»£ lÃ½ PhÃ¡p cháº¿ KhoÃ¡ng sáº£n</h1>
        <p style="color: #E8F5E8; margin: 5px 0 0 0;">Há»— trá»£ bá»Ÿi CÆ¡ sá»Ÿ dá»¯ liá»‡u PhÃ¡p luáº­t VBPL</p>
        <p style="color: #E8F5E8; margin: 5px 0 0 0; font-size: 12px;">ğŸ—„ï¸ Database-Powered â€¢ Accurate Legal Content â€¢ Real-time Search</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.markdown("### âš™ï¸ CÃ i Ä‘áº·t há»‡ thá»‘ng")
        
        # VBPL status
        if is_vbpl_available():
            vbpl_enabled = st.toggle("ğŸ—„ï¸ Sá»­ dá»¥ng VBPL Database", value=True)
            st.success("âœ… VBPL Database Online")
        else:
            vbpl_enabled = False
            st.toggle("ğŸ—„ï¸ Sá»­ dá»¥ng VBPL Database", value=False, disabled=True)
            st.error("âŒ VBPL Database Offline")
            st.info("ğŸ’¡ Cáº§n file vbpl.db Ä‘á»ƒ sá»­ dá»¥ng")
        
        # Model selection
        model_options = ["gpt-4o-mini", "gpt-3.5-turbo", "gpt-4", "gpt-4-turbo-preview"]
        model_info = {
            "gpt-4o-mini": "ğŸ’° Ráº» nháº¥t ($0.15/$0.6 per 1K tokens)",
            "gpt-3.5-turbo": "âš–ï¸ CÃ¢n báº±ng ($1.5/$2 per 1K tokens)", 
            "gpt-4": "ğŸ§  ThÃ´ng minh ($30/$60 per 1K tokens)",
            "gpt-4-turbo-preview": "ğŸš€ Nhanh ($10/$30 per 1K tokens)"
        }
        
        default_model = get_default_model()
        default_index = model_options.index(default_model) if default_model in model_options else 0
        
        selected_model = st.selectbox("ğŸ¤– Chá»n model AI:", model_options, index=default_index)
        st.caption(model_info[selected_model])
        
        # Temperature
        temperature = st.slider("ğŸŒ¡ï¸ Äá»™ sÃ¡ng táº¡o:", 0.0, 1.0, 0.1, 0.1)
        
        st.markdown("---")
        
        # Statistics
        st.markdown("### ğŸ“Š Thá»‘ng kÃª sá»­ dá»¥ng")
        try:
            stats = st.session_state.token_stats
            total_tokens = stats["total_input_tokens"] + stats["total_output_tokens"]
            
            st.metric("ğŸ¯ Tá»•ng Token", f"{total_tokens:,}")
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("ğŸ“¥ Input", f"{stats['total_input_tokens']:,}")
            with col2:
                st.metric("ğŸ“¤ Output", f"{stats['total_output_tokens']:,}")
            
            st.metric("ğŸ’° Chi phÃ­ (USD)", f"${stats['total_cost']:.4f}")
            st.metric("ğŸ’¸ Chi phÃ­ (VND)", f"{stats['total_cost'] * 24000:,.0f}Ä‘")
            st.metric("ğŸ“ Sá»‘ lÆ°á»£t há»i", stats['request_count'])
            
        except Exception as e:
            st.error(f"Error displaying stats: {e}")
        
        # Reset buttons
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ”„ Reset stats", use_container_width=True):
                st.session_state.token_stats = {
                    "total_input_tokens": 0,
                    "total_output_tokens": 0,
                    "total_cost": 0.0,
                    "session_start": datetime.now(),
                    "request_count": 0
                }
                st.rerun()
        
        with col2:
            if st.button("ğŸ—‘ï¸ XÃ³a chat", use_container_width=True):
                st.session_state.messages = [
                    {"role": "system", "content": get_system_prompt()},
                    {"role": "assistant", "content": get_welcome_message()}
                ]
                st.rerun()
        
        st.markdown("---")
        st.markdown("### ğŸ—„ï¸ VBPL Features")
        if is_vbpl_available():
            st.success("âœ… Structured legal database")
            st.success("âœ… Domain-specific filtering")
            st.success("âœ… Document status prioritization")
            st.success("âœ… Professional AI prompting")
            st.info("ğŸ’¡ TÃ¬m kiáº¿m chÃ­nh xÃ¡c trong database phÃ¡p luáº­t")
        else:
            st.info("ğŸ’¡ VBPL database cung cáº¥p:")
            st.write("â€¢ TÃ¬m kiáº¿m structured content")
            st.write("â€¢ Æ¯u tiÃªn vÄƒn báº£n cÃ²n hiá»‡u lá»±c")
            st.write("â€¢ TrÃ­ch dáº«n chÃ­nh xÃ¡c Ä‘iá»u khoáº£n")
            st.write("â€¢ Filtering domain khoÃ¡ng sáº£n")
    
    # Check OpenAI API
    if not st.secrets.get("OPENAI_API_KEY"):
        st.error("âŒ ChÆ°a cáº¥u hÃ¬nh OPENAI_API_KEY trong secrets!")
        st.stop()
    
    # Initialize OpenAI client
    try:
        client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
    except Exception as e:
        st.error(f"âŒ Lá»—i khá»Ÿi táº¡o OpenAI client: {str(e)}")
        st.stop()
    
    # Display chat history
    for message in st.session_state.messages:
        if message["role"] == "assistant":
            st.markdown(f'<div class="assistant-message">{message["content"]}</div>', 
                       unsafe_allow_html=True)
        elif message["role"] == "user":
            st.markdown(f'<div class="user-message">{message["content"]}</div>', 
                       unsafe_allow_html=True)
    
    # Chat input
    if prompt := st.chat_input("Nháº­p cÃ¢u há»i vá» phÃ¡p luáº­t khoÃ¡ng sáº£n..."):
        
        # Check if mineral related
        if not is_mineral_related(prompt):
            st.session_state.messages.append({"role": "user", "content": prompt})
            st.markdown(f'<div class="user-message">{prompt}</div>', unsafe_allow_html=True)
            
            polite_refusal = """Xin lá»—i, tÃ´i lÃ  trá»£ lÃ½ chuyÃªn vá» **phÃ¡p luáº­t khoÃ¡ng sáº£n** táº¡i Viá»‡t Nam.

TÃ´i chá»‰ cÃ³ thá»ƒ tÆ° váº¥n vá» cÃ¡c váº¥n Ä‘á» liÃªn quan Ä‘áº¿n:
- ğŸ”ï¸ Luáº­t KhoÃ¡ng sáº£n vÃ  cÃ¡c vÄƒn báº£n hÆ°á»›ng dáº«n
- âš–ï¸ Thá»§ tá»¥c cáº¥p phÃ©p thÄƒm dÃ², khai thÃ¡c khoÃ¡ng sáº£n
- ğŸ’° Thuáº¿, phÃ­ liÃªn quan Ä‘áº¿n khoÃ¡ng sáº£n
- ğŸŒ± Báº£o vá»‡ mÃ´i trÆ°á»ng trong hoáº¡t Ä‘á»™ng khoÃ¡ng sáº£n
- âš ï¸ Xá»­ pháº¡t vi pháº¡m hÃ nh chÃ­nh

HÃ£y Ä‘áº·t cÃ¢u há»i vá» lÄ©nh vá»±c khoÃ¡ng sáº£n Ä‘á»ƒ tÃ´i cÃ³ thá»ƒ há»— trá»£ báº¡n tá»‘t nháº¥t! ğŸ˜Š"""
            
            st.session_state.messages.append({"role": "assistant", "content": polite_refusal})
            st.markdown(f'<div class="assistant-message">{polite_refusal}</div>', 
                       unsafe_allow_html=True)
            return
        
        # Add user message
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.markdown(f'<div class="user-message">{prompt}</div>', unsafe_allow_html=True)
        
        # Process with VBPL if available
        if vbpl_enabled and is_vbpl_available():
            with st.spinner("ğŸ¤” Äang phÃ¢n tÃ­ch cÃ¢u há»i vá»›i VBPL database..."):
                
                vbpl_result = process_with_vbpl(prompt)
                
                if vbpl_result.get('success'):
                    # Successful VBPL response
                    response = vbpl_result['response']
                    
                    # Display response
                    st.markdown(f'<div class="assistant-message">{response}</div>', 
                               unsafe_allow_html=True)
                    
                    # Show detailed results
                    if vbpl_result.get('sources'):
                        show_vbpl_results_details(vbpl_result)
                    
                    # Update token stats (estimate)
                    input_tokens = count_tokens(prompt)
                    output_tokens = count_tokens(response)
                    update_stats(input_tokens, output_tokens, selected_model)
                    
                else:
                    # VBPL failed, show fallback response
                    response = vbpl_result['response']
                    st.markdown(f'<div class="assistant-message">{response}</div>', 
                               unsafe_allow_html=True)
        
        else:
            # Fallback when VBPL not available
            fallback_response = """ğŸ—„ï¸ **CÆ¡ sá»Ÿ dá»¯ liá»‡u VBPL khÃ´ng kháº£ dá»¥ng**

Äá»ƒ cÃ³ thÃ´ng tin chÃ­nh xÃ¡c nháº¥t vá» váº¥n Ä‘á» nÃ y, báº¡n vui lÃ²ng:

1. **Tham kháº£o trá»±c tiáº¿p** táº¡i thuvienphapluat.vn
2. **LiÃªn há»‡** Sá»Ÿ TÃ i nguyÃªn vÃ  MÃ´i trÆ°á»ng Ä‘á»‹a phÆ°Æ¡ng
3. **Tham kháº£o** Luáº­t KhoÃ¡ng sáº£n hiá»‡n hÃ nh vÃ  cÃ¡c nghá»‹ Ä‘á»‹nh hÆ°á»›ng dáº«n

ğŸ’¡ **Gá»£i Ã½**: Äá»ƒ sá»­ dá»¥ng tÃ­nh nÄƒng tÃ¬m kiáº¿m database chÃ­nh xÃ¡c, vui lÃ²ng cung cáº¥p file `vbpl.db`."""
            
            st.markdown(f'<div class="assistant-message">{fallback_response}</div>', 
                       unsafe_allow_html=True)
        
        # Add response to history
        st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main()