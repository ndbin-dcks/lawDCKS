import streamlit as st
from openai import OpenAI
import requests
import json
from datetime import datetime
import re
from urllib.parse import quote
import time
import os
import traceback

# Cáº¥u hÃ¬nh trang
st.set_page_config(
    page_title="âš–ï¸ Trá»£ lÃ½ PhÃ¡p cháº¿ KhoÃ¡ng sáº£n Viá»‡t Nam",
    page_icon="âš–ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Model pricing (USD per 1K tokens)
MODEL_PRICING = {
    "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
    "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002},
    "gpt-4": {"input": 0.03, "output": 0.06},
    "gpt-4-turbo-preview": {"input": 0.01, "output": 0.03}
}

def init_session_state():
    """Khá»Ÿi táº¡o session state an toÃ n"""
    if "token_stats" not in st.session_state:
        st.session_state.token_stats = {
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "total_cost": 0.0,
            "session_start": datetime.now(),
            "request_count": 0
        }
    
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "system", "content": get_strict_system_prompt()},
            {"role": "assistant", "content": get_welcome_message()}
        ]

def safe_get_stats():
    """Láº¥y stats má»™t cÃ¡ch an toÃ n"""
    try:
        init_session_state()
        stats = st.session_state.token_stats
        total_tokens = stats["total_input_tokens"] + stats["total_output_tokens"]
        
        return {
            "total_tokens": total_tokens,
            "input_tokens": stats["total_input_tokens"],
            "output_tokens": stats["total_output_tokens"],
            "total_cost_usd": stats["total_cost"],
            "total_cost_vnd": stats["total_cost"] * 24000,
            "requests": stats["request_count"],
            "session_duration": datetime.now() - stats["session_start"]
        }
    except Exception as e:
        return {
            "total_tokens": 0,
            "input_tokens": 0,
            "output_tokens": 0,
            "total_cost_usd": 0.0,
            "total_cost_vnd": 0.0,
            "requests": 0,
            "session_duration": datetime.now() - datetime.now()
        }

def update_stats(input_tokens, output_tokens, model):
    """Cáº­p nháº­t stats an toÃ n"""
    try:
        init_session_state()
        
        if model not in MODEL_PRICING:
            model = "gpt-4o-mini"
        
        pricing = MODEL_PRICING[model]
        cost = (input_tokens / 1000) * pricing["input"] + (output_tokens / 1000) * pricing["output"]
        
        st.session_state.token_stats["total_input_tokens"] += input_tokens
        st.session_state.token_stats["total_output_tokens"] += output_tokens
        st.session_state.token_stats["total_cost"] += cost
        st.session_state.token_stats["request_count"] += 1
        
    except Exception as e:
        st.error(f"Lá»—i cáº­p nháº­t stats: {e}")

def count_tokens(text):
    """Æ¯á»›c tÃ­nh sá»‘ token Ä‘Æ¡n giáº£n"""
    return len(str(text)) // 4

def get_strict_system_prompt():
    """System prompt nghiÃªm ngáº·t ngÄƒn hallucination"""
    try:
        with open("01.system_trainning.txt", "r", encoding="utf-8") as file:
            base_prompt = file.read()
            # ThÃªm strict instructions vÃ o cuá»‘i
            return base_prompt + """

ğŸš« HÆ¯á»šNG DáºªN NGHIÃŠM NGáº¶T Bá»” SUNG:

TUYá»†T Äá»I KHÃ”NG ÄÆ¯á»¢C:
1. Bá»‹a Ä‘áº·t sá»‘ luáº­t, sá»‘ Ä‘iá»u, sá»‘ khoáº£n náº¿u khÃ´ng cÃ³ trong thÃ´ng tin tÃ¬m kiáº¿m
2. TrÃ­ch dáº«n cá»¥ thá»ƒ cÃ¡c Ä‘iá»u khoáº£n phÃ¡p luáº­t mÃ  khÃ´ng cÃ³ nguá»“n xÃ¡c thá»±c
3. ÄÆ°a ra thÃ´ng tin chi tiáº¿t vá» ná»™i dung luáº­t náº¿u khÃ´ng cháº¯c cháº¯n 100%
4. Sá»­ dá»¥ng kiáº¿n thá»©c cÅ© vá» phÃ¡p luáº­t mÃ  khÃ´ng cÃ³ xÃ¡c nháº­n tá»« nguá»“n hiá»‡n táº¡i

CHá»ˆ ÄÆ¯á»¢C:
1. TrÃ­ch dáº«n thÃ´ng tin CÃ“ TRONG káº¿t quáº£ tÃ¬m kiáº¿m Ä‘Æ°á»£c cung cáº¥p
2. ÄÆ°a ra cÃ¡c nguyÃªn táº¯c chung vá» phÃ¡p luáº­t khoÃ¡ng sáº£n
3. HÆ°á»›ng dáº«n ngÆ°á»i há»i tham kháº£o nguá»“n chÃ­nh thá»‘ng
4. NÃ³i rÃµ khi thÃ´ng tin khÃ´ng Ä‘áº§y Ä‘á»§ hoáº·c cáº§n kiá»ƒm tra thÃªm

LUÃ”N Æ°u tiÃªn an toÃ n thÃ´ng tin hÆ¡n viá»‡c Ä‘Æ°a ra cÃ¢u tráº£ lá»i chi tiáº¿t."""
            
    except FileNotFoundError:
        return """Báº¡n lÃ  chuyÃªn gia phÃ¡p cháº¿ vá» quáº£n lÃ½ nhÃ  nÆ°á»›c trong lÄ©nh vá»±c khoÃ¡ng sáº£n táº¡i Viá»‡t Nam.

âš–ï¸ NGUYÃŠN Táº®C LÃ€M VIá»†C NGHIÃŠM NGáº¶T:

ğŸš« TUYá»†T Äá»I KHÃ”NG ÄÆ¯á»¢C:
1. Bá»‹a Ä‘áº·t sá»‘ luáº­t, sá»‘ Ä‘iá»u, sá»‘ khoáº£n náº¿u khÃ´ng cÃ³ trong thÃ´ng tin tÃ¬m kiáº¿m
2. TrÃ­ch dáº«n cá»¥ thá»ƒ cÃ¡c Ä‘iá»u khoáº£n phÃ¡p luáº­t mÃ  khÃ´ng cÃ³ nguá»“n xÃ¡c thá»±c
3. ÄÆ°a ra thÃ´ng tin chi tiáº¿t vá» ná»™i dung luáº­t náº¿u khÃ´ng cháº¯c cháº¯n 100%
4. Sá»­ dá»¥ng kiáº¿n thá»©c cÅ© vá» phÃ¡p luáº­t mÃ  khÃ´ng cÃ³ xÃ¡c nháº­n tá»« nguá»“n hiá»‡n táº¡i

âœ… CHá»ˆ ÄÆ¯á»¢C:
1. TrÃ­ch dáº«n thÃ´ng tin CÃ“ TRONG káº¿t quáº£ tÃ¬m kiáº¿m Ä‘Æ°á»£c cung cáº¥p
2. ÄÆ°a ra cÃ¡c nguyÃªn táº¯c chung vá» phÃ¡p luáº­t khoÃ¡ng sáº£n
3. HÆ°á»›ng dáº«n ngÆ°á»i há»i tham kháº£o nguá»“n chÃ­nh thá»‘ng
4. NÃ³i rÃµ khi thÃ´ng tin khÃ´ng Ä‘áº§y Ä‘á»§ hoáº·c cáº§n kiá»ƒm tra thÃªm

ğŸ¯ CÃCH TRáº¢ Lá»œI AN TOÃ€N:
- Khi cÃ³ thÃ´ng tin tá»« search: "Dá»±a theo thÃ´ng tin tÃ¬m kiáº¿m tá»« [nguá»“n]..."
- Khi khÃ´ng cháº¯c cháº¯n: "ThÃ´ng tin nÃ y cáº§n Ä‘Æ°á»£c kiá»ƒm tra táº¡i thuvienphapluat.vn"
- Khi thÃ´ng tin khÃ´ng Ä‘á»§: "TÃ´i khÃ´ng cÃ³ Ä‘á»§ thÃ´ng tin chÃ­nh xÃ¡c Ä‘á»ƒ tráº£ lá»i chi tiáº¿t"

QUAN TRá»ŒNG: An toÃ n thÃ´ng tin phÃ¡p luáº­t quan trá»ng hÆ¡n viá»‡c Ä‘Æ°a ra cÃ¢u tráº£ lá»i chi tiáº¿t."""

def get_welcome_message():
    """Láº¥y tin nháº¯n chÃ o má»«ng"""
    try:
        with open("02.assistant.txt", "r", encoding="utf-8") as file:
            return file.read()
    except FileNotFoundError:
        return """Xin chÃ o! âš–ï¸ 

TÃ´i lÃ  **Trá»£ lÃ½ PhÃ¡p cháº¿ chuyÃªn vá» Quáº£n lÃ½ NhÃ  nÆ°á»›c trong lÄ©nh vá»±c KhoÃ¡ng sáº£n táº¡i Viá»‡t Nam**.

ğŸ”ï¸ **TÃ´i cÃ³ thá»ƒ há»— trá»£ báº¡n vá»:**

âœ… **PhÃ¡p luáº­t KhoÃ¡ng sáº£n:**
   â€¢ Luáº­t KhoÃ¡ng sáº£n vÃ  cÃ¡c vÄƒn báº£n hÆ°á»›ng dáº«n
   â€¢ Nghá»‹ Ä‘á»‹nh, ThÃ´ng tÆ° cá»§a Bá»™ TÃ i nguyÃªn vÃ  MÃ´i trÆ°á»ng

âœ… **Thá»§ tá»¥c hÃ nh chÃ­nh:**
   â€¢ Cáº¥p Giáº¥y phÃ©p thÄƒm dÃ², khai thÃ¡c khoÃ¡ng sáº£n
   â€¢ Gia háº¡n, sá»­a Ä‘á»•i, bá»• sung giáº¥y phÃ©p
   â€¢ Thá»§ tá»¥c Ä‘Ã³ng cá»­a má»

ğŸ¯ **LÆ°u Ã½ quan trá»ng:** 
TÃ´i chá»‰ tÆ° váº¥n vá» lÄ©nh vá»±c **KhoÃ¡ng sáº£n**. Äá»ƒ cÃ³ thÃ´ng tin chÃ­nh xÃ¡c nháº¥t, báº¡n nÃªn tham kháº£o trá»±c tiáº¿p táº¡i **thuvienphapluat.vn**.

**Báº¡n cÃ³ tháº¯c máº¯c gÃ¬ vá» phÃ¡p luáº­t KhoÃ¡ng sáº£n khÃ´ng?** ğŸ¤”"""

def get_default_model():
    """Láº¥y model máº·c Ä‘á»‹nh"""
    try:
        with open("module_chatgpt.txt", "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "gpt-4o-mini"

def is_mineral_related(message):
    """Kiá»ƒm tra cÃ¢u há»i cÃ³ liÃªn quan Ä‘áº¿n khoÃ¡ng sáº£n khÃ´ng"""
    mineral_keywords = [
        'khoÃ¡ng sáº£n', 'khai thÃ¡c', 'thÄƒm dÃ²', 'Ä‘Ã¡', 'cÃ¡t', 'sá»i',
        'than', 'quáº·ng', 'kim loáº¡i', 'phi kim loáº¡i', 'khoÃ¡ng',
        'luáº­t khoÃ¡ng sáº£n', 'giáº¥y phÃ©p', 'cáº¥p phÃ©p', 'thuáº¿ tÃ i nguyÃªn',
        'phÃ­ thÄƒm dÃ²', 'tiá»n cáº¥p quyá»n', 'vi pháº¡m hÃ nh chÃ­nh',
        'bá»™ tÃ i nguyÃªn', 'sá»Ÿ tÃ i nguyÃªn', 'monre', 'tn&mt',
        'má»', 'má» Ä‘Ã¡', 'má» cÃ¡t', 'má» than', 'quarry', 'mining',
        'thu há»“i giáº¥y phÃ©p'
    ]
    
    message_lower = message.lower()
    return any(keyword in message_lower for keyword in mineral_keywords)

def should_search_web(message):
    """Kiá»ƒm tra cÃ³ cáº§n tÃ¬m kiáº¿m web khÃ´ng"""
    search_indicators = [
        'má»›i nháº¥t', 'cáº­p nháº­t', 'hiá»‡n hÃ nh', 'ban hÃ nh', 'sá»­a Ä‘á»•i',
        'bá»• sung', 'thay tháº¿', 'cÃ³ hiá»‡u lá»±c', 'quy Ä‘á»‹nh má»›i',
        'nghá»‹ Ä‘á»‹nh', 'thÃ´ng tÆ°', 'luáº­t', 'phÃ¡p luáº­t', 'Ä‘iá»u',
        'khi nÃ o', 'trÆ°á»ng há»£p nÃ o', 'Ä‘iá»u kiá»‡n', 'thu há»“i'
    ]
    
    message_lower = message.lower()
    return (is_mineral_related(message) and 
            any(indicator in message_lower for indicator in search_indicators))

# =================== SEARCH FUNCTIONS ===================

def is_high_quality_legal_content(title, content, url=""):
    """Kiá»ƒm tra ná»™i dung cÃ³ pháº£i vÄƒn báº£n phÃ¡p luáº­t cháº¥t lÆ°á»£ng cao khÃ´ng"""
    
    # 1. Kiá»ƒm tra nguá»“n uy tÃ­n
    trusted_domains = ['thuvienphapluat.vn', 'monre.gov.vn', 'moj.gov.vn', 'chinhphu.vn']
    is_trusted_source = any(domain in url.lower() for domain in trusted_domains)
    
    # 2. Kiá»ƒm tra cáº¥u trÃºc vÄƒn báº£n phÃ¡p luáº­t
    legal_structure_patterns = [
        r'(?:luáº­t|nghá»‹ Ä‘á»‹nh|thÃ´ng tÆ°|quyáº¿t Ä‘á»‹nh)\s+(?:sá»‘\s*)?\d+',
        r'Ä‘iá»u\s+\d+',
        r'khoáº£n\s+\d+',
        r'chÆ°Æ¡ng\s+[ivx\d]+',
        r'má»¥c\s+\d+'
    ]
    
    text = (title + ' ' + content).lower()
    has_legal_structure = sum(1 for pattern in legal_structure_patterns 
                            if re.search(pattern, text)) >= 2
    
    # 3. Kiá»ƒm tra tá»« khÃ³a khoÃ¡ng sáº£n cá»¥ thá»ƒ
    mineral_legal_terms = [
        'luáº­t khoÃ¡ng sáº£n', 'khai thÃ¡c khoÃ¡ng sáº£n', 'thÄƒm dÃ² khoÃ¡ng sáº£n',
        'giáº¥y phÃ©p khai thÃ¡c', 'giáº¥y phÃ©p thÄƒm dÃ²', 'thuáº¿ tÃ i nguyÃªn',
        'bá»™ tÃ i nguyÃªn', 'sá»Ÿ tÃ i nguyÃªn', 'thu há»“i giáº¥y phÃ©p'
    ]
    
    has_mineral_terms = any(term in text for term in mineral_legal_terms)
    
    # 4. Loáº¡i bá» ná»™i dung spam/khÃ´ng phÃ¹ há»£p
    spam_indicators = ['quáº£ng cÃ¡o', 'bÃ¡n hÃ ng', 'tuyá»ƒn dá»¥ng', '404', 'error']
    has_spam = any(spam in text for spam in spam_indicators)
    
    # 5. Kiá»ƒm tra Ä‘á»™ dÃ i ná»™i dung
    has_sufficient_content = len(content.strip()) > 100
    
    # TÃ­nh Ä‘iá»ƒm tá»•ng
    score = 0
    if is_trusted_source: score += 3
    if has_legal_structure: score += 2  
    if has_mineral_terms: score += 2
    if has_sufficient_content: score += 1
    if has_spam: score -= 3
    
    return score >= 4

def calculate_improved_confidence(query, title, content, url=""):
    """TÃ­nh confidence score cáº£i tiáº¿n vá»›i nhiá»u yáº¿u tá»‘"""
    
    confidence = 0.0
    query_lower = query.lower()
    text_lower = (title + ' ' + content).lower()
    
    # 1. Exact phrase matching (25%)
    if query_lower in text_lower:
        confidence += 0.25
    
    # 2. Word overlap (20%)
    query_words = set(query_lower.split())
    text_words = set(text_lower.split())
    if len(query_words) > 0:
        overlap_ratio = len(query_words.intersection(text_words)) / len(query_words)
        confidence += overlap_ratio * 0.20
    
    # 3. Legal document indicators (20%)
    legal_patterns = [
        (r'luáº­t\s+khoÃ¡ng sáº£n', 0.15),
        (r'luáº­t\s+(?:sá»‘\s*)?\d+.*khoÃ¡ng sáº£n', 0.12),
        (r'nghá»‹ Ä‘á»‹nh\s+(?:sá»‘\s*)?\d+.*khoÃ¡ng sáº£n', 0.10),
        (r'thÃ´ng tÆ°\s+(?:sá»‘\s*)?\d+.*khoÃ¡ng sáº£n', 0.08),
        (r'Ä‘iá»u\s+\d+', 0.05),
        (r'khoáº£n\s+\d+', 0.03)
    ]
    
    for pattern, weight in legal_patterns:
        if re.search(pattern, text_lower):
            confidence += weight
            break  # Chá»‰ tÃ­nh pattern cÃ³ trá»ng sá»‘ cao nháº¥t
    
    # 4. Source reliability (20%)
    source_scores = {
        'thuvienphapluat.vn': 0.20,
        'monre.gov.vn': 0.18,
        'chinhphu.vn': 0.15,
        'moj.gov.vn': 0.12,
        'gov.vn': 0.08
    }
    
    for domain, score in source_scores.items():
        if domain in url.lower():
            confidence += score
            break
    
    # 5. Title relevance bonus (10%)
    title_words = set(title.lower().split())
    title_overlap = len(query_words.intersection(title_words)) / len(query_words) if query_words else 0
    confidence += title_overlap * 0.10
    
    # 6. Content quality (5%)
    if len(content) > 200:
        confidence += 0.05
    elif len(content) > 100:
        confidence += 0.025
    
    # Penalties
    if any(spam in text_lower for spam in ['404', 'error', 'khÃ´ng tÃ¬m tháº¥y']):
        confidence *= 0.3
    
    return min(confidence, 1.0)

def extract_document_type(title):
    """TrÃ­ch xuáº¥t loáº¡i vÄƒn báº£n tá»« tiÃªu Ä‘á»"""
    title_lower = title.lower()
    
    if re.search(r'luáº­t\s+(?:sá»‘\s*)?\d+', title_lower):
        return 'Luáº­t'
    elif re.search(r'nghá»‹ Ä‘á»‹nh\s+(?:sá»‘\s*)?\d+', title_lower):
        return 'Nghá»‹ Ä‘á»‹nh'
    elif re.search(r'thÃ´ng tÆ°\s+(?:sá»‘\s*)?\d+', title_lower):
        return 'ThÃ´ng tÆ°'
    elif re.search(r'quyáº¿t Ä‘á»‹nh\s+(?:sá»‘\s*)?\d+', title_lower):
        return 'Quyáº¿t Ä‘á»‹nh'
    else:
        return 'VÄƒn báº£n'

def remove_duplicate_results(results):
    """Loáº¡i bá» káº¿t quáº£ trÃ¹ng láº·p"""
    unique_results = []
    seen_urls = set()
    seen_titles = set()
    
    for result in results:
        url = result.get('url', '')
        title = result.get('title', '').lower().strip()
        
        # Skip if URL or title already seen
        if url in seen_urls or title in seen_titles:
            continue
            
        # Skip if title too similar to existing ones
        is_duplicate = False
        for seen_title in seen_titles:
            if calculate_string_similarity(title, seen_title) > 0.8:
                is_duplicate = True
                break
        
        if not is_duplicate:
            seen_urls.add(url)
            seen_titles.add(title)
            unique_results.append(result)
    
    return unique_results

def calculate_string_similarity(str1, str2):
    """TÃ­nh similarity giá»¯a 2 string"""
    if not str1 or not str2:
        return 0.0
    
    words1 = set(str1.split())
    words2 = set(str2.split())
    
    if not words1 or not words2:
        return 0.0
    
    intersection = len(words1.intersection(words2))
    union = len(words1.union(words2))
    
    return intersection / union if union > 0 else 0.0
# Thay tháº¿ hÃ m advanced_web_search_improved() trong streamlit_app.py

def advanced_web_search_improved_v2(query, max_results=5):
    """Fixed web search - khÃ´ng dÃ¹ng site restriction"""
    results = []
    
    try:
        # Strategy 1: General Vietnamese legal queries (NO site: restriction)
        search_queries = [
            f'"{query}" luáº­t khoÃ¡ng sáº£n Viá»‡t Nam',
            f'"{query}" nghá»‹ Ä‘á»‹nh khoÃ¡ng sáº£n',
            f'luáº­t khoÃ¡ng sáº£n "{query}" thuvienphapluat',
            f'khoÃ¡ng sáº£n "{query}" quy Ä‘á»‹nh phÃ¡p luáº­t',
            f'"{query}" bá»™ tÃ i nguyÃªn mÃ´i trÆ°á»ng'
        ]
        
        for search_query in search_queries:
            if len(results) >= max_results:
                break
                
            try:
                # Remove site: restrictions, use general search
                params = {
                    'q': search_query,
                    'format': 'json',
                    'no_html': '1',
                    'skip_disambig': '1',
                    'safe_search': 'moderate'
                }
                
                response = requests.get("https://api.duckduckgo.com/", 
                                      params=params, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # Debug: Show what we got
                    st.write(f"ğŸ” **Query:** {search_query}")
                    st.write(f"ğŸ“Š **Abstract:** {'âœ…' if data.get('Abstract') else 'âŒ'}")
                    st.write(f"ğŸ”— **RelatedTopics:** {len(data.get('RelatedTopics', []))}")
                    
                    # Process Abstract vá»›i relaxed validation
                    if data.get('Abstract') and len(data['Abstract']) > 30:
                        title = data.get('AbstractText', 'ThÃ´ng tin phÃ¡p luáº­t')
                        content = data.get('Abstract')
                        url = data.get('AbstractURL', '')
                        
                        # Relaxed validation for Vietnamese content
                        if is_vietnamese_legal_content(title, content, url):
                            confidence = calculate_vietnamese_confidence(query, title, content, url)
                            
                            results.append({
                                'title': title,
                                'content': content,
                                'url': url,
                                'source': determine_source(url),
                                'priority': is_priority_source(url),
                                'confidence': confidence,
                                'document_type': extract_document_type(title),
                                'method': 'duckduckgo_abstract'
                            })
                            
                            st.success(f"âœ… **Found Abstract Result** (Confidence: {confidence:.2f})")
                    
                    # Process RelatedTopics vá»›i relaxed filtering
                    for i, topic in enumerate(data.get('RelatedTopics', [])):
                        if len(results) >= max_results:
                            break
                            
                        if isinstance(topic, dict) and topic.get('Text'):
                            title = topic.get('Text', '')[:100]
                            content = topic.get('Text', '')
                            url = topic.get('FirstURL', '')
                            
                            if (len(content) > 50 and 
                                is_vietnamese_legal_content(title, content, url)):
                                
                                confidence = calculate_vietnamese_confidence(query, title, content, url)
                                
                                results.append({
                                    'title': title + '...',
                                    'content': content,
                                    'url': url,
                                    'source': determine_source(url),
                                    'priority': is_priority_source(url),
                                    'confidence': confidence,
                                    'document_type': extract_document_type(title),
                                    'method': 'duckduckgo_related'
                                })
                                
                                st.success(f"âœ… **Found Related Topic {i+1}** (Confidence: {confidence:.2f})")
                
                time.sleep(0.8)  # Longer delay to avoid rate limiting
                
            except Exception as e:
                st.warning(f"âš ï¸ Query failed: {search_query} - Error: {e}")
                continue
    
    except Exception as e:
        st.error(f"âŒ Search failed completely: {e}")
    
    # If still no results, try backup strategy
    if not results:
        st.warning("ğŸ”„ **Trying backup search strategy...**")
        results = backup_search_strategy(query)
    
    # Remove duplicates vÃ  sort
    unique_results = remove_duplicate_results(results)
    unique_results.sort(
        key=lambda x: (x.get('priority', False), x.get('confidence', 0)), 
        reverse=True
    )
    
    return unique_results[:max_results]

def is_vietnamese_legal_content(title, content, url=""):
    """Relaxed validation cho Vietnamese legal content"""
    
    text = (title + ' ' + content).lower()
    
    # Vietnamese legal keywords (relaxed)
    legal_keywords = [
        'luáº­t', 'nghá»‹ Ä‘á»‹nh', 'thÃ´ng tÆ°', 'quyáº¿t Ä‘á»‹nh',
        'Ä‘iá»u', 'khoáº£n', 'quy Ä‘á»‹nh', 'phÃ¡p luáº­t',
        'khoÃ¡ng sáº£n', 'tÃ i nguyÃªn', 'mÃ´i trÆ°á»ng',
        'viá»‡t nam', 'chÃ­nh phá»§', 'bá»™', 'á»§y ban'
    ]
    
    # Must have at least 2 legal keywords
    keyword_count = sum(1 for keyword in legal_keywords if keyword in text)
    
    # Vietnamese text indicators
    vietnamese_chars = ['Äƒ', 'Ã¢', 'Ä‘', 'Ãª', 'Ã´', 'Æ¡', 'Æ°', 'Ã¡', 'Ã ', 'áº£', 'Ã£', 'áº¡']
    has_vietnamese = any(char in text for char in vietnamese_chars)
    
    # Length check
    sufficient_length = len(content.strip()) > 50
    
    # Negative filters
    spam_indicators = ['quáº£ng cÃ¡o', 'bÃ¡n hÃ ng', '404', 'error', 'not found']
    has_spam = any(spam in text for spam in spam_indicators)
    
    return keyword_count >= 2 and has_vietnamese and sufficient_length and not has_spam

def calculate_vietnamese_confidence(query, title, content, url=""):
    """Confidence calculation for Vietnamese content"""
    
    confidence = 0.2  # Lower base for general content
    
    query_lower = query.lower()
    text_lower = (title + ' ' + content).lower()
    
    # 1. Query word presence (30%)
    query_words = set(query_lower.split())
    text_words = set(text_lower.split())
    if len(query_words) > 0:
        overlap_ratio = len(query_words.intersection(text_words)) / len(query_words)
        confidence += overlap_ratio * 0.3
    
    # 2. Legal indicators (25%)
    legal_patterns = [
        (r'luáº­t.*khoÃ¡ng sáº£n', 0.2),
        (r'nghá»‹ Ä‘á»‹nh.*\d+', 0.15),
        (r'thÃ´ng tÆ°.*\d+', 0.1),
        (r'Ä‘iá»u\s+\d+', 0.08),
        (r'khoáº£n\s+\d+', 0.05)
    ]
    
    for pattern, weight in legal_patterns:
        if re.search(pattern, text_lower):
            confidence += weight
            break
    
    # 3. Source bonus (20%)
    source_scores = {
        'thuvienphapluat': 0.2,
        'monre.gov.vn': 0.18,
        'moj.gov.vn': 0.15,
        'chinhphu.vn': 0.12,
        'gov.vn': 0.1,
        'vnexpress': 0.05,
        'baomoi': 0.03
    }
    
    for domain, score in source_scores.items():
        if domain in url.lower():
            confidence += score
            break
    
    # 4. Content quality (15%)
    if len(content) > 200:
        confidence += 0.15
    elif len(content) > 100:
        confidence += 0.08
    
    # 5. Title relevance (10%)
    if any(word in title.lower() for word in query_lower.split()):
        confidence += 0.1
    
    return min(confidence, 1.0)

def determine_source(url):
    """XÃ¡c Ä‘á»‹nh nguá»“n tá»« URL"""
    if 'thuvienphapluat' in url.lower():
        return 'ThÆ° viá»‡n PhÃ¡p luáº­t VN'
    elif 'monre.gov.vn' in url.lower():
        return 'Bá»™ TN&MT'
    elif 'gov.vn' in url.lower():
        return 'CÆ¡ quan NhÃ  nÆ°á»›c'
    elif 'vnexpress' in url.lower():
        return 'VnExpress'
    elif 'baomoi' in url.lower():
        return 'BÃ¡o Má»›i'
    else:
        return 'TÃ¬m kiáº¿m web'

def is_priority_source(url):
    """Kiá»ƒm tra cÃ³ pháº£i priority source khÃ´ng"""
    priority_domains = ['thuvienphapluat', 'monre.gov.vn', 'moj.gov.vn', 'chinhphu.vn']
    return any(domain in url.lower() for domain in priority_domains)

def backup_search_strategy(query):
    """Backup search strategy náº¿u main search fail"""
    results = []
    
    try:
        st.info("ğŸ”„ **Trying backup: Basic Vietnamese search**")
        
        # Very simple Vietnamese queries
        simple_queries = [
            f'"{query}" viá»‡t nam',
            f'luáº­t viá»‡t nam {query}',
            f'{query} quy Ä‘á»‹nh'
        ]
        
        for simple_query in simple_queries:
            try:
                params = {
                    'q': simple_query,
                    'format': 'json',
                    'no_html': '1'
                }
                
                response = requests.get("https://api.duckduckgo.com/", 
                                      params=params, timeout=8)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # Very relaxed processing
                    if data.get('Abstract') and 'viá»‡t nam' in data.get('Abstract', '').lower():
                        results.append({
                            'title': data.get('AbstractText', 'ThÃ´ng tin tham kháº£o'),
                            'content': data.get('Abstract'),
                            'url': data.get('AbstractURL', ''),
                            'source': 'TÃ¬m kiáº¿m web',
                            'priority': False,
                            'confidence': 0.4,  # Lower confidence for backup
                            'document_type': 'ThÃ´ng tin tham kháº£o',
                            'method': 'backup_search'
                        })
                        
                        st.info(f"âœ… **Backup found result**")
                        break
                
                time.sleep(1)
                
            except Exception as e:
                continue
    
    except Exception as e:
        st.error(f"âŒ Backup search failed: {e}")
    
    return results

# Test function to debug search step by step
def test_search_step_by_step(query):
    """Test search vá»›i tá»«ng bÆ°á»›c debug"""
    
    st.markdown("### ğŸ§ª **Step-by-Step Search Test**")
    
    # Test 1: Basic connectivity
    st.write("**Step 1: Testing basic connectivity**")
    try:
        response = requests.get("https://api.duckduckgo.com/", timeout=5)
        st.success(f"âœ… Basic connectivity OK (Status: {response.status_code})")
    except Exception as e:
        st.error(f"âŒ Connectivity failed: {e}")
        return []
    
    # Test 2: Simple query
    st.write("**Step 2: Testing simple query**")
    try:
        params = {'q': 'vietnam law', 'format': 'json'}
        response = requests.get("https://api.duckduckgo.com/", params=params, timeout=10)
        data = response.json()
        
        has_abstract = bool(data.get('Abstract'))
        related_count = len(data.get('RelatedTopics', []))
        
        st.write(f"Simple query results: Abstract={has_abstract}, Related={related_count}")
        
        if has_abstract or related_count > 0:
            st.success("âœ… Simple query works")
        else:
            st.warning("âš ï¸ Simple query returns empty")
            
    except Exception as e:
        st.error(f"âŒ Simple query failed: {e}")
    
    # Test 3: Vietnamese query
    st.write("**Step 3: Testing Vietnamese query**")
    try:
        params = {'q': f'"{query}" viá»‡t nam', 'format': 'json'}
        response = requests.get("https://api.duckduckgo.com/", params=params, timeout=10)
        data = response.json()
        
        has_abstract = bool(data.get('Abstract'))
        related_count = len(data.get('RelatedTopics', []))
        
        st.write(f"Vietnamese query results: Abstract={has_abstract}, Related={related_count}")
        
        if has_abstract:
            st.code(data['Abstract'][:200] + "...")
        
    except Exception as e:
        st.error(f"âŒ Vietnamese query failed: {e}")
    
    # Test 4: Run our improved function
    st.write("**Step 4: Testing our improved search function**")
    try:
        results = advanced_web_search_improved_v2(query, max_results=3)
        st.success(f"âœ… Improved search returned {len(results)} results")
        return results
    except Exception as e:
        st.error(f"âŒ Improved search failed: {e}")
        return []

def advanced_web_search_improved(query, max_results=5):
    """Improved web search vá»›i better accuracy"""
    results = []
    
    try:
        # Search strategy 1: Direct thuvienphapluat.vn focus
        search_queries = [
            f'site:thuvienphapluat.vn "{query}" luáº­t khoÃ¡ng sáº£n',
            f'site:thuvienphapluat.vn khoÃ¡ng sáº£n "{query}"',
            f'site:monre.gov.vn "{query}" khoÃ¡ng sáº£n',
            f'"luáº­t khoÃ¡ng sáº£n" "{query}" site:thuvienphapluat.vn'
        ]
        
        for search_query in search_queries:
            if len(results) >= max_results:
                break
                
            try:
                params = {
                    'q': search_query,
                    'format': 'json',
                    'no_html': '1',
                    'skip_disambig': '1'
                }
                
                response = requests.get("https://api.duckduckgo.com/", 
                                      params=params, timeout=8)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # Process Abstract vá»›i validation
                    if data.get('Abstract') and len(data['Abstract']) > 50:
                        title = data.get('AbstractText', 'ThÃ´ng tin phÃ¡p luáº­t')
                        content = data.get('Abstract')
                        url = data.get('AbstractURL', '')
                        
                        if is_high_quality_legal_content(title, content, url):
                            confidence = calculate_improved_confidence(query, title, content, url)
                            
                            results.append({
                                'title': title,
                                'content': content,
                                'url': url,
                                'source': 'ThÆ° viá»‡n PhÃ¡p luáº­t' if 'thuvienphapluat' in url else 'CÆ¡ quan nhÃ  nÆ°á»›c',
                                'priority': True,
                                'confidence': confidence,
                                'document_type': extract_document_type(title)
                            })
                    
                    # Process RelatedTopics vá»›i strict filtering
                    for topic in data.get('RelatedTopics', []):
                        if len(results) >= max_results:
                            break
                            
                        if isinstance(topic, dict) and topic.get('Text'):
                            title = topic.get('Text', '')[:100]
                            content = topic.get('Text', '')
                            url = topic.get('FirstURL', '')
                            
                            if (is_high_quality_legal_content(title, content, url) and
                                len(content) > 100):
                                
                                confidence = calculate_improved_confidence(query, title, content, url)
                                
                                results.append({
                                    'title': title + '...',
                                    'content': content,
                                    'url': url,
                                    'source': 'ThÆ° viá»‡n PhÃ¡p luáº­t' if 'thuvienphapluat' in url else 'TÃ¬m kiáº¿m web',
                                    'priority': 'thuvienphapluat' in url,
                                    'confidence': confidence,
                                    'document_type': extract_document_type(title)
                                })
                
                time.sleep(0.5)  # Rate limiting
                
            except Exception as e:
                continue
    
    except Exception as e:
        st.warning(f"âš ï¸ TÃ¬m kiáº¿m web gáº·p lá»—i: {e}")
    
    # Remove duplicates vÃ  sort by confidence
    unique_results = remove_duplicate_results(results)
    
    # Sort by priority and confidence
    unique_results.sort(
        key=lambda x: (x.get('priority', False), x.get('confidence', 0)), 
        reverse=True
    )
    
    return unique_results[:max_results]

def create_search_summary(search_results):
    """Táº¡o summary ngáº¯n gá»n vá» search results"""
    summary = ""
    for i, result in enumerate(search_results[:3], 1):
        confidence = result.get('confidence', 0)
        summary += f"{i}. {result['title'][:50]}... (Confidence: {confidence:.2f})\n"
    return summary

def create_safe_enhanced_search_prompt(user_message, search_results):
    """Táº¡o prompt an toÃ n ngÄƒn AI hallucination"""
    
    if not search_results:
        return f"""
{user_message}

QUAN TRá»ŒNG: KHÃ”NG tÃ¬m tháº¥y thÃ´ng tin chÃ­nh xÃ¡c tá»« cÃ¡c nguá»“n phÃ¡p luáº­t chÃ­nh thá»‘ng.

HÆ¯á»šNG DáºªN TRáº¢ Lá»œI:
1. TUYá»†T Äá»I KHÃ”NG Ä‘Æ°á»£c bá»‹a Ä‘áº·t sá»‘ luáº­t, sá»‘ Ä‘iá»u, sá»‘ khoáº£n
2. TUYá»†T Äá»I KHÃ”NG Ä‘Æ°á»£c trÃ­ch dáº«n cá»¥ thá»ƒ náº¿u khÃ´ng cÃ³ trong káº¿t quáº£ search
3. CHá»ˆ Ä‘Æ°á»£c nÃ³i vá» cÃ¡c nguyÃªn táº¯c chung vÃ  khuyáº¿n nghá»‹ tham kháº£o nguá»“n chÃ­nh thá»‘ng

HÃ£y tráº£ lá»i: "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin chÃ­nh xÃ¡c vá» váº¥n Ä‘á» nÃ y tá»« cÃ¡c nguá»“n phÃ¡p luáº­t chÃ­nh thá»‘ng. Äá»ƒ cÃ³ thÃ´ng tin chÃ­nh xÃ¡c nháº¥t vá» [váº¥n Ä‘á» cá»¥ thá»ƒ], báº¡n vui lÃ²ng:

1. Tham kháº£o trá»±c tiáº¿p táº¡i thuvienphapluat.vn
2. LiÃªn há»‡ Sá»Ÿ TÃ i nguyÃªn vÃ  MÃ´i trÆ°á»ng Ä‘á»‹a phÆ°Æ¡ng  
3. Tham kháº£o vÄƒn báº£n Luáº­t KhoÃ¡ng sáº£n hiá»‡n hÃ nh vÃ  cÃ¡c nghá»‹ Ä‘á»‹nh hÆ°á»›ng dáº«n

TÃ´i khÃ´ng thá»ƒ Ä‘Æ°a ra thÃ´ng tin cá»¥ thá»ƒ vá» Ä‘iá»u khoáº£n phÃ¡p luáº­t mÃ  khÃ´ng cÃ³ nguá»“n xÃ¡c thá»±c."
"""
    
    # Kiá»ƒm tra cháº¥t lÆ°á»£ng search results
    high_quality_results = [r for r in search_results if r.get('confidence', 0) > 0.8]
    trusted_results = [r for r in search_results if r.get('priority', False)]
    
    if not high_quality_results and not trusted_results:
        return f"""
{user_message}

Cáº¢NH BÃO: Káº¿t quáº£ tÃ¬m kiáº¿m cÃ³ Ä‘á»™ tin cáº­y tháº¥p.

HÆ¯á»šNG DáºªN TRáº¢ Lá»œI AN TOÃ€N:
1. KHÃ”NG Ä‘Æ°á»£c trÃ­ch dáº«n cá»¥ thá»ƒ sá»‘ luáº­t, sá»‘ Ä‘iá»u náº¿u khÃ´ng cháº¯c cháº¯n 100%
2. CHá»ˆ Ä‘Æ°á»£c nÃ³i vá» cÃ¡c nguyÃªn táº¯c chung
3. PHáº¢I khuyáº¿n nghá»‹ kiá»ƒm tra táº¡i nguá»“n chÃ­nh thá»‘ng

Káº¿t quáº£ tÃ¬m kiáº¿m (Äá»˜ TIN Cáº¬Y THáº¤P):
{create_search_summary(search_results)}

HÃ£y tráº£ lá»i tháº­n trá»ng vÃ  luÃ´n disclaimer vá» Ä‘á»™ tin cáº­y tháº¥p.
"""
    
    # Chá»‰ dÃ¹ng results cÃ³ confidence cao
    validated_results = [r for r in search_results if r.get('confidence', 0) > 0.7]
    
    search_info = "\n\n=== THÃ”NG TIN PHÃP LUáº¬T ÄÃƒ KIá»‚M Äá»ŠNH ===\n"
    
    for i, result in enumerate(validated_results, 1):
        confidence = result.get('confidence', 0)
        doc_type = result.get('document_type', 'VÄƒn báº£n')
        
        search_info += f"\nNguá»“n {i} - {doc_type} [Tin cáº­y: {confidence:.2f}]:\n"
        search_info += f"TiÃªu Ä‘á»: {result['title']}\n"
        search_info += f"Ná»™i dung: {result['content'][:800]}\n"
        search_info += f"URL: {result.get('url', '')}\n"
        search_info += "---\n"
    
    search_info += f"""
HÆ¯á»šNG DáºªN TRáº¢ Lá»œI NGHIÃŠM NGáº¶T:
1. CHá»ˆ Ä‘Æ°á»£c trÃ­ch dáº«n thÃ´ng tin CÃ“ TRONG káº¿t quáº£ tÃ¬m kiáº¿m trÃªn
2. TUYá»†T Äá»I KHÃ”NG Ä‘Æ°á»£c bá»‹a Ä‘áº·t sá»‘ Ä‘iá»u, sá»‘ khoáº£n, tÃªn luáº­t
3. Náº¿u thÃ´ng tin khÃ´ng Ä‘áº§y Ä‘á»§, pháº£i ghi rÃµ "ThÃ´ng tin khÃ´ng Ä‘áº§y Ä‘á»§, cáº§n tham kháº£o thÃªm"
4. PHáº¢I cÃ³ disclaimer: "ThÃ´ng tin tham kháº£o, vui lÃ²ng kiá»ƒm tra táº¡i thuvienphapluat.vn"
5. Náº¿u cÃ³ doubt gÃ¬, Æ°u tiÃªn nÃ³i "KhÃ´ng thá»ƒ xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c"

=== Káº¾T THÃšC THÃ”NG TIN ===

CÃ¢u há»i: {user_message}
"""
    
    return search_info

# =================== MAIN APPLICATION ===================

def main():
    init_session_state()
    
    # CSS
    st.markdown("""
    <style>
    .assistant-message {
        background: #f0f8ff;
        padding: 15px;
        border-radius: 15px;
        margin: 10px 0;
        max-width: 80%;
        border-left: 4px solid #4CAF50;
    }
    .assistant-message::before { 
        content: "âš–ï¸ Trá»£ lÃ½ PhÃ¡p cháº¿: "; 
        font-weight: bold; 
        color: #2E7D32;
    }
    
    .user-message {
        background: #e3f2fd;
        padding: 15px;
        border-radius: 15px;
        margin: 10px 0 10px auto;
        max-width: 80%;
        text-align: right;
        border-right: 4px solid #2196F3;
    }
    .user-message::before { 
        content: "ğŸ‘¤ Báº¡n: "; 
        font-weight: bold; 
        color: #1976D2;
    }
    
    .stats-box {
        background: #f5f5f5;
        padding: 10px;
        border-radius: 8px;
        border: 1px solid #ddd;
        margin: 5px 0;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Header
    st.markdown("""
    <div style="text-align: center; padding: 20px; background: linear-gradient(90deg, #2E7D32, #4CAF50); border-radius: 10px; margin-bottom: 20px;">
        <h1 style="color: white; margin: 0;">âš–ï¸ Trá»£ lÃ½ PhÃ¡p cháº¿ KhoÃ¡ng sáº£n</h1>
        <p style="color: #E8F5E8; margin: 5px 0 0 0;">ChuyÃªn gia tÆ° váº¥n Quáº£n lÃ½ NhÃ  nÆ°á»›c vá» KhoÃ¡ng sáº£n táº¡i Viá»‡t Nam</p>
        <p style="color: #E8F5E8; margin: 5px 0 0 0; font-size: 12px;">ğŸ›¡ï¸ Safe Mode â€¢ Debug Enabled â€¢ Anti-Hallucination</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.markdown("### âš™ï¸ CÃ i Ä‘áº·t há»‡ thá»‘ng")
        
        # Web search toggle
        web_search_enabled = st.toggle("ğŸ” TÃ¬m kiáº¿m phÃ¡p luáº­t online (Debug Mode)", value=True)
        
        # Debug mode toggle
        debug_mode = st.toggle("ğŸ› Debug Mode (Hiá»ƒn thá»‹ search details)", value=True)
        
        # Model selection
        model_options = ["gpt-4o-mini", "gpt-3.5-turbo", "gpt-4", "gpt-4-turbo-preview"]
        model_info = {
            "gpt-4o-mini": "ğŸ’° Ráº» nháº¥t ($0.15/$0.6 per 1K tokens)",
            "gpt-3.5-turbo": "âš–ï¸ CÃ¢n báº±ng ($1.5/$2 per 1K tokens)", 
            "gpt-4": "ğŸ§  ThÃ´ng minh ($30/$60 per 1K tokens)",
            "gpt-4-turbo-preview": "ğŸš€ Nhanh ($10/$30 per 1K tokens)"
        }
        
        default_model = get_default_model()
        default_index = model_options.index(default_model) if default_model in model_options else 0
        
        selected_model = st.selectbox("ğŸ¤– Chá»n model AI:", model_options, index=default_index)
        st.caption(model_info[selected_model])
        
        # Temperature
        temperature = st.slider("ğŸŒ¡ï¸ Äá»™ sÃ¡ng táº¡o:", 0.0, 1.0, 0.3, 0.1)
        
        st.markdown("---")
        
        # Stats
        st.markdown("### ğŸ“Š Thá»‘ng kÃª sá»­ dá»¥ng")
        
        try:
            stats = safe_get_stats()
            
            st.markdown('<div class="stats-box">', unsafe_allow_html=True)
            st.metric("ğŸ¯ Tá»•ng Token", f"{stats['total_tokens']:,}")
            col1, col2 = st.columns(2)
            with col1:
                st.metric("ğŸ“¥ Input", f"{stats['input_tokens']:,}")
            with col2:
                st.metric("ğŸ“¤ Output", f"{stats['output_tokens']:,}")
            st.markdown('</div>', unsafe_allow_html=True)
            
            st.markdown('<div class="stats-box">', unsafe_allow_html=True)
            st.metric("ğŸ’° Chi phÃ­ (USD)", f"${stats['total_cost_usd']:.4f}")
            st.metric("ğŸ’¸ Chi phÃ­ (VND)", f"{stats['total_cost_vnd']:,.0f}Ä‘")
            st.markdown('</div>', unsafe_allow_html=True)
            
            st.markdown('<div class="stats-box">', unsafe_allow_html=True)
            st.metric("ğŸ“ Sá»‘ lÆ°á»£t há»i", stats['requests'])
            duration = str(stats['session_duration']).split('.')[0]
            st.metric("â±ï¸ Thá»i gian", duration)
            st.markdown('</div>', unsafe_allow_html=True)
            
        except Exception as e:
            st.error(f"Lá»—i hiá»ƒn thá»‹ stats: {e}")
        
        # Buttons
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ”„ Reset stats", use_container_width=True):
                try:
                    st.session_state.token_stats = {
                        "total_input_tokens": 0,
                        "total_output_tokens": 0,
                        "total_cost": 0.0,
                        "session_start": datetime.now(),
                        "request_count": 0
                    }
                    st.rerun()
                except Exception as e:
                    st.error(f"Lá»—i reset: {e}")
        
        with col2:
            if st.button("ğŸ—‘ï¸ XÃ³a chat", use_container_width=True):
                try:
                    st.session_state.messages = [
                        {"role": "system", "content": get_strict_system_prompt()},
                        {"role": "assistant", "content": get_welcome_message()}
                    ]
                    st.rerun()
                except Exception as e:
                    st.error(f"Lá»—i xÃ³a chat: {e}")
        
        st.markdown("---")
        st.markdown("### ğŸ›¡ï¸ Safe Mode Features")
        st.success("âœ… Anti-hallucination prompts")
        st.success("âœ… Source verification")
        st.success("âœ… Confidence scoring")
        st.success("âœ… Debug search results")
        st.info("ğŸ’¡ NgÄƒn AI bá»‹a Ä‘áº·t thÃ´ng tin phÃ¡p luáº­t")
    
    # Check API key
    if not st.secrets.get("OPENAI_API_KEY"):
        st.error("âŒ ChÆ°a cáº¥u hÃ¬nh OPENAI_API_KEY trong secrets!")
        st.stop()
    
    # Initialize OpenAI client
    try:
        client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
    except Exception as e:
        st.error(f"âŒ Lá»—i khá»Ÿi táº¡o OpenAI client: {str(e)}")
        st.stop()
    
    # Display chat history
    for message in st.session_state.messages:
        if message["role"] == "assistant":
            st.markdown(f'<div class="assistant-message">{message["content"]}</div>', 
                       unsafe_allow_html=True)
        elif message["role"] == "user":
            st.markdown(f'<div class="user-message">{message["content"]}</div>', 
                       unsafe_allow_html=True)
    
    
# Thay tháº¿ pháº§n xá»­ lÃ½ chat input trong main() tá»« dÃ²ng "if prompt := st.chat_input"

    # Chat input
    if prompt := st.chat_input("Nháº­p cÃ¢u há»i vá» phÃ¡p luáº­t khoÃ¡ng sáº£n..."):
        
        # ALWAYS SHOW DEBUG INFO - KHÃ”NG Cáº¦N TOGGLE
        st.markdown("## ğŸ› **FORCED DEBUG MODE**")
        st.info(f"ğŸ“ **User Input:** {prompt}")
        
        # Check if mineral related
        mineral_check = is_mineral_related(prompt)
        st.info(f"ğŸ¯ **Is Mineral Related:** {mineral_check}")
        
        if not mineral_check:
            st.session_state.messages.append({"role": "user", "content": prompt})
            st.markdown(f'<div class="user-message">{prompt}</div>', unsafe_allow_html=True)
            
            polite_refusal = """Xin lá»—i, tÃ´i lÃ  trá»£ lÃ½ chuyÃªn vá» **phÃ¡p luáº­t khoÃ¡ng sáº£n** táº¡i Viá»‡t Nam."""
            
            st.session_state.messages.append({"role": "assistant", "content": polite_refusal})
            st.markdown(f'<div class="assistant-message">{polite_refusal}</div>', 
                       unsafe_allow_html=True)
            return
        
        # Add user message
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.markdown(f'<div class="user-message">{prompt}</div>', unsafe_allow_html=True)
        
        # Check search conditions
        web_search_check = should_search_web(prompt)
        st.info(f"ğŸ” **Should Search Web:** {web_search_check}")
        st.info(f"ğŸ” **Web Search Enabled (from toggle):** {web_search_enabled}")
        
        search_will_run = web_search_enabled and web_search_check
        st.info(f"âš¡ **Search Will Run:** {search_will_run}")
        
        # Process response vá»›i FORCED DEBUG
        st.markdown("---")
        st.markdown("### ğŸ”„ **Processing Response...**")
        
        search_results = []
        final_prompt = prompt
        
        if search_will_run:
            st.success("âœ… **SEARCH CONDITIONS MET - Starting search...**")
            
            # Manual search test
            st.markdown("### ğŸ§ª **Manual Search Test**")
            
            try:
                # Test basic DuckDuckGo API call
                st.write("â³ Testing basic DuckDuckGo API...")
                
                test_params = {
                    'q': f'site:thuvienphapluat.vn {prompt}',
                    'format': 'json',
                    'no_html': '1'
                }
                
                test_response = requests.get("https://api.duckduckgo.com/", 
                                           params=test_params, timeout=10)
                
                st.success(f"âœ… DuckDuckGo API Response: {test_response.status_code}")
                
                if test_response.status_code == 200:
                    test_data = test_response.json()
                    
                    st.write("ğŸ“Š **API Response Keys:**")
                    st.json(list(test_data.keys()))
                    
                    # Show Abstract if exists
                    if test_data.get('Abstract'):
                        st.write("ğŸ“„ **Abstract Found:**")
                        st.code(test_data['Abstract'][:300] + "...")
                    else:
                        st.warning("âš ï¸ No Abstract in response")
                    
                    # Show RelatedTopics count
                    related_count = len(test_data.get('RelatedTopics', []))
                    st.write(f"ğŸ”— **Related Topics Count:** {related_count}")
                    
                    if related_count > 0:
                        st.write("ğŸ“ **First Related Topic:**")
                        first_topic = test_data['RelatedTopics'][0]
                        st.json(first_topic)
                    
                else:
                    st.error(f"âŒ API Error: {test_response.status_code}")
                    
            except Exception as e:
                st.error(f"âŒ API Test Failed: {str(e)}")
                import traceback
                st.code(traceback.format_exc())
            
            # Now try our actual search function
            st.markdown("### ğŸ” **Our Search Function Test**")
            
            try:
                st.write("â³ Calling advanced_web_search_improved...")
                search_results = advanced_web_search_improved_v2(prompt)
                st.success(f"âœ… Search function completed. Results: {len(search_results)}")
                
                if search_results:
                    st.markdown("### ğŸ“‹ **SEARCH RESULTS FOUND:**")
                    
                    for i, result in enumerate(search_results, 1):
                        st.markdown(f"#### Result {i}:")
                        st.json({
                            "title": result.get('title', ''),
                            "content": result.get('content', '')[:200] + "...",
                            "url": result.get('url', ''),
                            "confidence": result.get('confidence', 0),
                            "priority": result.get('priority', False),
                            "source": result.get('source', '')
                        })
                        st.markdown("---")
                    
                    # Create final prompt
                    final_prompt = create_safe_enhanced_search_prompt(prompt, search_results)
                    
                    st.markdown("### ğŸ¤– **Final Prompt (First 500 chars):**")
                    st.code(final_prompt[:500] + "...")
                    
                else:
                    st.error("âŒ Search function returned 0 results")
                    
                    # Debug why no results
                    st.markdown("### ğŸ” **Debug: Why No Results?**")
                    
                    # Test individual components
                    st.write("Testing search query construction...")
                    
                    test_queries = [
                        f'site:thuvienphapluat.vn "{prompt}" luáº­t khoÃ¡ng sáº£n',
                        f'site:thuvienphapluat.vn khoÃ¡ng sáº£n "{prompt}"',
                        f'site:monre.gov.vn "{prompt}" khoÃ¡ng sáº£n'
                    ]
                    
                    for i, query in enumerate(test_queries, 1):
                        st.code(f"Query {i}: {query}")
                    
                    # Safe fallback prompt
                    final_prompt = f"""
{prompt}

CRITICAL: Search function returned no results.
HÃ£y tráº£ lá»i: "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin chÃ­nh xÃ¡c vá» váº¥n Ä‘á» nÃ y tá»« cÃ¡c nguá»“n phÃ¡p luáº­t chÃ­nh thá»‘ng. Äá»ƒ cÃ³ thÃ´ng tin chÃ­nh xÃ¡c nháº¥t, báº¡n vui lÃ²ng tham kháº£o trá»±c tiáº¿p táº¡i thuvienphapluat.vn"
"""
                    
            except Exception as e:
                st.error(f"âŒ Search function failed: {str(e)}")
                import traceback
                st.code(traceback.format_exc())
                
                # Emergency fallback
                final_prompt = f"""
{prompt}

CRITICAL: Search function crashed.
HÃ£y tráº£ lá»i: "TÃ´i gáº·p lá»—i ká»¹ thuáº­t khi tÃ¬m kiáº¿m thÃ´ng tin. Vui lÃ²ng tham kháº£o trá»±c tiáº¿p táº¡i thuvienphapluat.vn"
"""
        
        else:
            st.warning("âš ï¸ **SEARCH NOT TRIGGERED**")
            st.write("Possible reasons:")
            st.write("- Web search toggle is OFF")
            st.write("- Query doesn't match search indicators")
            st.write("- Not mineral related")
            
            # No search fallback prompt
            final_prompt = f"""
{prompt}

QUAN TRá»ŒNG: KhÃ´ng cÃ³ tÃ¬m kiáº¿m web Ä‘Æ°á»£c thá»±c hiá»‡n.
HÆ¯á»šNG DáºªN: Chá»‰ Ä‘Æ°á»£c nÃ³i vá» nguyÃªn táº¯c chung vÃ  khuyáº¿n nghá»‹ tham kháº£o nguá»“n chÃ­nh thá»‘ng.
"""
        
        # Show final prompt to be sent to AI
        st.markdown("### ğŸ“¨ **Final Prompt to AI:**")
        with st.expander("Click to view full prompt", expanded=False):
            st.code(final_prompt)
        
        # Generate AI response (keeping original logic)
        messages_for_api = [
            msg for msg in st.session_state.messages[:-1] 
            if msg["role"] != "system" or msg == st.session_state.messages[0]
        ]
        messages_for_api.append({"role": "user", "content": final_prompt})
        
        input_text = "\n".join([msg["content"] for msg in messages_for_api])
        input_tokens = count_tokens(input_text)
        
        # Generate response
        try:
            response = ""
            
            stream = client.chat.completions.create(
                model=selected_model,
                messages=messages_for_api,
                stream=True,
                temperature=temperature,
                max_tokens=2000
            )
            
            response_container = st.empty()
            
            for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    response += chunk.choices[0].delta.content
                    response_container.markdown(
                        f'<div class="assistant-message">{response}â–Œ</div>', 
                        unsafe_allow_html=True
                    )
            
            # Final response
            response_container.markdown(
                f'<div class="assistant-message">{response}</div>', 
                unsafe_allow_html=True
            )
            
            # Update stats
            output_tokens = count_tokens(response)
            update_stats(input_tokens, output_tokens, selected_model)
            
        except Exception as e:
            error_msg = f"âŒ Lá»—i AI response: {str(e)}"
            st.markdown(f'<div class="assistant-message">{error_msg}</div>', 
                       unsafe_allow_html=True)
            response = error_msg
        
        # Add response to history
        st.session_state.messages.append({"role": "assistant", "content": response})
if __name__ == "__main__":
    main()